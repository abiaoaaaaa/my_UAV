{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T07:43:43.332350600Z",
     "start_time": "2024-05-15T07:43:38.816170Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env_test1\n",
    "env = env_test1.DroneEnv()\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c32894c611bf66"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "#vec_env = VecNormalize(env, norm_obs=True, norm_reward=True,clip_obs=1)\n",
    "\n",
    "\n",
    "# Evaluation callback\n",
    "callbacks = []\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=8,\n",
    "    best_model_save_path=\".\",\n",
    "    log_path=\".\",\n",
    "    eval_freq=4000,\n",
    ")\n",
    "\n",
    "callbacks.append(eval_callback)\n",
    "kwargs = {}\n",
    "kwargs[\"callback\"] = callbacks\n",
    "\n",
    "log_name = \"ppo_run_\" + str(time.time())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T07:43:50.140349100Z",
     "start_time": "2024-05-15T07:43:50.110828300Z"
    }
   },
   "id": "a6364eb784437bc7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard/ppo_run_1715756228.2208412_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 445  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=-18106.38 +/- 0.00\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.81e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015927805 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 6.74e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.4e+05      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 1.14e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 415  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 467           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017168734 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.000368      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.67e+06      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0014       |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 3.12e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-23958.30 +/- 0.00\n",
      "Episode length: 346.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 346           |\n",
      "|    mean_reward          | -2.4e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8000          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041435892 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.000273     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.02e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00105      |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 1.66e+06      |\n",
      "-------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 459  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 491           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015961341 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 1.93e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.45e+06      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000368     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 6.88e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-19233.53 +/- 0.00\n",
      "Episode length: 326.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -1.92e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009900724 |\n",
      "|    clip_fraction        | 0.00327      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 9.48e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 1.36e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 491   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.471409e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 8.4e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.47e+06     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000266    |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 4.43e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8171.95 +/- 0.00\n",
      "Episode length: 340.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 340           |\n",
      "|    mean_reward          | -8.17e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 16000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012589531 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.09e+05      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000322     |\n",
      "|    std                  | 0.984         |\n",
      "|    value_loss           | 1.12e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 498   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 512           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053317344 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -7.75e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.82e+05      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.07e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-3224.22 +/- 0.00\n",
      "Episode length: 326.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -3.22e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030783532 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 3.46e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.25e+05      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000933     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 509   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038709096 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.15e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.89e+06      |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.00125      |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 4.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-5979.19 +/- 0.00\n",
      "Episode length: 349.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 349           |\n",
      "|    mean_reward          | -5.98e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 24000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069888023 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -4.53e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.16e+05      |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000922     |\n",
      "|    std                  | 0.983         |\n",
      "|    value_loss           | 5.92e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 518   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 529           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 50            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032254265 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.6e+06       |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.000938     |\n",
      "|    std                  | 0.991         |\n",
      "|    value_loss           | 3.88e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-7157.97 +/- 0.00\n",
      "Episode length: 346.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | -7.16e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001371957 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.24e+05    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.1e+05     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 525   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 534           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 57            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039774805 |\n",
      "|    clip_fraction        | 0.000684      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 1.91e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.56e+05      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.0008       |\n",
      "|    std                  | 0.988         |\n",
      "|    value_loss           | 6.93e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-5192.04 +/- 0.00\n",
      "Episode length: 358.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | -5.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002202701 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.12e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 2.57e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 529   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018257245 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 3.95e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-9625.18 +/- 0.00\n",
      "Episode length: 336.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 336           |\n",
      "|    mean_reward          | -9.63e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 36000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058872753 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.95e+06      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00115      |\n",
      "|    std                  | 0.96          |\n",
      "|    value_loss           | 2.99e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 533   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 69    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 540           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 71            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048949843 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.92e+05      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000764     |\n",
      "|    std                  | 0.947         |\n",
      "|    value_loss           | 1.17e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-20964.72 +/- 0.00\n",
      "Episode length: 331.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 331           |\n",
      "|    mean_reward          | -2.1e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084106496 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.63e+05      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00183      |\n",
      "|    std                  | 0.94          |\n",
      "|    value_loss           | 1.66e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 537   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 544           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 79            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055907236 |\n",
      "|    clip_fraction        | 0.000928      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+05      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.000819     |\n",
      "|    std                  | 0.937         |\n",
      "|    value_loss           | 9.39e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-5275.74 +/- 0.00\n",
      "Episode length: 341.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | -5.28e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013833264 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.79e+04     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.952        |\n",
      "|    value_loss           | 2.89e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 539   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 83    |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 544           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 86            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076049403 |\n",
      "|    clip_fraction        | 0.00146       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.47e+05      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.00157      |\n",
      "|    std                  | 0.955         |\n",
      "|    value_loss           | 1.87e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-2354.24 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | -2.35e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001981632 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 2.98e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.24e+05    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 2.39e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 540   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015204042 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.19e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    std                  | 0.943        |\n",
      "|    value_loss           | 6.25e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-2483.65 +/- 0.00\n",
      "Episode length: 361.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 361          |\n",
      "|    mean_reward          | -2.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008712029 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    std                  | 0.951        |\n",
      "|    value_loss           | 5.91e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 540   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 545           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 101           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076621736 |\n",
      "|    clip_fraction        | 0.004         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.98e+05      |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00176      |\n",
      "|    std                  | 0.949         |\n",
      "|    value_loss           | 2.82e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-2759.56 +/- 0.00\n",
      "Episode length: 344.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -2.76e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047184955 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.99e+04     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    std                  | 0.942        |\n",
      "|    value_loss           | 1.24e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 542   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023742206 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+05     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    std                  | 0.95         |\n",
      "|    value_loss           | 2.87e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-2773.78 +/- 0.00\n",
      "Episode length: 349.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -2.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022073658 |\n",
      "|    clip_fraction        | 0.00605      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.46e+04     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    std                  | 0.937        |\n",
      "|    value_loss           | 9.83e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 541   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 113   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013965135 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.63e+05     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.92         |\n",
      "|    value_loss           | 6.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-3674.98 +/- 0.00\n",
      "Episode length: 340.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004773815 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.77e+04    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    std                  | 0.91        |\n",
      "|    value_loss           | 2.09e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 543   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 120   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 547          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010357145 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.39e+04     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000579    |\n",
      "|    std                  | 0.903        |\n",
      "|    value_loss           | 5.08e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-5449.20 +/- 0.00\n",
      "Episode length: 336.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | -5.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011877308 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.07e+04     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.908        |\n",
      "|    value_loss           | 6.69e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 127   |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 548          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 130          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013734194 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.17e+05     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.919        |\n",
      "|    value_loss           | 1.43e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-3595.79 +/- 0.00\n",
      "Episode length: 342.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 342         |\n",
      "|    mean_reward          | -3.6e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004336537 |\n",
      "|    clip_fraction        | 0.0196      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.64e+04    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    std                  | 0.916       |\n",
      "|    value_loss           | 2.75e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 545   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 135   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 548         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001289693 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.56e+05    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00204    |\n",
      "|    std                  | 0.916       |\n",
      "|    value_loss           | 1.32e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-3661.13 +/- 0.00\n",
      "Episode length: 340.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -3.66e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001993261 |\n",
      "|    clip_fraction        | 0.00376     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.84e+05    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    std                  | 0.919       |\n",
      "|    value_loss           | 4.89e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 142   |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 547          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016640053 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.82e+05     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    std                  | 0.902        |\n",
      "|    value_loss           | 9.34e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-4383.72 +/- 0.00\n",
      "Episode length: 340.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -4.38e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004598881 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 2.98e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.33e+04    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    std                  | 0.912       |\n",
      "|    value_loss           | 8.61e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 150   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 547          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028328032 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00413     |\n",
      "|    std                  | 0.92         |\n",
      "|    value_loss           | 9.14e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-4367.52 +/- 0.00\n",
      "Episode length: 339.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -4.37e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025551685 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.26e+04     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    std                  | 0.91         |\n",
      "|    value_loss           | 3.25e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 158   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-3305.88 +/- 0.00\n",
      "Episode length: 363.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 363         |\n",
      "|    mean_reward          | -3.31e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004897276 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.85e+05    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    std                  | 0.894       |\n",
      "|    value_loss           | 7.5e+05     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 541   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 162   |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005067397 |\n",
      "|    clip_fraction        | 0.0187      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63e+04    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    std                  | 0.855       |\n",
      "|    value_loss           | 3.43e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-3295.69 +/- 0.00\n",
      "Episode length: 364.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | -3.3e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031054204 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93e+04     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.85         |\n",
      "|    value_loss           | 3.68e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 541   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 170   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 173          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067564733 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+04      |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00501     |\n",
      "|    std                  | 0.837        |\n",
      "|    value_loss           | 3.13e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-3113.80 +/- 0.00\n",
      "Episode length: 345.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -3.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031538592 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.16e+05     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    std                  | 0.852        |\n",
      "|    value_loss           | 1.88e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 541   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 177   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053664455 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.22e+04     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 9.61e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-3636.31 +/- 0.00\n",
      "Episode length: 342.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | -3.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038188573 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.97e+04     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00325     |\n",
      "|    std                  | 0.865        |\n",
      "|    value_loss           | 1.75e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 185    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 188          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051792674 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.86e+04     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    std                  | 0.864        |\n",
      "|    value_loss           | 1.7e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-2479.91 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 362          |\n",
      "|    mean_reward          | -2.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019376753 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.27e+04     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    std                  | 0.878        |\n",
      "|    value_loss           | 7.5e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 192    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 544        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 195        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00614134 |\n",
      "|    clip_fraction        | 0.0671     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.78e+03   |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00588   |\n",
      "|    std                  | 0.88       |\n",
      "|    value_loss           | 1.51e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-2828.87 +/- 0.00\n",
      "Episode length: 349.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -2.83e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056572678 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+04     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 1.71e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 200    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001142201 |\n",
      "|    clip_fraction        | 0.0022      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97e+04    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.000352   |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 1.58e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-2573.07 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -2.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053770356 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.37e+04     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00476     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 2.78e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 207    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 546        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 210        |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00541629 |\n",
      "|    clip_fraction        | 0.0313     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.78e+04   |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.00227   |\n",
      "|    std                  | 0.849      |\n",
      "|    value_loss           | 2.98e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-2770.71 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -2.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031178184 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.6e+05      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.000393    |\n",
      "|    std                  | 0.856        |\n",
      "|    value_loss           | 2.74e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 214    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 217          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011908824 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+04     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000698    |\n",
      "|    std                  | 0.871        |\n",
      "|    value_loss           | 4.1e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-2731.66 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 350        |\n",
      "|    mean_reward          | -2.73e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 120000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00226627 |\n",
      "|    clip_fraction        | 0.0144     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.48e+04   |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.000745  |\n",
      "|    std                  | 0.885      |\n",
      "|    value_loss           | 3.55e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 222    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 225          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057871286 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+04     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 1.69e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-4456.33 +/- 0.00\n",
      "Episode length: 348.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | -4.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042518983 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.95e+04     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    std                  | 0.871        |\n",
      "|    value_loss           | 9.53e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 229    |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017133609 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+05     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.875        |\n",
      "|    value_loss           | 3.51e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-5392.36 +/- 0.00\n",
      "Episode length: 348.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | -5.39e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062919324 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+04     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 4.32e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 236    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003751052 |\n",
      "|    clip_fraction        | 0.00786     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12e+05    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 5.9e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-3130.74 +/- 0.00\n",
      "Episode length: 351.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -3.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061315047 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+05     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00375     |\n",
      "|    std                  | 0.865        |\n",
      "|    value_loss           | 2.28e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 545    |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 244    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 247         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005170763 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23e+04    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    std                  | 0.868       |\n",
      "|    value_loss           | 1.06e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-2856.37 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -2.86e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 136000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005372744 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.62e+04    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    std                  | 0.863       |\n",
      "|    value_loss           | 5.42e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 545    |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 251    |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 547         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003946036 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.06e+03    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 1.47e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-4121.57 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.12e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067602093 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.66e+03     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 259    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 262          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056517767 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.24e+04     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    std                  | 0.824        |\n",
      "|    value_loss           | 2.93e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-4168.63 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | -4.17e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041245334 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.66e+04     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 3.86e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 267    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 270          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063478816 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.97e+04     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    std                  | 0.823        |\n",
      "|    value_loss           | 4.87e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-4552.55 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -4.55e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039012483 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.03e+03     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 0.827        |\n",
      "|    value_loss           | 2.42e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 275    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 278          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055721607 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.72e+03     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    std                  | 0.829        |\n",
      "|    value_loss           | 1.93e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-4827.72 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.83e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013740677 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.26e+04     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000195    |\n",
      "|    std                  | 0.83         |\n",
      "|    value_loss           | 1.01e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 282    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005828021 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.17e+04    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 6.18e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-4347.59 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | -4.35e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004075269 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.78e+04    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 9.48e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 290    |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 293          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060061878 |\n",
      "|    clip_fraction        | 0.0597       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.43e+04     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00473     |\n",
      "|    std                  | 0.827        |\n",
      "|    value_loss           | 1.11e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-3755.22 +/- 0.00\n",
      "Episode length: 355.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | -3.76e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019448919 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -4.42e-05    |\n",
      "|    std                  | 0.828        |\n",
      "|    value_loss           | 4.18e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 298    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006138459 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.44e+04    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    std                  | 0.832       |\n",
      "|    value_loss           | 1.72e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-3965.69 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | -3.97e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017154868 |\n",
      "|    clip_fraction        | 0.0943       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.39e+03     |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | 0.00188      |\n",
      "|    std                  | 0.816        |\n",
      "|    value_loss           | 9.38e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 306    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 308          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011039303 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45e+04     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -5.49e-05    |\n",
      "|    std                  | 0.813        |\n",
      "|    value_loss           | 5.1e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-4945.81 +/- 0.00\n",
      "Episode length: 351.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054094763 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+04      |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00016     |\n",
      "|    std                  | 0.813        |\n",
      "|    value_loss           | 3.54e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 313    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-4781.81 +/- 0.00\n",
      "Episode length: 351.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -4.78e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013068992 |\n",
      "|    clip_fraction        | 0.00952      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72e+04     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00274     |\n",
      "|    std                  | 0.813        |\n",
      "|    value_loss           | 9.77e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 317    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 320          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039099017 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.19e+04     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000351    |\n",
      "|    std                  | 0.809        |\n",
      "|    value_loss           | 3.43e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-3454.70 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -3.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039747357 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.55e+04     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    std                  | 0.794        |\n",
      "|    value_loss           | 2.51e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 325    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007189887 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.65e+03    |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    std                  | 0.781       |\n",
      "|    value_loss           | 2.73e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-4107.17 +/- 0.00\n",
      "Episode length: 355.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 355        |\n",
      "|    mean_reward          | -4.11e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06976159 |\n",
      "|    clip_fraction        | 0.0937     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.36e+03   |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | 0.00369    |\n",
      "|    std                  | 0.746      |\n",
      "|    value_loss           | 1.16e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 540    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 333    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001583267 |\n",
      "|    clip_fraction        | 0.00156     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.19e+03    |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.000323   |\n",
      "|    std                  | 0.745       |\n",
      "|    value_loss           | 3.12e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-4098.94 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -4.1e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 184000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005256887 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.76e+03    |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.000405   |\n",
      "|    std                  | 0.74        |\n",
      "|    value_loss           | 2.29e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 340    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003150151 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.24e+04    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.000249   |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 2.5e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-4983.78 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -4.98e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 188000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007210727 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.23e+04    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00119     |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 1.48e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 347    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 350        |\n",
      "|    total_timesteps      | 190464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14931132 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.49e+04   |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | 0.0386     |\n",
      "|    std                  | 0.731      |\n",
      "|    value_loss           | 5.92e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-5595.12 +/- 0.00\n",
      "Episode length: 346.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | -5.6e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.431563e-05 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.48e+04     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.000143    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 1.88e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 354    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 357          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019137724 |\n",
      "|    clip_fraction        | 0.00669      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.21e+04     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 6.44e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-5079.43 +/- 0.00\n",
      "Episode length: 348.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 348        |\n",
      "|    mean_reward          | -5.08e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 196000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00267819 |\n",
      "|    clip_fraction        | 0.00542    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.44e+04   |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.00193   |\n",
      "|    std                  | 0.73       |\n",
      "|    value_loss           | 7.71e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 361    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029853802 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.76e+04    |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.00319     |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 7.78e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-4893.82 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 350           |\n",
      "|    mean_reward          | -4.89e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 200000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049034256 |\n",
      "|    clip_fraction        | 0.00103       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.98e+04      |\n",
      "|    n_updates            | 970           |\n",
      "|    policy_gradient_loss | -0.000108     |\n",
      "|    std                  | 0.725         |\n",
      "|    value_loss           | 9.01e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 368    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 202752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006943006 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.55e+04    |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | 0.00466     |\n",
      "|    std                  | 0.729       |\n",
      "|    value_loss           | 8.7e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-5576.29 +/- 0.00\n",
      "Episode length: 349.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -5.58e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 204000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013026772 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.49e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.81e+04     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.729        |\n",
      "|    value_loss           | 1.49e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 545    |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 375    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 101          |\n",
      "|    time_elapsed         | 378          |\n",
      "|    total_timesteps      | 206848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033463961 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.49e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54e+04     |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 6.78e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-4896.03 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -4.9e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018858515 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 8.52e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63e+04     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 3.49e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 546    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 382    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 547          |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 385          |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032018023 |\n",
      "|    clip_fraction        | 0.00601      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0174       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.33e+04     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 4.54e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-4643.56 +/- 0.00\n",
      "Episode length: 348.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | -4.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 212000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004197119 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0149       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.23e+04     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000192    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 1.14e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 546    |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 389    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 105        |\n",
      "|    time_elapsed         | 392        |\n",
      "|    total_timesteps      | 215040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00396362 |\n",
      "|    clip_fraction        | 0.0136     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.0031     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.26e+04   |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.00299   |\n",
      "|    std                  | 0.728      |\n",
      "|    value_loss           | 9.36e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-4631.94 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.63e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 216000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028216233 |\n",
      "|    clip_fraction        | 0.00684      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.00888      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.44e+04     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.727        |\n",
      "|    value_loss           | 5e+04        |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 546    |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 397    |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 400          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005170455 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.00809      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.33e+04     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    std                  | 0.729        |\n",
      "|    value_loss           | 5.81e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-3801.66 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -3.8e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003024058 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0211      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.96e+04    |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    std                  | 0.73        |\n",
      "|    value_loss           | 2.69e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 545    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 405    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 408          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023894093 |\n",
      "|    clip_fraction        | 0.00469      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0253       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.75e+03     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.000212    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 2.37e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-4190.46 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | -4.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023263765 |\n",
      "|    clip_fraction        | 0.00835      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0225       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+04     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.0002      |\n",
      "|    std                  | 0.729        |\n",
      "|    value_loss           | 4.99e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 545    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 413    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 416         |\n",
      "|    total_timesteps      | 227328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002894476 |\n",
      "|    clip_fraction        | 0.00708     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0233      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.52e+03    |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.000786   |\n",
      "|    std                  | 0.727       |\n",
      "|    value_loss           | 1.11e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-4794.08 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.79e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 228000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050913463 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0613       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.08e+03     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.725        |\n",
      "|    value_loss           | 1.71e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 421    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 424          |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042421855 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0456       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75e+04     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.725        |\n",
      "|    value_loss           | 5.03e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-4422.53 +/- 0.00\n",
      "Episode length: 351.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 351           |\n",
      "|    mean_reward          | -4.42e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 232000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047767733 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0.0314        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.34e+04      |\n",
      "|    n_updates            | 1130          |\n",
      "|    policy_gradient_loss | -0.000429     |\n",
      "|    std                  | 0.719         |\n",
      "|    value_loss           | 8.62e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 429    |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 432          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025904714 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0247       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.8e+04      |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.723        |\n",
      "|    value_loss           | 1.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-4918.03 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -4.92e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 236000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024077254 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0271       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+04     |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    std                  | 0.718        |\n",
      "|    value_loss           | 3.8e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 437    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 440          |\n",
      "|    total_timesteps      | 239616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039496473 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0251       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.23e+04     |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 7.03e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-5232.48 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -5.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027323295 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.028        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+04     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    std                  | 0.719        |\n",
      "|    value_loss           | 7.17e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 445    |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 448         |\n",
      "|    total_timesteps      | 243712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005665377 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0224      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.11e+04    |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 1.09e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-3801.82 +/- 0.00\n",
      "Episode length: 355.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | -3.8e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 244000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027151022 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0275       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.37e+03     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.000113    |\n",
      "|    std                  | 0.714        |\n",
      "|    value_loss           | 1.55e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 453    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004309605 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0592      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.79e+03    |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00259    |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 1.83e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-4450.41 +/- 0.00\n",
      "Episode length: 351.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -4.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 248000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016691366 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0301       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.79e+04     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | 0.00134      |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 5.79e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 460    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 123         |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 251904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008334168 |\n",
      "|    clip_fraction        | 0.0193      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0648      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.23e+04    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.000746   |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 4.59e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-3561.26 +/- 0.00\n",
      "Episode length: 356.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | -3.56e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 252000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031629181 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0554       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.97e+03     |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.717        |\n",
      "|    value_loss           | 4.45e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 468    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-4275.90 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 353         |\n",
      "|    mean_reward          | -4.28e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006362181 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0378      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.8e+04     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.00115    |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 2.97e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 472    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 475          |\n",
      "|    total_timesteps      | 258048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015816975 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.031        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+04     |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | -0.000665    |\n",
      "|    std                  | 0.712        |\n",
      "|    value_loss           | 5.26e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-4116.51 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -4.12e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 260000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098196855 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0617       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+04     |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 2.64e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 480    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 483          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013414352 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0658       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.26e+04     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | 0.000321     |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 4.06e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-5205.21 +/- 0.00\n",
      "Episode length: 349.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -5.21e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 264000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057093324 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0407       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.23e+03     |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.000192    |\n",
      "|    std                  | 0.708        |\n",
      "|    value_loss           | 3.62e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 487    |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 490          |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031747343 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0425       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.59e+04     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.712        |\n",
      "|    value_loss           | 1.02e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-4732.55 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.73e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028586192 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0577       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46e+04     |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 0.719        |\n",
      "|    value_loss           | 3.72e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 495    |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 498          |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010081641 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0383       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24e+04     |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.00021     |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 8.55e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-4766.50 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031223067 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.13e+04     |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.000325    |\n",
      "|    std                  | 0.716        |\n",
      "|    value_loss           | 5.54e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 503    |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 505         |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004853433 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.84e+04    |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 6.49e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-5591.24 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 350         |\n",
      "|    mean_reward          | -5.59e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 276000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002433192 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.0364      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.77e+04    |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.000695   |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 3.56e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 510    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 513          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026420157 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.059        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+04     |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.000291    |\n",
      "|    std                  | 0.725        |\n",
      "|    value_loss           | 3.75e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-4646.24 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -4.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 280000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042287833 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0324       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.39e+04     |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 1.81e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 541    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 517    |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 520          |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026759873 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.032        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.4e+04      |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    std                  | 0.727        |\n",
      "|    value_loss           | 3.64e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-4577.37 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -4.58e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 284000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047678403 |\n",
      "|    clip_fraction        | 0.054        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0273       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+04     |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    std                  | 0.721        |\n",
      "|    value_loss           | 1e+05        |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 525    |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 528          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083592385 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.04         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.55e+04     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 3.27e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-4987.54 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -4.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 288000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017197262 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0524       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.16e+04     |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | 0.000459     |\n",
      "|    std                  | 0.728        |\n",
      "|    value_loss           | 4.65e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 532    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 142          |\n",
      "|    time_elapsed         | 535          |\n",
      "|    total_timesteps      | 290816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037431328 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.047        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.43e+04     |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 6.69e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-5671.81 +/- 0.00\n",
      "Episode length: 348.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 348         |\n",
      "|    mean_reward          | -5.67e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 292000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002588178 |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0439      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38e+04    |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    std                  | 0.721       |\n",
      "|    value_loss           | 1.13e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 539    |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 144        |\n",
      "|    time_elapsed         | 542        |\n",
      "|    total_timesteps      | 294912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00956491 |\n",
      "|    clip_fraction        | 0.0528     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.0304     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.62e+04   |\n",
      "|    n_updates            | 1430       |\n",
      "|    policy_gradient_loss | -0.00171   |\n",
      "|    std                  | 0.718      |\n",
      "|    value_loss           | 1.1e+05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-4968.48 +/- 0.00\n",
      "Episode length: 350.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 350         |\n",
      "|    mean_reward          | -4.97e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008333277 |\n",
      "|    clip_fraction        | 0.0537      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0405      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.53e+04    |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    std                  | 0.719       |\n",
      "|    value_loss           | 5.98e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 547    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 550          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024336232 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0321       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.77e+04     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.000676    |\n",
      "|    std                  | 0.718        |\n",
      "|    value_loss           | 1.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-4465.61 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -4.47e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029190606 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0223       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.2e+04      |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    std                  | 0.718        |\n",
      "|    value_loss           | 1.02e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 554    |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 557          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048376303 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0232       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.94e+04     |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.00502     |\n",
      "|    std                  | 0.708        |\n",
      "|    value_loss           | 1.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-4135.12 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -4.14e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017621721 |\n",
      "|    clip_fraction        | 0.00806      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0256       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08e+03     |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.000707    |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.2e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 561    |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 564          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012476877 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0383       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+04     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 7.03e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-3538.69 +/- 0.00\n",
      "Episode length: 356.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | -3.54e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 308000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003663816 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0506      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57e+04    |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 4.04e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 568    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 152         |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005831562 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.0845      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.78e+03    |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 2.92e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-3423.28 +/- 0.00\n",
      "Episode length: 356.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | -3.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 312000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064885565 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0578       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25e+04     |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 1.58e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 576    |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 579          |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024931312 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0843       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.15e+04     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 2.5e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-3494.04 +/- 0.00\n",
      "Episode length: 358.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | -3.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044162786 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+04     |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 4.79e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 583    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 586          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016504792 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0907       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.64e+03     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.000542    |\n",
      "|    std                  | 0.66         |\n",
      "|    value_loss           | 2.09e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-4066.14 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -4.07e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002859098 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | 0.092       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.67e+03    |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    std                  | 0.648       |\n",
      "|    value_loss           | 1.48e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 591    |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 593          |\n",
      "|    total_timesteps      | 323584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021340991 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.979       |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.1e+03      |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -0.000905    |\n",
      "|    std                  | 0.639        |\n",
      "|    value_loss           | 2.97e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-4075.39 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | -4.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034118202 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.971       |\n",
      "|    explained_variance   | 0.0819       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.98e+03     |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.000692    |\n",
      "|    std                  | 0.64         |\n",
      "|    value_loss           | 2.24e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 598    |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 601          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036402913 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.97        |\n",
      "|    explained_variance   | 0.0868       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+04     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 0.639        |\n",
      "|    value_loss           | 4.14e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-2996.24 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 362          |\n",
      "|    mean_reward          | -3e+03       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058052666 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.965       |\n",
      "|    explained_variance   | 0.0933       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.03e+03     |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.632        |\n",
      "|    value_loss           | 1.92e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 605    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 608          |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055255694 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.956       |\n",
      "|    explained_variance   | 0.0445       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.99e+03     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    std                  | 0.626        |\n",
      "|    value_loss           | 1.81e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-3170.74 +/- 0.00\n",
      "Episode length: 357.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 357          |\n",
      "|    mean_reward          | -3.17e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049232715 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+04     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.623        |\n",
      "|    value_loss           | 1.85e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 613    |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 616          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035747355 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.0725       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.23e+03     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 1.17e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-4243.62 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 352        |\n",
      "|    mean_reward          | -4.24e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 336000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00795022 |\n",
      "|    clip_fraction        | 0.0983     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.945     |\n",
      "|    explained_variance   | 0.133      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.96e+03   |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.00741   |\n",
      "|    std                  | 0.62       |\n",
      "|    value_loss           | 1.68e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 620    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 339968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006659448 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | 0.0677      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.97e+04    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.00457    |\n",
      "|    std                  | 0.62        |\n",
      "|    value_loss           | 8.86e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-4294.03 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -4.29e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061522163 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.944       |\n",
      "|    explained_variance   | 0.0749       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.29e+04     |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    std                  | 0.622        |\n",
      "|    value_loss           | 5.37e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 628    |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-2591.46 +/- 0.00\n",
      "Episode length: 355.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | -2.59e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 344000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059718788 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.936       |\n",
      "|    explained_variance   | 0.0996       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.29e+03     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 1.54e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 632    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012546619 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.928       |\n",
      "|    explained_variance   | 0.0834       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.87e+04     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.613        |\n",
      "|    value_loss           | 6.89e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-2812.04 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -2.81e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037682413 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.923       |\n",
      "|    explained_variance   | 0.0894       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.78e+04     |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    std                  | 0.605        |\n",
      "|    value_loss           | 4.09e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 639    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 171         |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 350208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005387255 |\n",
      "|    clip_fraction        | 0.0251      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | 0.044       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.95e+04    |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 2.33e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-5299.77 +/- 0.00\n",
      "Episode length: 349.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -5.3e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 352000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015552051 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.0638       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+04     |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.00046     |\n",
      "|    std                  | 0.609        |\n",
      "|    value_loss           | 6.45e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 647    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 650          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036997222 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.928       |\n",
      "|    explained_variance   | 0.0762       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18e+03     |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    std                  | 0.614        |\n",
      "|    value_loss           | 4.15e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-5560.90 +/- 0.00\n",
      "Episode length: 348.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | -5.56e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 356000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020606196 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.935       |\n",
      "|    explained_variance   | 0.0919       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+04     |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    std                  | 0.617        |\n",
      "|    value_loss           | 4.13e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 654    |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 657         |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001280956 |\n",
      "|    clip_fraction        | 0.011       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.931      |\n",
      "|    explained_variance   | 0.0761      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.5e+04     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 7.37e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-3946.52 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -3.95e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066182716 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.928       |\n",
      "|    explained_variance   | 0.0643       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.19e+04     |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 9.72e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 662    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 665          |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019792155 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.935       |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94e+03     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    std                  | 0.62         |\n",
      "|    value_loss           | 2.6e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-4081.85 +/- 0.00\n",
      "Episode length: 351.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -4.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 364000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036382277 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.948       |\n",
      "|    explained_variance   | 0.0563       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.68e+04     |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | -0.000348    |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 6.48e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 670    |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 179         |\n",
      "|    time_elapsed         | 673         |\n",
      "|    total_timesteps      | 366592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003181323 |\n",
      "|    clip_fraction        | 0.0085      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.0565      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.5e+04     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.000767   |\n",
      "|    std                  | 0.625       |\n",
      "|    value_loss           | 1.22e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-3813.86 +/- 0.00\n",
      "Episode length: 352.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -3.81e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 368000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037416215 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.083        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+04     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    std                  | 0.623        |\n",
      "|    value_loss           | 1.33e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 677    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 680          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005143916 |\n",
      "|    clip_fraction        | 0.00249      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.945       |\n",
      "|    explained_variance   | 0.0657       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.79e+04     |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -8.36e-05    |\n",
      "|    std                  | 0.622        |\n",
      "|    value_loss           | 9.66e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-3101.14 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | -3.1e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 372000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035211816 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.948       |\n",
      "|    explained_variance   | 0.0589       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.5e+04      |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.628        |\n",
      "|    value_loss           | 6.75e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 684    |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 687          |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017921453 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.957       |\n",
      "|    explained_variance   | 0.0732       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27e+04     |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.000936    |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 2.27e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-2605.52 +/- 0.00\n",
      "Episode length: 355.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | -2.61e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 376000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058690757 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.0666       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.47e+04     |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 3.84e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 692    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 695          |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009980178 |\n",
      "|    clip_fraction        | 0.00439      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | 0.0937       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04e+04     |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.000295    |\n",
      "|    std                  | 0.609        |\n",
      "|    value_loss           | 3.44e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-2624.10 +/- 0.00\n",
      "Episode length: 354.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 354           |\n",
      "|    mean_reward          | -2.62e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 380000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00063930464 |\n",
      "|    clip_fraction        | 0.0104        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.922        |\n",
      "|    explained_variance   | 0.0876        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.7e+04       |\n",
      "|    n_updates            | 1850          |\n",
      "|    policy_gradient_loss | -0.00128      |\n",
      "|    std                  | 0.606         |\n",
      "|    value_loss           | 6.21e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 699    |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 702          |\n",
      "|    total_timesteps      | 382976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037965756 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.913       |\n",
      "|    explained_variance   | 0.0452       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.95e+04     |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    std                  | 0.601        |\n",
      "|    value_loss           | 1.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-2196.72 +/- 0.00\n",
      "Episode length: 358.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | -2.2e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022569974 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.911       |\n",
      "|    explained_variance   | 0.0564       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.06e+04     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.000834    |\n",
      "|    std                  | 0.603        |\n",
      "|    value_loss           | 2.79e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 707    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 709         |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002603609 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | 0.0483      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.09e+04    |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 4.12e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-2196.98 +/- 0.00\n",
      "Episode length: 358.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | -2.2e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 388000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037828279 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.921       |\n",
      "|    explained_variance   | 0.0552       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.34e+04     |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 0.606        |\n",
      "|    value_loss           | 5.9e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 714    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 717          |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078068934 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.909       |\n",
      "|    explained_variance   | 0.0672       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+04     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    std                  | 0.596        |\n",
      "|    value_loss           | 1.56e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-2394.47 +/- 0.00\n",
      "Episode length: 356.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | -2.39e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 392000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025582793 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.897       |\n",
      "|    explained_variance   | 0.0939       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.06e+04     |\n",
      "|    n_updates            | 1910         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    std                  | 0.591        |\n",
      "|    value_loss           | 2.08e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 721    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 545           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 724           |\n",
      "|    total_timesteps      | 395264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081763393 |\n",
      "|    clip_fraction        | 0.00859       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.89         |\n",
      "|    explained_variance   | 0.0819        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.36e+03      |\n",
      "|    n_updates            | 1920          |\n",
      "|    policy_gradient_loss | -0.000181     |\n",
      "|    std                  | 0.588         |\n",
      "|    value_loss           | 2.47e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-3139.41 +/- 0.00\n",
      "Episode length: 353.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | -3.14e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 396000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065780184 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.882       |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+04     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.00428     |\n",
      "|    std                  | 0.58         |\n",
      "|    value_loss           | 2.25e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 544    |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 729    |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924289 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.875      |\n",
      "|    explained_variance   | 0.0716      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.1e+04     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    std                  | 0.581       |\n",
      "|    value_loss           | 1.18e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-2373.46 +/- 0.00\n",
      "Episode length: 355.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | -2.37e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043282546 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.874       |\n",
      "|    explained_variance   | 0.0932       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+04     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.579        |\n",
      "|    value_loss           | 3.65e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 545    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 736    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "Before training: mean_reward:-2389.45 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "model.learn(\n",
    "    total_timesteps=400000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50)\n",
    "print(f\"Before training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "model.save(\"last_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T07:09:37.426117400Z",
     "start_time": "2024-05-15T06:57:09.553812300Z"
    }
   },
   "id": "6b8247d12e488bd3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard/ppo_run_1715759030.109828_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 532  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=-4085.45 +/- 3161.50\n",
      "Episode length: 367.75 +/- 38.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 368          |\n",
      "|    mean_reward          | -4.09e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018891745 |\n",
      "|    clip_fraction        | 0.00874      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | 0.0534       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.26e+04     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    std                  | 0.606        |\n",
      "|    value_loss           | 2.34e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 456  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 504          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041696336 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.914       |\n",
      "|    explained_variance   | -0.0277      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.76e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    std                  | 0.6          |\n",
      "|    value_loss           | 1.35e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-2422.88 +/- 759.31\n",
      "Episode length: 340.38 +/- 43.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -2.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015812771 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.902       |\n",
      "|    explained_variance   | 0.0314       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.93e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000346    |\n",
      "|    std                  | 0.592        |\n",
      "|    value_loss           | 1.72e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 482  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 505          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018837377 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.892       |\n",
      "|    explained_variance   | 0.0305       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+04     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    std                  | 0.588        |\n",
      "|    value_loss           | 1.13e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-4126.48 +/- 2273.50\n",
      "Episode length: 381.50 +/- 30.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 382          |\n",
      "|    mean_reward          | -4.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060715238 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.884       |\n",
      "|    explained_variance   | 0.0596       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94e+04     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    std                  | 0.583        |\n",
      "|    value_loss           | 3.39e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 483   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060795294 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.879       |\n",
      "|    explained_variance   | 0.0388       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+05     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    std                  | 0.582        |\n",
      "|    value_loss           | 3.13e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-3118.64 +/- 2029.59\n",
      "Episode length: 384.62 +/- 38.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 385         |\n",
      "|    mean_reward          | -3.12e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005251943 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.876      |\n",
      "|    explained_variance   | 0.0806      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.25e+04    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    std                  | 0.581       |\n",
      "|    value_loss           | 3.54e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057980744 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.88        |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36e+04     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.583        |\n",
      "|    value_loss           | 6.42e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-5191.11 +/- 4419.62\n",
      "Episode length: 347.25 +/- 46.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | -5.19e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004993277 |\n",
      "|    clip_fraction        | 0.0291      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.877      |\n",
      "|    explained_variance   | 0.0993      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.36e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.58        |\n",
      "|    value_loss           | 5.09e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040443884 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.869       |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.22e+04     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.573        |\n",
      "|    value_loss           | 3.8e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-4271.63 +/- 2667.51\n",
      "Episode length: 360.88 +/- 53.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 361          |\n",
      "|    mean_reward          | -4.27e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016671906 |\n",
      "|    clip_fraction        | 0.00557      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.86        |\n",
      "|    explained_variance   | 0.0578       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.95e+05     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000335    |\n",
      "|    std                  | 0.57         |\n",
      "|    value_loss           | 4.55e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 486   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020779716 |\n",
      "|    clip_fraction        | 0.00762      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.86        |\n",
      "|    explained_variance   | 0.0514       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000822    |\n",
      "|    std                  | 0.573        |\n",
      "|    value_loss           | 4.4e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-9573.15 +/- 6782.22\n",
      "Episode length: 377.00 +/- 37.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 377         |\n",
      "|    mean_reward          | -9.57e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001911248 |\n",
      "|    clip_fraction        | 0.00366     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.0502      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.96e+04    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.000759   |\n",
      "|    std                  | 0.557       |\n",
      "|    value_loss           | 2.51e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 487   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053944634 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.833       |\n",
      "|    explained_variance   | 0.0745       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+05     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    std                  | 0.555        |\n",
      "|    value_loss           | 1.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-2610.13 +/- 1115.70\n",
      "Episode length: 324.50 +/- 39.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -2.61e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026001753 |\n",
      "|    clip_fraction        | 0.00962      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.825       |\n",
      "|    explained_variance   | 0.0676       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.43e+04     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 0.55         |\n",
      "|    value_loss           | 2.08e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 491   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002281016 |\n",
      "|    clip_fraction        | 0.0455      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.817      |\n",
      "|    explained_variance   | 0.0813      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.72e+04    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 3.79e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-5661.23 +/- 5278.21\n",
      "Episode length: 386.38 +/- 44.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 386         |\n",
      "|    mean_reward          | -5.66e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006513791 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.81       |\n",
      "|    explained_variance   | 0.0561      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.82e+05    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 2.61e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 492   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040318286 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.814       |\n",
      "|    explained_variance   | 0.0538       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78e+04     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 0.548        |\n",
      "|    value_loss           | 1.47e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-4261.19 +/- 5444.61\n",
      "Episode length: 364.25 +/- 62.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | -4.26e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054926937 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.822       |\n",
      "|    explained_variance   | 0.0569       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.11e+04     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.552        |\n",
      "|    value_loss           | 2.27e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 492   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 83    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026316806 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.823       |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    std                  | 0.55         |\n",
      "|    value_loss           | 2.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-3006.71 +/- 1999.66\n",
      "Episode length: 354.75 +/- 48.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 355         |\n",
      "|    mean_reward          | -3.01e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004483259 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.824      |\n",
      "|    explained_variance   | 0.0542      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.05e+05    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00236    |\n",
      "|    std                  | 0.553       |\n",
      "|    value_loss           | 1.45e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 497           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 94            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059166685 |\n",
      "|    clip_fraction        | 0.0062        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.828        |\n",
      "|    explained_variance   | 0.0555        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.32e+04      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000306     |\n",
      "|    std                  | 0.556         |\n",
      "|    value_loss           | 2.65e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-6870.64 +/- 5393.14\n",
      "Episode length: 356.00 +/- 37.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | -6.87e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035781222 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.826       |\n",
      "|    explained_variance   | 0.0559       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17e+04     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    std                  | 0.548        |\n",
      "|    value_loss           | 1.72e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 498         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004674469 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.814      |\n",
      "|    explained_variance   | 0.0352      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.1e+05     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 7.11e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-7063.36 +/- 6508.25\n",
      "Episode length: 333.00 +/- 49.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 333         |\n",
      "|    mean_reward          | -7.06e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003699149 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.81       |\n",
      "|    explained_variance   | 0.0911      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.24e+04    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00091    |\n",
      "|    std                  | 0.543       |\n",
      "|    value_loss           | 1.27e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 107   |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022486122 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.808       |\n",
      "|    explained_variance   | 0.0704       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+05     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    std                  | 0.543        |\n",
      "|    value_loss           | 4.43e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-6222.54 +/- 5097.22\n",
      "Episode length: 411.25 +/- 27.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 411          |\n",
      "|    mean_reward          | -6.22e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053842533 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.806       |\n",
      "|    explained_variance   | 0.0349       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+05     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    std                  | 0.541        |\n",
      "|    value_loss           | 5.96e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 115   |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021943937 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.803       |\n",
      "|    explained_variance   | 0.0554       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+04     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000464    |\n",
      "|    std                  | 0.538        |\n",
      "|    value_loss           | 1.53e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-7773.22 +/- 7640.01\n",
      "Episode length: 337.50 +/- 53.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 338         |\n",
      "|    mean_reward          | -7.77e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004485779 |\n",
      "|    clip_fraction        | 0.0187      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | 0.0426      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.26e+05    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00058    |\n",
      "|    std                  | 0.537       |\n",
      "|    value_loss           | 5.5e+05     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060731475 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.8         |\n",
      "|    explained_variance   | 0.0507       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46e+04     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00369     |\n",
      "|    std                  | 0.54         |\n",
      "|    value_loss           | 2.75e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-4412.73 +/- 4685.37\n",
      "Episode length: 345.25 +/- 26.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -4.41e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021915692 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.803       |\n",
      "|    explained_variance   | 0.0385       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+05     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.542        |\n",
      "|    value_loss           | 5.26e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 132   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017944709 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.806       |\n",
      "|    explained_variance   | 0.049        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.57e+05     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    std                  | 0.542        |\n",
      "|    value_loss           | 9.63e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-7446.85 +/- 9443.29\n",
      "Episode length: 307.00 +/- 23.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -7.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025982172 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.809       |\n",
      "|    explained_variance   | 0.0304       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.1e+05      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    std                  | 0.545        |\n",
      "|    value_loss           | 1.34e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 497   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 140   |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 500         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004121433 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.811      |\n",
      "|    explained_variance   | 0.0326      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.05e+04    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    std                  | 0.543       |\n",
      "|    value_loss           | 2.56e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-8591.30 +/- 6638.11\n",
      "Episode length: 340.00 +/- 42.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -8.59e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033910125 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.811       |\n",
      "|    explained_variance   | 0.0324       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08e+05     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    std                  | 0.547        |\n",
      "|    value_loss           | 2.81e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 498   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 148   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030201694 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.817       |\n",
      "|    explained_variance   | 0.0203       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.52e+05     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    std                  | 0.548        |\n",
      "|    value_loss           | 1.06e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-6559.00 +/- 5212.00\n",
      "Episode length: 346.88 +/- 39.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | -6.56e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046468377 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.817       |\n",
      "|    explained_variance   | 0.0467       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.93e+04     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    std                  | 0.548        |\n",
      "|    value_loss           | 1.04e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 497   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 156   |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018887557 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.826       |\n",
      "|    explained_variance   | 0.0514       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.9e+05      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    std                  | 0.556        |\n",
      "|    value_loss           | 4.52e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-7431.70 +/- 6975.12\n",
      "Episode length: 342.12 +/- 51.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | -7.43e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005565373 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.828       |\n",
      "|    explained_variance   | 0.0326       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.44e+05     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    std                  | 0.552        |\n",
      "|    value_loss           | 3.68e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 165   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049421582 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.825       |\n",
      "|    explained_variance   | 0.0262       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.551        |\n",
      "|    value_loss           | 7.51e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-6923.69 +/- 7044.75\n",
      "Episode length: 318.88 +/- 37.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -6.92e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015774022 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.821       |\n",
      "|    explained_variance   | 0.0437       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.82e+05     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.55         |\n",
      "|    value_loss           | 5.95e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 173   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-5635.03 +/- 4892.23\n",
      "Episode length: 337.25 +/- 50.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 337          |\n",
      "|    mean_reward          | -5.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024156775 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.831       |\n",
      "|    explained_variance   | 0.0403       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+05     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.562        |\n",
      "|    value_loss           | 2.78e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 494   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 178   |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 181          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009901332 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.844       |\n",
      "|    explained_variance   | 0.0435       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.55e+05     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    std                  | 0.563        |\n",
      "|    value_loss           | 5.01e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-7817.80 +/- 5610.45\n",
      "Episode length: 339.00 +/- 55.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -7.82e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024647224 |\n",
      "|    clip_fraction        | 0.00752      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.834       |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.72e+04     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.551        |\n",
      "|    value_loss           | 1.5e+05      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 494   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 186   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061579486 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.823       |\n",
      "|    explained_variance   | 0.0432       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.95e+05     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    std                  | 0.552        |\n",
      "|    value_loss           | 6.94e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-8201.80 +/- 9334.23\n",
      "Episode length: 338.00 +/- 49.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 338          |\n",
      "|    mean_reward          | -8.2e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042099217 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.831       |\n",
      "|    explained_variance   | 0.0703       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.11e+05     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    std                  | 0.559        |\n",
      "|    value_loss           | 3.9e+05      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 194   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 498          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 197          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011204479 |\n",
      "|    clip_fraction        | 0.00273      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.838       |\n",
      "|    explained_variance   | 0.0368       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.92e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000927    |\n",
      "|    std                  | 0.56         |\n",
      "|    value_loss           | 1.08e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8612.72 +/- 8419.92\n",
      "Episode length: 349.50 +/- 59.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -8.61e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013144026 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.842       |\n",
      "|    explained_variance   | 0.0474       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.11e+04     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000562    |\n",
      "|    std                  | 0.564        |\n",
      "|    value_loss           | 2.1e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 202    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 206          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034489702 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.849       |\n",
      "|    explained_variance   | 0.0946       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    std                  | 0.565        |\n",
      "|    value_loss           | 2.39e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-9312.36 +/- 7452.18\n",
      "Episode length: 360.38 +/- 49.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 360          |\n",
      "|    mean_reward          | -9.31e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026234724 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.851       |\n",
      "|    explained_variance   | 0.0689       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.88e+05     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00376     |\n",
      "|    std                  | 0.569        |\n",
      "|    value_loss           | 4.32e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 211    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034615977 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.855       |\n",
      "|    explained_variance   | 0.039        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+05     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    std                  | 0.57         |\n",
      "|    value_loss           | 1.32e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-8076.12 +/- 5243.42\n",
      "Episode length: 339.00 +/- 47.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -8.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019875136 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.86        |\n",
      "|    explained_variance   | 0.0333       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71e+06     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.574        |\n",
      "|    value_loss           | 1.89e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 220    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025279005 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.869       |\n",
      "|    explained_variance   | 0.0303       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+05     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 0.582        |\n",
      "|    value_loss           | 4.69e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-7585.89 +/- 7346.06\n",
      "Episode length: 346.12 +/- 32.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | -7.59e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002652161 |\n",
      "|    clip_fraction        | 0.0154      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.49e+06    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    std                  | 0.585       |\n",
      "|    value_loss           | 2.7e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 228    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 493          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012331256 |\n",
      "|    clip_fraction        | 0.0063       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.889       |\n",
      "|    explained_variance   | 0.0157       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.85e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.593        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-6864.40 +/- 5285.89\n",
      "Episode length: 341.50 +/- 51.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | -6.86e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013966939 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.896       |\n",
      "|    explained_variance   | 0.0104       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+06     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.000551    |\n",
      "|    std                  | 0.593        |\n",
      "|    value_loss           | 1.35e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 491    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 237    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 493          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 240          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014291315 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.899       |\n",
      "|    explained_variance   | 0.0189       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.88e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    std                  | 0.597        |\n",
      "|    value_loss           | 1.67e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-5570.57 +/- 5636.64\n",
      "Episode length: 359.75 +/- 42.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 360          |\n",
      "|    mean_reward          | -5.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011402118 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.896       |\n",
      "|    explained_variance   | 0.0301       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.6e+05      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000425    |\n",
      "|    std                  | 0.588        |\n",
      "|    value_loss           | 8.05e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 491    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 245    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001691405 |\n",
      "|    clip_fraction        | 0.0138      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.889      |\n",
      "|    explained_variance   | 0.0213      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.45e+05    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 2.07e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-8076.99 +/- 8066.58\n",
      "Episode length: 344.75 +/- 53.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -8.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015378848 |\n",
      "|    clip_fraction        | 0.00845      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.891       |\n",
      "|    explained_variance   | 0.0219       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.41e+04     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    std                  | 0.591        |\n",
      "|    value_loss           | 5.59e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 254    |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 258          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022913695 |\n",
      "|    clip_fraction        | 0.00806      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.899       |\n",
      "|    explained_variance   | 0.0392       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64e+05     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    std                  | 0.599        |\n",
      "|    value_loss           | 2.97e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-9454.86 +/- 7989.83\n",
      "Episode length: 352.25 +/- 53.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | -9.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020100414 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.908       |\n",
      "|    explained_variance   | 0.0319       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.77e+05     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.000861    |\n",
      "|    std                  | 0.602        |\n",
      "|    value_loss           | 1.56e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 489    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 263    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 266          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011262338 |\n",
      "|    clip_fraction        | 0.00288      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.911       |\n",
      "|    explained_variance   | 0.0233       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+06     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000707    |\n",
      "|    std                  | 0.602        |\n",
      "|    value_loss           | 2.89e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-5529.82 +/- 6590.36\n",
      "Episode length: 344.25 +/- 49.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -5.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010887114 |\n",
      "|    clip_fraction        | 0.00601      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.913       |\n",
      "|    explained_variance   | 0.0284       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+04     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.604        |\n",
      "|    value_loss           | 1.01e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 489    |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 271    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 275          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012538393 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | 0.0306       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.63e+05     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.609        |\n",
      "|    value_loss           | 2.54e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-11307.50 +/- 7232.62\n",
      "Episode length: 350.38 +/- 50.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -1.13e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056239115 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | 0.0201       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    std                  | 0.603        |\n",
      "|    value_loss           | 1.91e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 488    |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 280    |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036168254 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.0371       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.18e+05     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 5.87e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-12521.98 +/- 7706.78\n",
      "Episode length: 367.25 +/- 29.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 367         |\n",
      "|    mean_reward          | -1.25e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002111698 |\n",
      "|    clip_fraction        | 0.00967     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.0316      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.55e+05    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00115    |\n",
      "|    std                  | 0.62        |\n",
      "|    value_loss           | 6.75e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 488    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 289    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 292          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014543303 |\n",
      "|    clip_fraction        | 0.00942      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.0212       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+06     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.618        |\n",
      "|    value_loss           | 1.92e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-4656.88 +/- 4431.10\n",
      "Episode length: 375.62 +/- 40.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 376          |\n",
      "|    mean_reward          | -4.66e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035071583 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.0462       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.47e+05     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    std                  | 0.63         |\n",
      "|    value_loss           | 5.65e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 489    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 297    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 300          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032694973 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.954       |\n",
      "|    explained_variance   | 0.026        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.39e+05     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.627        |\n",
      "|    value_loss           | 1.52e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-3990.46 +/- 4796.93\n",
      "Episode length: 362.12 +/- 31.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 362          |\n",
      "|    mean_reward          | -3.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046566045 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.0572       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63e+05     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 2.37e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 489    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 305    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 490         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003367383 |\n",
      "|    clip_fraction        | 0.00894     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.0917      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.59e+03    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    std                  | 0.623       |\n",
      "|    value_loss           | 4.52e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-1801.90 +/- 251.25\n",
      "Episode length: 318.12 +/- 38.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -1.8e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031245714 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.945       |\n",
      "|    explained_variance   | 0.0494       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.23e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    std                  | 0.624        |\n",
      "|    value_loss           | 9.22e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 489    |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 313    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 316          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011480502 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.944       |\n",
      "|    explained_variance   | 0.0441       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.46e+05     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.62         |\n",
      "|    value_loss           | 2.09e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-2701.65 +/- 1718.43\n",
      "Episode length: 314.75 +/- 30.89\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 315           |\n",
      "|    mean_reward          | -2.7e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 156000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096276094 |\n",
      "|    clip_fraction        | 0.00283       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.944        |\n",
      "|    explained_variance   | 0.0408        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.38e+05      |\n",
      "|    n_updates            | 760           |\n",
      "|    policy_gradient_loss | -3.91e-05     |\n",
      "|    std                  | 0.624         |\n",
      "|    value_loss           | 1.25e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 321    |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 324          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015711121 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.946       |\n",
      "|    explained_variance   | 0.0287       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25e+06     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.623        |\n",
      "|    value_loss           | 2.19e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-6012.33 +/- 5586.49\n",
      "Episode length: 347.75 +/- 43.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | -6.01e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020756316 |\n",
      "|    clip_fraction        | 0.00845      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.945       |\n",
      "|    explained_variance   | 0.032        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.51e+05     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.623        |\n",
      "|    value_loss           | 9.9e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 329    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016740577 |\n",
      "|    clip_fraction        | 0.00713      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.958       |\n",
      "|    explained_variance   | 0.0295       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    std                  | 0.638        |\n",
      "|    value_loss           | 8.07e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-11489.58 +/- 9991.05\n",
      "Episode length: 362.50 +/- 56.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 362         |\n",
      "|    mean_reward          | -1.15e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 164000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002536958 |\n",
      "|    clip_fraction        | 0.01        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.42e+04    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    std                  | 0.649       |\n",
      "|    value_loss           | 2.03e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 491    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 337    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 341          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026571953 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.982       |\n",
      "|    explained_variance   | 0.0704       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+05     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 0.643        |\n",
      "|    value_loss           | 4.53e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-11857.59 +/- 8551.87\n",
      "Episode length: 357.88 +/- 53.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | -1.19e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006935131 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.978       |\n",
      "|    explained_variance   | 0.0503       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.85e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.000585    |\n",
      "|    std                  | 0.645        |\n",
      "|    value_loss           | 1.53e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 491    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 345    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-10284.91 +/- 7816.41\n",
      "Episode length: 346.12 +/- 43.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010481676 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.976       |\n",
      "|    explained_variance   | 0.0335       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.83e+05     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.000286    |\n",
      "|    std                  | 0.641        |\n",
      "|    value_loss           | 1.63e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 350    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 491        |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 353        |\n",
      "|    total_timesteps      | 174080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00230155 |\n",
      "|    clip_fraction        | 0.00928    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.983     |\n",
      "|    explained_variance   | 0.0336     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.88e+05   |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.00107   |\n",
      "|    std                  | 0.651      |\n",
      "|    value_loss           | 1.36e+06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-5136.63 +/- 5350.87\n",
      "Episode length: 347.62 +/- 16.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 348         |\n",
      "|    mean_reward          | -5.14e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005091658 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.997      |\n",
      "|    explained_variance   | 0.0206      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.6e+05     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00414    |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 1.24e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 358    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 362          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005018028 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0164       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.41e+05     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000356    |\n",
      "|    std                  | 0.663        |\n",
      "|    value_loss           | 1.25e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-9187.31 +/- 7288.29\n",
      "Episode length: 357.50 +/- 58.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | -9.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 180000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009850382 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0266       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+05     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -6.55e-05    |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 4.55e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 367    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 371          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040225172 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0224       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.56e+05     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    std                  | 0.665        |\n",
      "|    value_loss           | 1.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-4333.40 +/- 3995.18\n",
      "Episode length: 361.38 +/- 41.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 361         |\n",
      "|    mean_reward          | -4.33e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 184000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002987334 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0431      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.8e+03     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    std                  | 0.676       |\n",
      "|    value_loss           | 5.9e+05     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 375    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 491         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004501516 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0605      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.55e+05    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 7.92e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-4990.65 +/- 4855.49\n",
      "Episode length: 335.62 +/- 49.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | -4.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044055805 |\n",
      "|    clip_fraction        | 0.0137       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    std                  | 0.682        |\n",
      "|    value_loss           | 1.54e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 490    |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 383    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 386          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024490869 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0895       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+05     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    std                  | 0.685        |\n",
      "|    value_loss           | 4.6e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-5054.77 +/- 5886.61\n",
      "Episode length: 311.12 +/- 27.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -5.05e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014855156 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0805       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.86e+05     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.682        |\n",
      "|    value_loss           | 1.06e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 491    |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 391    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 394          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005241252 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0736       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7e+05        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.000484    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 1.32e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-5127.43 +/- 5473.00\n",
      "Episode length: 351.38 +/- 61.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 351         |\n",
      "|    mean_reward          | -5.13e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602683 |\n",
      "|    clip_fraction        | 0.0192      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0405      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.37e+05    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    std                  | 0.677       |\n",
      "|    value_loss           | 1.24e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 491    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 399    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 493          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 402          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013892464 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0559       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+05     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 2.65e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-10617.36 +/- 11348.21\n",
      "Episode length: 377.88 +/- 67.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 378          |\n",
      "|    mean_reward          | -1.06e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021650349 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0804       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.49e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00246     |\n",
      "|    std                  | 0.676        |\n",
      "|    value_loss           | 5.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 407    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 493          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 411          |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023399568 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0663       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+05      |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 5.81e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-8273.79 +/- 8957.75\n",
      "Episode length: 330.62 +/- 50.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | -8.27e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 204000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014557552 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0642       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.51e+05     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 8.6e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 415    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 493           |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 418           |\n",
      "|    total_timesteps      | 206848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00062667485 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.0781        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.59e+05      |\n",
      "|    n_updates            | 1000          |\n",
      "|    policy_gradient_loss | -0.00058      |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 8.56e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-6852.67 +/- 6559.57\n",
      "Episode length: 342.25 +/- 63.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | -6.85e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015633674 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.105        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57e+05     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    std                  | 0.668        |\n",
      "|    value_loss           | 2.36e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 424    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 493          |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 427          |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007820681 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+05      |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.000709    |\n",
      "|    std                  | 0.671        |\n",
      "|    value_loss           | 1.52e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-9051.96 +/- 10276.81\n",
      "Episode length: 342.00 +/- 36.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 342         |\n",
      "|    mean_reward          | -9.05e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 212000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003186493 |\n",
      "|    clip_fraction        | 0.00728     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0817      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.66e+04    |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 2.28e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 432    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001781293 |\n",
      "|    clip_fraction        | 0.00869     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.51e+05    |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    std                  | 0.666       |\n",
      "|    value_loss           | 1.75e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-3941.26 +/- 4880.26\n",
      "Episode length: 363.50 +/- 52.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | -3.94e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 216000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008723384 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0768       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.48e+05     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.000783    |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 5.69e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 440    |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 219136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001263167 |\n",
      "|    clip_fraction        | 0.00381     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0896      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.87e+05    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    std                  | 0.666       |\n",
      "|    value_loss           | 4.4e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-7934.32 +/- 5761.53\n",
      "Episode length: 369.75 +/- 33.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 370         |\n",
      "|    mean_reward          | -7.93e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002871726 |\n",
      "|    clip_fraction        | 0.00259     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0866      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.82e+05    |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.000669   |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 6.13e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 448    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 451          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014489031 |\n",
      "|    clip_fraction        | 0.00884      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.08         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.4e+04      |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.662        |\n",
      "|    value_loss           | 2.75e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-4669.13 +/- 4655.51\n",
      "Episode length: 370.50 +/- 51.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 370          |\n",
      "|    mean_reward          | -4.67e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012191207 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.103        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.92e+05     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.000664    |\n",
      "|    std                  | 0.663        |\n",
      "|    value_loss           | 3.73e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 456    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 459          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032787735 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.32e+05     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.00259     |\n",
      "|    std                  | 0.664        |\n",
      "|    value_loss           | 3.11e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-6498.69 +/- 5315.80\n",
      "Episode length: 340.00 +/- 56.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -6.5e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 228000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011637681 |\n",
      "|    clip_fraction        | 0.00356      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.096        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.16e+05     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.662        |\n",
      "|    value_loss           | 5.31e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 464    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 468          |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024570008 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0837       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.29e+05     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    std                  | 0.667        |\n",
      "|    value_loss           | 7.51e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-9309.04 +/- 5469.27\n",
      "Episode length: 369.62 +/- 48.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 370          |\n",
      "|    mean_reward          | -9.31e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 232000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007294595 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.77e+05     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -4.67e-05    |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 3.21e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 473    |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 476          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048100958 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0927       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 8.8e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-6981.88 +/- 6221.07\n",
      "Episode length: 349.12 +/- 48.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 349           |\n",
      "|    mean_reward          | -6.98e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 236000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073935743 |\n",
      "|    clip_fraction        | 0.00205       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0.0794        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+05      |\n",
      "|    n_updates            | 1150          |\n",
      "|    policy_gradient_loss | -0.00193      |\n",
      "|    std                  | 0.682         |\n",
      "|    value_loss           | 1.32e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 481    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 484          |\n",
      "|    total_timesteps      | 239616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011244665 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0875       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.69e+04     |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.684        |\n",
      "|    value_loss           | 4.76e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-3565.31 +/- 4505.73\n",
      "Episode length: 328.38 +/- 42.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 328         |\n",
      "|    mean_reward          | -3.57e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001123968 |\n",
      "|    clip_fraction        | 0.002       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0777      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.18e+05    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    std                  | 0.686       |\n",
      "|    value_loss           | 1.2e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 488    |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 491          |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017799123 |\n",
      "|    clip_fraction        | 0.00576      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0931       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.28e+04     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 4.21e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-6995.59 +/- 4895.68\n",
      "Episode length: 325.12 +/- 49.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | -7e+03       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 244000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006709957 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0796       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+05     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00034     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 7.43e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 496    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 499          |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037326773 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0956       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.04e+05     |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 9.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-5496.51 +/- 5665.72\n",
      "Episode length: 378.38 +/- 33.26\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 378           |\n",
      "|    mean_reward          | -5.5e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 248000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076045934 |\n",
      "|    clip_fraction        | 0.00439       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0.153         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+05      |\n",
      "|    n_updates            | 1210          |\n",
      "|    policy_gradient_loss | -0.00173      |\n",
      "|    std                  | 0.689         |\n",
      "|    value_loss           | 2.07e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 504    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 507          |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022124448 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75e+04     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.000519    |\n",
      "|    std                  | 0.683        |\n",
      "|    value_loss           | 2.88e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-8905.82 +/- 8870.46\n",
      "Episode length: 352.50 +/- 39.59\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 352           |\n",
      "|    mean_reward          | -8.91e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 252000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047581297 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0.131         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+05      |\n",
      "|    n_updates            | 1230          |\n",
      "|    policy_gradient_loss | -0.000965     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 3.2e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 512    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-2083.33 +/- 351.01\n",
      "Episode length: 327.62 +/- 43.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -2.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 256000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018084734 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+04     |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    std                  | 0.682        |\n",
      "|    value_loss           | 1.15e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 517    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 520          |\n",
      "|    total_timesteps      | 258048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028072535 |\n",
      "|    clip_fraction        | 0.00547      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0985       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.58e+05     |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 0.689        |\n",
      "|    value_loss           | 1.2e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-2409.08 +/- 318.69\n",
      "Episode length: 394.38 +/- 52.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 394         |\n",
      "|    mean_reward          | -2.41e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003559545 |\n",
      "|    clip_fraction        | 0.0082      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.1e+05     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 2.37e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 525    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 495           |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 528           |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068746763 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0.0688        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+05      |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    std                  | 0.685         |\n",
      "|    value_loss           | 6.61e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-2277.61 +/- 274.86\n",
      "Episode length: 378.25 +/- 38.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | -2.28e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 264000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003226987 |\n",
      "|    clip_fraction        | 0.00752     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.278      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.77e+03    |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 3.06e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 533    |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 537          |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004741531 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0754       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.96e+05     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.000585    |\n",
      "|    std                  | 0.667        |\n",
      "|    value_loss           | 1.71e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-2108.95 +/- 377.54\n",
      "Episode length: 332.62 +/- 60.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 333          |\n",
      "|    mean_reward          | -2.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018829589 |\n",
      "|    clip_fraction        | 0.00337      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.999       |\n",
      "|    explained_variance   | -0.175       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+04     |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.647        |\n",
      "|    value_loss           | 2.67e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 542    |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 545          |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007308697 |\n",
      "|    clip_fraction        | 0.00273      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.985       |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.87e+05     |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.649        |\n",
      "|    value_loss           | 9.23e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-2418.20 +/- 238.66\n",
      "Episode length: 375.00 +/- 35.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 375          |\n",
      "|    mean_reward          | -2.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049124956 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.993       |\n",
      "|    explained_variance   | 0.0767       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+05     |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    std                  | 0.658        |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 550    |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 553          |\n",
      "|    total_timesteps      | 274432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037478849 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.991       |\n",
      "|    explained_variance   | -0.183       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.14e+03     |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    std                  | 0.649        |\n",
      "|    value_loss           | 1.8e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-3574.82 +/- 3915.65\n",
      "Episode length: 353.50 +/- 24.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | -3.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 276000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016843679 |\n",
      "|    clip_fraction        | 0.00396      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.981       |\n",
      "|    explained_variance   | -0.0454      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+04     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    std                  | 0.641        |\n",
      "|    value_loss           | 2.44e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 558    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 561          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005353647 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.974       |\n",
      "|    explained_variance   | 0.061        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.03e+05     |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.641        |\n",
      "|    value_loss           | 3.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-4856.57 +/- 5343.10\n",
      "Episode length: 339.62 +/- 34.18\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 340           |\n",
      "|    mean_reward          | -4.86e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 280000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059615355 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.972        |\n",
      "|    explained_variance   | 0.0927        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+06      |\n",
      "|    n_updates            | 1360          |\n",
      "|    policy_gradient_loss | -0.000879     |\n",
      "|    std                  | 0.639         |\n",
      "|    value_loss           | 9.09e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 566    |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 569          |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031110775 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.969       |\n",
      "|    explained_variance   | 0.0415       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.17e+04     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 6.68e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-3847.22 +/- 3111.90\n",
      "Episode length: 375.50 +/- 32.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 376          |\n",
      "|    mean_reward          | -3.85e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 284000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027671305 |\n",
      "|    clip_fraction        | 0.0062       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.963       |\n",
      "|    explained_variance   | -0.0569      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.46e+03     |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 2.22e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 574    |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 578          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003010421 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.961       |\n",
      "|    explained_variance   | 0.0953       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+05     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 3.72e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-2448.12 +/- 308.81\n",
      "Episode length: 398.25 +/- 50.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 398          |\n",
      "|    mean_reward          | -2.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 288000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046543907 |\n",
      "|    clip_fraction        | 0.00933      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.955       |\n",
      "|    explained_variance   | -0.0341      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52e+04     |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 3.01e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 583    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 495           |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 586           |\n",
      "|    total_timesteps      | 290816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064603577 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.95         |\n",
      "|    explained_variance   | 0.111         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.99e+05      |\n",
      "|    n_updates            | 1410          |\n",
      "|    policy_gradient_loss | -0.00108      |\n",
      "|    std                  | 0.626         |\n",
      "|    value_loss           | 4.18e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-2445.70 +/- 272.92\n",
      "Episode length: 367.25 +/- 44.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 367         |\n",
      "|    mean_reward          | -2.45e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 292000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005364546 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | -0.197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.76e+03    |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    std                  | 0.619       |\n",
      "|    value_loss           | 1.92e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 591    |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 594          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045556407 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.938       |\n",
      "|    explained_variance   | -0.128       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+04      |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    std                  | 0.617        |\n",
      "|    value_loss           | 1.52e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-2293.18 +/- 197.66\n",
      "Episode length: 371.62 +/- 32.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 372         |\n",
      "|    mean_reward          | -2.29e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002399098 |\n",
      "|    clip_fraction        | 0.00171     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 826         |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.000865   |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 1.83e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 599    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 602          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013499062 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | -0.514       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03e+04     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.6          |\n",
      "|    value_loss           | 1.87e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-3498.23 +/- 3674.09\n",
      "Episode length: 360.50 +/- 32.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 360          |\n",
      "|    mean_reward          | -3.5e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012738549 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.9         |\n",
      "|    explained_variance   | -0.152       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+04     |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.000575    |\n",
      "|    std                  | 0.592        |\n",
      "|    value_loss           | 1.29e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 607    |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 611          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005620125 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.894       |\n",
      "|    explained_variance   | 0.0921       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.1e+04      |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    std                  | 0.591        |\n",
      "|    value_loss           | 5.81e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-2430.89 +/- 144.76\n",
      "Episode length: 391.25 +/- 30.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 391          |\n",
      "|    mean_reward          | -2.43e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026980517 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.894       |\n",
      "|    explained_variance   | 0.0123       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.25e+04     |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    std                  | 0.591        |\n",
      "|    value_loss           | 9.63e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 617    |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 620          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025867661 |\n",
      "|    clip_fraction        | 0.00771      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.89        |\n",
      "|    explained_variance   | -0.24        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.41e+03     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.000478    |\n",
      "|    std                  | 0.587        |\n",
      "|    value_loss           | 1.63e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-3721.27 +/- 4011.55\n",
      "Episode length: 374.00 +/- 43.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 374          |\n",
      "|    mean_reward          | -3.72e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 308000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009127443 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.88        |\n",
      "|    explained_variance   | -0.141       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+04     |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.000547    |\n",
      "|    std                  | 0.579        |\n",
      "|    value_loss           | 1.76e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 625    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 628          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023599565 |\n",
      "|    clip_fraction        | 0.00464      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.871       |\n",
      "|    explained_variance   | -0.14        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09e+03     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.577        |\n",
      "|    value_loss           | 1.8e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-5586.81 +/- 4763.89\n",
      "Episode length: 317.12 +/- 45.44\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 317           |\n",
      "|    mean_reward          | -5.59e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 312000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021148374 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.869        |\n",
      "|    explained_variance   | 0.0898        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.94e+05      |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | -0.000253     |\n",
      "|    std                  | 0.578         |\n",
      "|    value_loss           | 1.59e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 633    |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 636          |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019845273 |\n",
      "|    clip_fraction        | 0.00562      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.867       |\n",
      "|    explained_variance   | -0.293       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.14e+03     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -0.000173    |\n",
      "|    std                  | 0.573        |\n",
      "|    value_loss           | 1.47e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-3574.14 +/- 3611.85\n",
      "Episode length: 336.75 +/- 49.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 337           |\n",
      "|    mean_reward          | -3.57e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 316000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045764804 |\n",
      "|    clip_fraction        | 0.00166       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.862        |\n",
      "|    explained_variance   | 0.0978        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.48e+05      |\n",
      "|    n_updates            | 1540          |\n",
      "|    policy_gradient_loss | -0.00128      |\n",
      "|    std                  | 0.573         |\n",
      "|    value_loss           | 9.93e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 642    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 645          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039740065 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.856       |\n",
      "|    explained_variance   | -0.258       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.65e+03     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    std                  | 0.566        |\n",
      "|    value_loss           | 2.12e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-6413.41 +/- 7559.39\n",
      "Episode length: 356.88 +/- 31.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 357         |\n",
      "|    mean_reward          | -6.41e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001873703 |\n",
      "|    clip_fraction        | 0.00313     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.075       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.93e+05    |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    std                  | 0.566       |\n",
      "|    value_loss           | 6.7e+05     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 650    |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 495         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 653         |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002852689 |\n",
      "|    clip_fraction        | 0.00684     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.835      |\n",
      "|    explained_variance   | -0.189      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.55e+03    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.000992   |\n",
      "|    std                  | 0.549       |\n",
      "|    value_loss           | 1.88e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-1951.55 +/- 387.26\n",
      "Episode length: 316.00 +/- 46.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | -1.95e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023911928 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.827       |\n",
      "|    explained_variance   | -0.0674      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27e+04     |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    std                  | 0.555        |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 657    |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 495           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 660           |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054299354 |\n",
      "|    clip_fraction        | 0.00156       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.831        |\n",
      "|    explained_variance   | 0.0879        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.22e+05      |\n",
      "|    n_updates            | 1590          |\n",
      "|    policy_gradient_loss | -0.00035      |\n",
      "|    std                  | 0.555         |\n",
      "|    value_loss           | 7.77e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-2227.93 +/- 256.16\n",
      "Episode length: 368.62 +/- 40.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 369          |\n",
      "|    mean_reward          | -2.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041603935 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.827       |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.24e+04     |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    std                  | 0.554        |\n",
      "|    value_loss           | 1.5e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 665    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 668          |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054600243 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.823       |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.17e+04     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    std                  | 0.548        |\n",
      "|    value_loss           | 3.2e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-2147.41 +/- 291.82\n",
      "Episode length: 350.00 +/- 52.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -2.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014392778 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.814       |\n",
      "|    explained_variance   | -0.174       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.93e+03     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | 0.000101     |\n",
      "|    std                  | 0.542        |\n",
      "|    value_loss           | 1.8e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 673    |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 676          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005495368 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.808       |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.78e+04     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.543        |\n",
      "|    value_loss           | 9.91e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-1938.62 +/- 482.69\n",
      "Episode length: 325.88 +/- 79.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 326         |\n",
      "|    mean_reward          | -1.94e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004036192 |\n",
      "|    clip_fraction        | 0.00981     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.809      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.79e+04    |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 7.34e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 496    |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 680    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 683          |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020597577 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.803       |\n",
      "|    explained_variance   | -0.197       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.27e+03     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.000785    |\n",
      "|    std                  | 0.537        |\n",
      "|    value_loss           | 1.43e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-3371.55 +/- 3452.17\n",
      "Episode length: 326.12 +/- 52.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -3.37e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013399458 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.797       |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.82e+05     |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.000577    |\n",
      "|    std                  | 0.537        |\n",
      "|    value_loss           | 1.21e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 496    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 688    |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-4565.98 +/- 5914.60\n",
      "Episode length: 348.38 +/- 45.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 348        |\n",
      "|    mean_reward          | -4.57e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 344000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00393571 |\n",
      "|    clip_fraction        | 0.0208     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.788     |\n",
      "|    explained_variance   | -0.157     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.04e+04   |\n",
      "|    n_updates            | 1670       |\n",
      "|    policy_gradient_loss | -0.00203   |\n",
      "|    std                  | 0.525      |\n",
      "|    value_loss           | 1.85e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 496    |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 693    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 696          |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042959787 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.772       |\n",
      "|    explained_variance   | -0.0701      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.03e+03     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.000166    |\n",
      "|    std                  | 0.522        |\n",
      "|    value_loss           | 1.34e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-5926.19 +/- 7032.06\n",
      "Episode length: 343.88 +/- 55.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -5.93e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023657316 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.771       |\n",
      "|    explained_variance   | -0.0695      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.52e+03     |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    std                  | 0.523        |\n",
      "|    value_loss           | 1.6e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 496    |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 701    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 496           |\n",
      "|    iterations           | 171           |\n",
      "|    time_elapsed         | 704           |\n",
      "|    total_timesteps      | 350208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033799798 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.77         |\n",
      "|    explained_variance   | 0.108         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.18e+05      |\n",
      "|    n_updates            | 1700          |\n",
      "|    policy_gradient_loss | -0.000482     |\n",
      "|    std                  | 0.523         |\n",
      "|    value_loss           | 1.29e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-4713.13 +/- 6373.02\n",
      "Episode length: 390.38 +/- 26.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 390          |\n",
      "|    mean_reward          | -4.71e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 352000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037102127 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.767       |\n",
      "|    explained_variance   | -0.14        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.58e+03     |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.518        |\n",
      "|    value_loss           | 1.79e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 710    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 713          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029616482 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.755       |\n",
      "|    explained_variance   | -0.101       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.67e+03     |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000658    |\n",
      "|    std                  | 0.513        |\n",
      "|    value_loss           | 1.33e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-1835.61 +/- 293.97\n",
      "Episode length: 312.00 +/- 52.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -1.84e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 356000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011114618 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.75        |\n",
      "|    explained_variance   | -0.053       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+04     |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.000985    |\n",
      "|    std                  | 0.513        |\n",
      "|    value_loss           | 1.74e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 718    |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 175          |\n",
      "|    time_elapsed         | 722          |\n",
      "|    total_timesteps      | 358400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014187354 |\n",
      "|    clip_fraction        | 0.00771      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.744       |\n",
      "|    explained_variance   | -0.0842      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08e+03     |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.000133    |\n",
      "|    std                  | 0.504        |\n",
      "|    value_loss           | 1.1e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-2198.88 +/- 576.21\n",
      "Episode length: 343.62 +/- 52.69\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 344           |\n",
      "|    mean_reward          | -2.2e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 360000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048117182 |\n",
      "|    clip_fraction        | 0.00591       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.734        |\n",
      "|    explained_variance   | -0.0666       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+04       |\n",
      "|    n_updates            | 1750          |\n",
      "|    policy_gradient_loss | -0.00111      |\n",
      "|    std                  | 0.503         |\n",
      "|    value_loss           | 2.83e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 495    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 727    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 495        |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 731        |\n",
      "|    total_timesteps      | 362496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00176874 |\n",
      "|    clip_fraction        | 0.00283    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.726     |\n",
      "|    explained_variance   | -0.0127    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.94e+03   |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.000405  |\n",
      "|    std                  | 0.497      |\n",
      "|    value_loss           | 2.79e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-4447.08 +/- 5660.64\n",
      "Episode length: 337.00 +/- 52.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 337          |\n",
      "|    mean_reward          | -4.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 364000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011877028 |\n",
      "|    clip_fraction        | 0.00752      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.721       |\n",
      "|    explained_variance   | 0.0455       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+04     |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | -0.000757    |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 4.84e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 736    |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 495         |\n",
      "|    iterations           | 179         |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 366592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005727408 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.718      |\n",
      "|    explained_variance   | 0.0297      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.03e+04    |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.00236    |\n",
      "|    std                  | 0.496       |\n",
      "|    value_loss           | 1.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-1913.35 +/- 325.94\n",
      "Episode length: 316.50 +/- 52.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | -1.91e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 368000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004121308 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | -0.0323     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.01e+03    |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    std                  | 0.485       |\n",
      "|    value_loss           | 1.28e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 745    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 749          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077681276 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.695       |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+04     |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00304     |\n",
      "|    std                  | 0.485        |\n",
      "|    value_loss           | 1.92e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-2938.67 +/- 2966.51\n",
      "Episode length: 331.88 +/- 57.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -2.94e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 372000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006026262 |\n",
      "|    clip_fraction        | 0.00352      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.695       |\n",
      "|    explained_variance   | 0.0608       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.56e+04     |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.485        |\n",
      "|    value_loss           | 1.78e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 754    |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 758         |\n",
      "|    total_timesteps      | 374784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004041197 |\n",
      "|    clip_fraction        | 0.00791     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.04e+04    |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    std                  | 0.48        |\n",
      "|    value_loss           | 2.35e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-8763.48 +/- 9169.93\n",
      "Episode length: 343.25 +/- 31.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | -8.76e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 376000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042034965 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.681       |\n",
      "|    explained_variance   | -0.0729      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.01e+03     |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.476        |\n",
      "|    value_loss           | 1.24e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 763    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 185         |\n",
      "|    time_elapsed         | 767         |\n",
      "|    total_timesteps      | 378880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006328241 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | -0.0522     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.31e+04    |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    std                  | 0.465       |\n",
      "|    value_loss           | 1.21e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-4862.49 +/- 7936.85\n",
      "Episode length: 349.88 +/- 34.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | -4.86e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011227904 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.654       |\n",
      "|    explained_variance   | 0.0594       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.22e+05     |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 0.465        |\n",
      "|    value_loss           | 1.05e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 772    |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 493          |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 775          |\n",
      "|    total_timesteps      | 382976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024821272 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.655       |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.47e+04     |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | 1.73e-06     |\n",
      "|    std                  | 0.466        |\n",
      "|    value_loss           | 2.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-3094.80 +/- 3509.69\n",
      "Episode length: 327.38 +/- 53.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -3.09e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019970133 |\n",
      "|    clip_fraction        | 0.00557      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.657       |\n",
      "|    explained_variance   | 0.105        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.37e+03     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.000851    |\n",
      "|    std                  | 0.468        |\n",
      "|    value_loss           | 7.48e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 780    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 783         |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006616854 |\n",
      "|    clip_fraction        | 0.0213      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.95e+05    |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.00258    |\n",
      "|    std                  | 0.47        |\n",
      "|    value_loss           | 2.12e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-2048.11 +/- 367.82\n",
      "Episode length: 356.38 +/- 64.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | -2.05e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 388000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010943844 |\n",
      "|    clip_fraction        | 0.00947      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.659       |\n",
      "|    explained_variance   | -0.0518      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+03     |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.000497    |\n",
      "|    std                  | 0.464        |\n",
      "|    value_loss           | 1.74e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 788    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 791          |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038695594 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.649       |\n",
      "|    explained_variance   | -0.219       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.13e+03     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.000315    |\n",
      "|    std                  | 0.462        |\n",
      "|    value_loss           | 1.32e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-5727.54 +/- 6660.33\n",
      "Episode length: 346.25 +/- 50.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | -5.73e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 392000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016062965 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.64        |\n",
      "|    explained_variance   | -0.0504      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.73e+03     |\n",
      "|    n_updates            | 1910         |\n",
      "|    policy_gradient_loss | -0.000862    |\n",
      "|    std                  | 0.455        |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 493    |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 796    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 193          |\n",
      "|    time_elapsed         | 799          |\n",
      "|    total_timesteps      | 395264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007344126 |\n",
      "|    clip_fraction        | 0.0505       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.633       |\n",
      "|    explained_variance   | -0.0398      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.77e+03     |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.000147    |\n",
      "|    std                  | 0.457        |\n",
      "|    value_loss           | 1.02e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-5539.01 +/- 6786.12\n",
      "Episode length: 319.50 +/- 43.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -5.54e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 396000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046977075 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.633       |\n",
      "|    explained_variance   | 0.0493       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+04     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    std                  | 0.454        |\n",
      "|    value_loss           | 2.11e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 803    |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 807          |\n",
      "|    total_timesteps      | 399360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019379316 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | -0.0405      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.53e+03     |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.438        |\n",
      "|    value_loss           | 8.63e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-1943.13 +/- 299.96\n",
      "Episode length: 337.62 +/- 49.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 338         |\n",
      "|    mean_reward          | -1.94e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004776218 |\n",
      "|    clip_fraction        | 0.00986     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | -0.0619     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.25e+04    |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    std                  | 0.432       |\n",
      "|    value_loss           | 1.65e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 494    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 811    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x2b734f9f250>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "model.set_parameters(\"best_model\")\n",
    "model.learn(\n",
    "    total_timesteps=400000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T07:57:43.962833300Z",
     "start_time": "2024-05-15T07:44:10.681990300Z"
    }
   },
   "id": "22172852ef6517bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 绘图检验"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64de305199a42f8d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model.save(\"last_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T07:58:49.439305500Z",
     "start_time": "2024-05-15T07:58:49.418297100Z"
    }
   },
   "id": "44d8e7b741796641"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     35\u001B[0m action, _states \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(state, deterministic\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 36\u001B[0m next_state, reward, done, t, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m#if reward < -10:\u001B[39;00m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;66;03m#print(state, action, reward)\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m500\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\my_UAV\\study\\move obstacle random target\\env_test1.py:70\u001B[0m, in \u001B[0;36mDroneEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobstacles:\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m(i[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m600\u001B[39m):\n\u001B[1;32m---> 70\u001B[0m         \u001B[43mi\u001B[49m[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m         i[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n",
      "File \u001B[1;32m~\\Desktop\\my_UAV\\study\\move obstacle random target\\env_test1.py:70\u001B[0m, in \u001B[0;36mDroneEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobstacles:\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m(i[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m600\u001B[39m):\n\u001B[1;32m---> 70\u001B[0m         \u001B[43mi\u001B[49m[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m         i[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.2.5\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.2.5\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2000x800 with 10 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个包含10个子图的窗口\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axs = axs.ravel()  # 将二维数组展平，方便通过索引访问每个子图\n",
    "\n",
    "env = env_test1.DroneEnv()\n",
    "for i in range(10):\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    trajectory_x = [env.xy_p[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_y = [env.xy_p[1]]  # 存储无人机路径的y坐标\n",
    "    trajectory_ex = [env.xy_e[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_ey = [env.xy_e[1]]  # 存储无人机路径的y坐标\n",
    "    axs[i].set_xlim(env.space1.low[0], env.space1.high[0])\n",
    "    axs[i].set_ylim(env.space1.low[1], env.space1.high[1])\n",
    "    axs[i].set_xlabel('X')\n",
    "    axs[i].set_ylabel('Y')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Episode {i+1}')\n",
    "    # 通过预训练模型控制无人机执行任务并绘制路径\n",
    "    model = PPO.load(\"last_model\") \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "       \n",
    "        count += 1\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        next_state, reward, done, t, info = env.step(action)\n",
    "        #if reward < -10:\n",
    "            #print(state, action, reward)\n",
    "        if count > 500:\n",
    "            done = True\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory_x.append(env.xy_p[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_y.append(env.xy_p[1])  # 更新无人机路径的y坐标\n",
    "        trajectory_ex.append(env.xy_e[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_ey.append(env.xy_e[1])  # 更新无人机路径的y坐标\n",
    "\n",
    "    # 绘制无人机路径\n",
    "    axs[i].plot(trajectory_x, trajectory_y, color='red', linewidth=0.5)\n",
    "    axs[i].plot(trajectory_ex, trajectory_ey, color='red', linewidth=0.5)\n",
    "\n",
    "    # 打印每个episode的总奖励\n",
    "    print(f'Episode {i+1} total reward:', total_reward)\n",
    "\n",
    "# 显示子图窗口\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-14T12:29:07.924594400Z"
    }
   },
   "id": "85b9e88968d1a4a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 47\u001B[0m\n\u001B[0;32m     44\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# 动态更新障碍物的可视化\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m \u001B[43mdraw_obstacles\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobstacles\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m500\u001B[39m:\n\u001B[0;32m     50\u001B[0m     done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[7], line 25\u001B[0m, in \u001B[0;36mdraw_obstacles\u001B[1;34m(obstacles)\u001B[0m\n\u001B[0;32m     22\u001B[0m plt\u001B[38;5;241m.\u001B[39mlegend()\n\u001B[0;32m     23\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDrone Environment\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\pyplot.py:1017\u001B[0m, in \u001B[0;36mdraw\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1000\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdraw\u001B[39m():\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m \u001B[38;5;124;03m    Redraw the current figure.\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;124;03m    .FigureCanvasBase.draw\u001B[39;00m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1017\u001B[0m     \u001B[43mgcf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\backend_bases.py:2082\u001B[0m, in \u001B[0;36mFigureCanvasBase.draw_idle\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2080\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_idle_drawing:\n\u001B[0;32m   2081\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_idle_draw_cntx():\n\u001B[1;32m-> 2082\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:400\u001B[0m, in \u001B[0;36mFigureCanvasAgg.draw\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;66;03m# Acquire a lock on the shared font cache.\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m RendererAgg\u001B[38;5;241m.\u001B[39mlock, \\\n\u001B[0;32m    398\u001B[0m      (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoolbar\u001B[38;5;241m.\u001B[39m_wait_cursor_for_draw_cm() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoolbar\n\u001B[0;32m    399\u001B[0m       \u001B[38;5;28;01melse\u001B[39;00m nullcontext()):\n\u001B[1;32m--> 400\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfigure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001B[39;00m\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;66;03m# don't forget to call the superclass.\u001B[39;00m\n\u001B[0;32m    403\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mdraw()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\artist.py:95\u001B[0m, in \u001B[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001B[1;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(draw)\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdraw_wrapper\u001B[39m(artist, renderer, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 95\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m renderer\u001B[38;5;241m.\u001B[39m_rasterizing:\n\u001B[0;32m     97\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstop_rasterizing()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[1;34m(artist, renderer)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[1;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\figure.py:3175\u001B[0m, in \u001B[0;36mFigure.draw\u001B[1;34m(self, renderer)\u001B[0m\n\u001B[0;32m   3172\u001B[0m         \u001B[38;5;66;03m# ValueError can occur when resizing a window.\u001B[39;00m\n\u001B[0;32m   3174\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch\u001B[38;5;241m.\u001B[39mdraw(renderer)\n\u001B[1;32m-> 3175\u001B[0m \u001B[43mmimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_draw_list_compositing_images\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3176\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43martists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuppressComposite\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sfig \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubfigs:\n\u001B[0;32m   3179\u001B[0m     sfig\u001B[38;5;241m.\u001B[39mdraw(renderer)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\image.py:131\u001B[0m, in \u001B[0;36m_draw_list_compositing_images\u001B[1;34m(renderer, parent, artists, suppress_composite)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_composite \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_images:\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m artists:\n\u001B[1;32m--> 131\u001B[0m         \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;66;03m# Composite any adjacent images together\u001B[39;00m\n\u001B[0;32m    134\u001B[0m     image_group \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[1;34m(artist, renderer)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[1;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\axes\\_base.py:3064\u001B[0m, in \u001B[0;36m_AxesBase.draw\u001B[1;34m(self, renderer)\u001B[0m\n\u001B[0;32m   3061\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m artists_rasterized:\n\u001B[0;32m   3062\u001B[0m     _draw_rasterized(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure, artists_rasterized, renderer)\n\u001B[1;32m-> 3064\u001B[0m \u001B[43mmimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_draw_list_compositing_images\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3065\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43martists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfigure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuppressComposite\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3067\u001B[0m renderer\u001B[38;5;241m.\u001B[39mclose_group(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maxes\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   3068\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\image.py:131\u001B[0m, in \u001B[0;36m_draw_list_compositing_images\u001B[1;34m(renderer, parent, artists, suppress_composite)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_composite \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_images:\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m artists:\n\u001B[1;32m--> 131\u001B[0m         \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;66;03m# Composite any adjacent images together\u001B[39;00m\n\u001B[0;32m    134\u001B[0m     image_group \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[1;34m(artist, renderer)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[1;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\legend.py:734\u001B[0m, in \u001B[0;36mLegend.draw\u001B[1;34m(self, renderer)\u001B[0m\n\u001B[0;32m    731\u001B[0m     Shadow(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlegendPatch, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mdraw(renderer)\n\u001B[0;32m    733\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlegendPatch\u001B[38;5;241m.\u001B[39mdraw(renderer)\n\u001B[1;32m--> 734\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_legend_box\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    736\u001B[0m renderer\u001B[38;5;241m.\u001B[39mclose_group(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlegend\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\artist.py:39\u001B[0m, in \u001B[0;36m_prevent_rasterization.<locals>.draw_wrapper\u001B[1;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[0;32m     36\u001B[0m     renderer\u001B[38;5;241m.\u001B[39mstop_rasterizing()\n\u001B[0;32m     37\u001B[0m     renderer\u001B[38;5;241m.\u001B[39m_rasterizing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\offsetbox.py:413\u001B[0m, in \u001B[0;36mOffsetBox.draw\u001B[1;34m(self, renderer)\u001B[0m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    409\u001B[0m \u001B[38;5;124;03mUpdate the location of children if necessary and draw them\u001B[39;00m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;124;03mto the given *renderer*.\u001B[39;00m\n\u001B[0;32m    411\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    412\u001B[0m bbox, offsets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_bbox_and_child_offsets(renderer)\n\u001B[1;32m--> 413\u001B[0m px, py \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_offset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c, (ox, oy) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_visible_children(), offsets):\n\u001B[0;32m    415\u001B[0m     c\u001B[38;5;241m.\u001B[39mset_offset((px \u001B[38;5;241m+\u001B[39m ox, py \u001B[38;5;241m+\u001B[39m oy))\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\offsetbox.py:60\u001B[0m, in \u001B[0;36m_compat_get_offset.<locals>.get_offset\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     56\u001B[0m params \u001B[38;5;241m=\u001B[39m _api\u001B[38;5;241m.\u001B[39mselect_matching_signature(sigs, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     57\u001B[0m bbox \u001B[38;5;241m=\u001B[39m (params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m params \u001B[38;5;28;01melse\u001B[39;00m\n\u001B[0;32m     58\u001B[0m         Bbox\u001B[38;5;241m.\u001B[39mfrom_bounds(\u001B[38;5;241m-\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxdescent\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m-\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mydescent\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     59\u001B[0m                          params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m], params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mself\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrenderer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\offsetbox.py:313\u001B[0m, in \u001B[0;36mOffsetBox.get_offset\u001B[1;34m(self, bbox, renderer)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;129m@_compat_get_offset\u001B[39m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_offset\u001B[39m(\u001B[38;5;28mself\u001B[39m, bbox, renderer):\n\u001B[0;32m    300\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;124;03m    Return the offset as a tuple (x, y).\u001B[39;00m\n\u001B[0;32m    302\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;124;03m    renderer : `.RendererBase` subclass\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 313\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_offset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_offset)\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_offset)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\legend.py:695\u001B[0m, in \u001B[0;36mLegend._findoffset\u001B[1;34m(self, width, height, xdescent, ydescent, renderer)\u001B[0m\n\u001B[0;32m    692\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Helper function to locate the legend.\"\"\"\u001B[39;00m\n\u001B[0;32m    694\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loc \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:  \u001B[38;5;66;03m# \"best\".\u001B[39;00m\n\u001B[1;32m--> 695\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_find_best_position\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loc \u001B[38;5;129;01min\u001B[39;00m Legend\u001B[38;5;241m.\u001B[39mcodes\u001B[38;5;241m.\u001B[39mvalues():  \u001B[38;5;66;03m# Fixed location.\u001B[39;00m\n\u001B[0;32m    697\u001B[0m     bbox \u001B[38;5;241m=\u001B[39m Bbox\u001B[38;5;241m.\u001B[39mfrom_bounds(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, width, height)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\legend.py:1144\u001B[0m, in \u001B[0;36mLegend._find_best_position\u001B[1;34m(self, width, height, renderer, consider)\u001B[0m\n\u001B[0;32m   1137\u001B[0m badness \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1138\u001B[0m \u001B[38;5;66;03m# XXX TODO: If markers are present, it would be good to take them\u001B[39;00m\n\u001B[0;32m   1139\u001B[0m \u001B[38;5;66;03m# into account when checking vertex overlaps in the next line.\u001B[39;00m\n\u001B[0;32m   1140\u001B[0m badness \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28msum\u001B[39m(legendBox\u001B[38;5;241m.\u001B[39mcount_contains(line\u001B[38;5;241m.\u001B[39mvertices)\n\u001B[0;32m   1141\u001B[0m                \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines)\n\u001B[0;32m   1142\u001B[0m            \u001B[38;5;241m+\u001B[39m legendBox\u001B[38;5;241m.\u001B[39mcount_contains(offsets)\n\u001B[0;32m   1143\u001B[0m            \u001B[38;5;241m+\u001B[39m legendBox\u001B[38;5;241m.\u001B[39mcount_overlaps(bboxes)\n\u001B[1;32m-> 1144\u001B[0m            \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintersects_bbox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlegendBox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlines\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m badness \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m l, b\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\legend.py:1144\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1137\u001B[0m badness \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1138\u001B[0m \u001B[38;5;66;03m# XXX TODO: If markers are present, it would be good to take them\u001B[39;00m\n\u001B[0;32m   1139\u001B[0m \u001B[38;5;66;03m# into account when checking vertex overlaps in the next line.\u001B[39;00m\n\u001B[0;32m   1140\u001B[0m badness \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28msum\u001B[39m(legendBox\u001B[38;5;241m.\u001B[39mcount_contains(line\u001B[38;5;241m.\u001B[39mvertices)\n\u001B[0;32m   1141\u001B[0m                \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines)\n\u001B[0;32m   1142\u001B[0m            \u001B[38;5;241m+\u001B[39m legendBox\u001B[38;5;241m.\u001B[39mcount_contains(offsets)\n\u001B[0;32m   1143\u001B[0m            \u001B[38;5;241m+\u001B[39m legendBox\u001B[38;5;241m.\u001B[39mcount_overlaps(bboxes)\n\u001B[1;32m-> 1144\u001B[0m            \u001B[38;5;241m+\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[43mline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintersects_bbox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlegendBox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1145\u001B[0m                  \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines))\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m badness \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m l, b\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\matplotlib\\path.py:662\u001B[0m, in \u001B[0;36mPath.intersects_bbox\u001B[1;34m(self, bbox, filled)\u001B[0m\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mintersects_bbox\u001B[39m(\u001B[38;5;28mself\u001B[39m, bbox, filled\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    654\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    655\u001B[0m \u001B[38;5;124;03m    Return whether this path intersects a given `~.transforms.Bbox`.\u001B[39;00m\n\u001B[0;32m    656\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    660\u001B[0m \u001B[38;5;124;03m    The bounding box is always considered filled.\u001B[39;00m\n\u001B[0;32m    661\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 662\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_intersects_rectangle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilled\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 假设env.obstacles返回当前时间步的障碍物位置列表\n",
    "\n",
    "plt.ion()  # 开启交互模式，允许动态更新图形\n",
    "fig, ax = plt.subplots()\n",
    "circles = []  # 存储障碍物的圆形对象\n",
    "\n",
    "def draw_obstacles(obstacles):\n",
    "    # 清除之前的障碍物绘制\n",
    "    for circle in circles:\n",
    "        ax.add_patch(circle)\n",
    "    plt.draw()\n",
    "\n",
    "    # 重新绘制障碍物\n",
    "    for k in obstacles:\n",
    "        circle = plt.Circle(k, env.r_obstacles, color='lightblue', fill=True)\n",
    "        ax.add_patch(circle)\n",
    "        circles.append(circle)\n",
    "    plt.xlim(env.space1.low[0], env.space1.high[0])\n",
    "    plt.ylim(env.space1.low[1], env.space1.high[1])\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.title('Drone Environment')\n",
    "\n",
    "    plt.draw()  # 更新图形\n",
    "\n",
    "env = env_test1.DroneEnv()\n",
    "model = PPO.load(\"last_model\") \n",
    "done = False\n",
    "r = 0\n",
    "state, info = env.reset()\n",
    "\n",
    "# 绘制初始障碍物\n",
    "draw_obstacles(env.obstacles)\n",
    "\n",
    "count = 0\n",
    "while not done:\n",
    "    count += 1\n",
    "    print(count)\n",
    "\n",
    "    action, _states = model.predict(state, deterministic=True)\n",
    "    next_state, reward, done, t, info = env.step(action)\n",
    "    r += reward\n",
    "    state = next_state\n",
    "\n",
    "    # 动态更新障碍物的可视化\n",
    "    draw_obstacles(env.obstacles)\n",
    "\n",
    "    if count > 500:\n",
    "        done = True\n",
    "\n",
    "plt.ioff()  # 关闭交互模式\n",
    "plt.show()  # 显示最终的绘图结果"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-14T12:53:58.365804300Z"
    }
   },
   "id": "1f53ae7c7aa06a89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d265033f535bd507"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
