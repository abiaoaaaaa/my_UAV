{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T06:23:33.537556300Z",
     "start_time": "2024-04-18T06:23:28.940991300Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env_test1\n",
    "env = env_test1.DroneEnv()\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c32894c611bf66",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2cd539b-67c5-4dc7-988b-6963b7385b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation callback\n",
    "callbacks = []\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=8,\n",
    "    best_model_save_path=\".\",\n",
    "    log_path=\".\",\n",
    "    eval_freq=4000,\n",
    ")\n",
    "\n",
    "callbacks.append(eval_callback)\n",
    "kwargs = {}\n",
    "kwargs[\"callback\"] = callbacks\n",
    "\n",
    "log_name = \"ppo_run_\" + str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6364eb784437bc7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T06:23:33.541022200Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard/ppo_run_1713687832.6596043_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 817  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=-10835.17 +/- 9321.84\n",
      "Episode length: 326.25 +/- 23.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -1.08e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029642964 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.26e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.96e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.47e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 514  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019682671 |\n",
      "|    clip_fraction        | 0.00464      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.000147    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.78e+04     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 2.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-9893.71 +/- 11594.84\n",
      "Episode length: 315.62 +/- 31.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | -9.89e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051553133 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -2.43e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.92e+04     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 9.18e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 487  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 503           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035032988 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.21e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000522     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.45e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-9405.71 +/- 9162.89\n",
      "Episode length: 311.12 +/- 57.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -9.41e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022548572 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 2.52e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.18e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 3.23e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 481   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036205414 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 3.31e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.32e+04     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00378     |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 1.17e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-15252.01 +/- 11600.48\n",
      "Episode length: 322.88 +/- 39.08\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -1.53e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 16000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012638094 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 7.93e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.2e+06       |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000355     |\n",
      "|    std                  | 0.96          |\n",
      "|    value_loss           | 3.14e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.708814e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 9.18e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.56e+05     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000488    |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 1.56e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-7832.35 +/- 6737.75\n",
      "Episode length: 308.00 +/- 19.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | -7.83e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011396175 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.000102   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 2.41e+04    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 488           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 46            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018408825 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 4.17e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.55e+06      |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.00082      |\n",
      "|    std                  | 0.926         |\n",
      "|    value_loss           | 2.54e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-5568.49 +/- 4689.38\n",
      "Episode length: 295.00 +/- 41.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | -5.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006882199 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 3.64e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.46e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.948        |\n",
      "|    value_loss           | 1.42e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 54         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00077356 |\n",
      "|    clip_fraction        | 0.000391   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 2.15e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.58e+05   |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00141   |\n",
      "|    std                  | 0.939      |\n",
      "|    value_loss           | 1.5e+06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-4482.65 +/- 6473.99\n",
      "Episode length: 304.00 +/- 54.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 304         |\n",
      "|    mean_reward          | -4.48e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002663623 |\n",
      "|    clip_fraction        | 0.00381     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 1.73e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+05    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.513277e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.91e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+06     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    std                  | 0.928        |\n",
      "|    value_loss           | 2.33e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-4481.26 +/- 5169.56\n",
      "Episode length: 295.25 +/- 50.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | -4.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.958303e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.31e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.18e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00017     |\n",
      "|    std                  | 0.931        |\n",
      "|    value_loss           | 2.02e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012509142 |\n",
      "|    clip_fraction        | 0.00112      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.27e+06     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.911        |\n",
      "|    value_loss           | 2.11e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-2940.11 +/- 3271.43\n",
      "Episode length: 272.62 +/- 34.99\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 273           |\n",
      "|    mean_reward          | -2.94e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 36000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0794373e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.33         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.73e+05      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000192     |\n",
      "|    std                  | 0.911         |\n",
      "|    value_loss           | 3.28e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 480   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-7452.87 +/- 6283.88\n",
      "Episode length: 277.62 +/- 49.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 278         |\n",
      "|    mean_reward          | -7.45e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011675134 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 1.67e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.876       |\n",
      "|    value_loss           | 1.2e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 480           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 106           |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070773734 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.67e+05      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.00204      |\n",
      "|    std                  | 0.871         |\n",
      "|    value_loss           | 2.08e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-4801.48 +/- 4996.01\n",
      "Episode length: 304.38 +/- 47.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 304          |\n",
      "|    mean_reward          | -4.8e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032076808 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11e+04     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.871        |\n",
      "|    value_loss           | 9.98e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 111   |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023766654 |\n",
      "|    clip_fraction        | 0.00273      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08e+05     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000599    |\n",
      "|    std                  | 0.861        |\n",
      "|    value_loss           | 6.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-4748.83 +/- 6275.80\n",
      "Episode length: 296.38 +/- 24.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 296          |\n",
      "|    mean_reward          | -4.75e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009095837 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+06     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000842    |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 4.05e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 120   |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025086985 |\n",
      "|    clip_fraction        | 0.00488      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72e+06     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 2.2e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-6194.54 +/- 6429.77\n",
      "Episode length: 326.75 +/- 38.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 327         |\n",
      "|    mean_reward          | -6.19e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004128187 |\n",
      "|    clip_fraction        | 0.00801     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.81e+05    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000745   |\n",
      "|    std                  | 0.857       |\n",
      "|    value_loss           | 1.45e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 128   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032602432 |\n",
      "|    clip_fraction        | 0.00996      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+06     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    std                  | 0.858        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-5005.86 +/- 6512.74\n",
      "Episode length: 307.50 +/- 41.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | -5.01e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004895909 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.04e+05     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00043     |\n",
      "|    std                  | 0.858        |\n",
      "|    value_loss           | 1.04e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 137   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00592896 |\n",
      "|    clip_fraction        | 0.0385     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.39e+05   |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00343   |\n",
      "|    std                  | 0.846      |\n",
      "|    value_loss           | 5.14e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-2744.57 +/- 3510.51\n",
      "Episode length: 300.25 +/- 50.93\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 300           |\n",
      "|    mean_reward          | -2.74e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 68000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033695478 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.25         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.9e+05       |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00105      |\n",
      "|    std                  | 0.849         |\n",
      "|    value_loss           | 2.37e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 145   |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002645306 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71e+05    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000438   |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 4.45e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-14222.48 +/- 12429.50\n",
      "Episode length: 338.50 +/- 36.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 338          |\n",
      "|    mean_reward          | -1.42e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042363526 |\n",
      "|    clip_fraction        | 0.00928      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+05     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 8.8e+05      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 154   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002177515 |\n",
      "|    clip_fraction        | 0.00288     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.33e+05    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 7.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-2717.48 +/- 2300.58\n",
      "Episode length: 305.50 +/- 77.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | -2.72e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000860849 |\n",
      "|    clip_fraction        | 0.000195    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.19e+06    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.000855   |\n",
      "|    std                  | 0.84        |\n",
      "|    value_loss           | 2.46e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054182815 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.3e+04      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    std                  | 0.843        |\n",
      "|    value_loss           | 1.67e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-3534.54 +/- 2550.11\n",
      "Episode length: 311.50 +/- 42.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -3.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021265207 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.52e+05     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 172   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006487474 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.5e+05     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0029     |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 8.47e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-9585.38 +/- 12697.34\n",
      "Episode length: 310.62 +/- 44.92\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 311           |\n",
      "|    mean_reward          | -9.59e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 84000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060662813 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.25         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.11e+05      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000111     |\n",
      "|    std                  | 0.847         |\n",
      "|    value_loss           | 2.85e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 475   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 180   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-7147.57 +/- 7431.32\n",
      "Episode length: 318.50 +/- 50.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -7.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061098724 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18e+05     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    std                  | 0.834        |\n",
      "|    value_loss           | 1.29e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 185   |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010241703 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+06     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.836        |\n",
      "|    value_loss           | 1.44e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-9868.51 +/- 8733.03\n",
      "Episode length: 325.62 +/- 46.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -9.87e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040867487 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.47e+05     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.836        |\n",
      "|    value_loss           | 8.44e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 194   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067133605 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.24e+06     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    std                  | 0.835        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-9468.72 +/- 8086.48\n",
      "Episode length: 324.50 +/- 48.12\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 324           |\n",
      "|    mean_reward          | -9.47e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 96000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049809995 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.24         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.35e+06      |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    std                  | 0.832         |\n",
      "|    value_loss           | 2.63e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 203   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 206          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029520364 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.89e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    std                  | 0.828        |\n",
      "|    value_loss           | 2.59e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-5456.12 +/- 5755.34\n",
      "Episode length: 342.75 +/- 35.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 343         |\n",
      "|    mean_reward          | -5.46e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008011256 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22e+06    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 1.51e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 474    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 211    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002263059 |\n",
      "|    clip_fraction        | 0.00957     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35e+05    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.000524   |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 2.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-15059.54 +/- 8463.57\n",
      "Episode length: 294.62 +/- 46.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | -1.51e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001040586 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.3e+05     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000277   |\n",
      "|    std                  | 0.805       |\n",
      "|    value_loss           | 1.21e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 480    |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 217    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 219          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015816229 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64e+06     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    std                  | 0.803        |\n",
      "|    value_loss           | 2.57e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-9461.25 +/- 6898.32\n",
      "Episode length: 316.88 +/- 37.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | -9.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010044618 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+06     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    std                  | 0.8          |\n",
      "|    value_loss           | 1.25e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 486    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 223    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 225          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064495634 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.792        |\n",
      "|    value_loss           | 7.44e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-3042.31 +/- 4124.76\n",
      "Episode length: 307.00 +/- 67.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -3.04e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037936212 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    std                  | 0.794        |\n",
      "|    value_loss           | 2.15e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 228    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 496         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008169918 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51e+03    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 6.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-9992.32 +/- 9333.05\n",
      "Episode length: 328.38 +/- 35.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -9.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022639134 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.43e+03     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 9.25e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 498    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 234    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 236          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039139865 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.16e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 0.758        |\n",
      "|    value_loss           | 1.13e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-8277.49 +/- 6988.22\n",
      "Episode length: 321.75 +/- 45.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -8.28e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024414326 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+05     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 0.757        |\n",
      "|    value_loss           | 5.05e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 503    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 240    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 242          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008507621 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.86e+05     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000867    |\n",
      "|    std                  | 0.758        |\n",
      "|    value_loss           | 1.63e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-6649.90 +/- 7834.69\n",
      "Episode length: 327.12 +/- 34.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -6.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023046248 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.96e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00091     |\n",
      "|    std                  | 0.762        |\n",
      "|    value_loss           | 6.44e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 507    |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 245    |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044672783 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+03     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    std                  | 0.734        |\n",
      "|    value_loss           | 6.01e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-4656.59 +/- 5426.40\n",
      "Episode length: 336.38 +/- 32.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 336         |\n",
      "|    mean_reward          | -4.66e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004483409 |\n",
      "|    clip_fraction        | 0.0123      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.06e+04    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    std                  | 0.734       |\n",
      "|    value_loss           | 8.76e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 512    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 251    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 515           |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 254           |\n",
      "|    total_timesteps      | 131072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015887554 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.24e+05      |\n",
      "|    n_updates            | 630           |\n",
      "|    policy_gradient_loss | -0.00027      |\n",
      "|    std                  | 0.733         |\n",
      "|    value_loss           | 1.81e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-5893.15 +/- 7859.29\n",
      "Episode length: 324.00 +/- 38.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | -5.89e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 132000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002590755 |\n",
      "|    clip_fraction        | 0.00264     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.14e+04    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    std                  | 0.729       |\n",
      "|    value_loss           | 5.57e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 517    |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 257    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017305536 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.19e+03    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00499     |\n",
      "|    std                  | 0.737       |\n",
      "|    value_loss           | 5.4e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-9234.84 +/- 9807.40\n",
      "Episode length: 335.88 +/- 60.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | -9.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007086221 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.04e+04     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000761    |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 4.98e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 520    |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 263    |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 523           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 265           |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.2756055e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.71e+05      |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.000671     |\n",
      "|    std                  | 0.738         |\n",
      "|    value_loss           | 1.84e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-11819.57 +/- 10237.29\n",
      "Episode length: 341.00 +/- 38.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | -1.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045081833 |\n",
      "|    clip_fraction        | 0.0601       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+05     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | 8.88e-05     |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 2.12e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 524    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 269    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 527            |\n",
      "|    iterations           | 70             |\n",
      "|    time_elapsed         | 271            |\n",
      "|    total_timesteps      | 143360         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.40390475e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.12          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.42e+06       |\n",
      "|    n_updates            | 690            |\n",
      "|    policy_gradient_loss | 8.74e-05       |\n",
      "|    std                  | 0.738          |\n",
      "|    value_loss           | 1.94e+06       |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-5682.43 +/- 10532.51\n",
      "Episode length: 286.00 +/- 40.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | -5.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040413076 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.89e+05     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | 0.00124      |\n",
      "|    std                  | 0.737        |\n",
      "|    value_loss           | 7.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 529    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 274    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 532          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 277          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007798376 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.49e+04     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 7.29e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-4296.06 +/- 6498.71\n",
      "Episode length: 307.75 +/- 43.87\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -4.3e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 148000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031309653 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.34e+05      |\n",
      "|    n_updates            | 720           |\n",
      "|    policy_gradient_loss | -0.000658     |\n",
      "|    std                  | 0.737         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 533    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 280    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 536          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 282          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067641987 |\n",
      "|    clip_fraction        | 0.218        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+04     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | 0.00606      |\n",
      "|    std                  | 0.736        |\n",
      "|    value_loss           | 8.69e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-5744.04 +/- 8033.90\n",
      "Episode length: 334.50 +/- 46.92\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 334           |\n",
      "|    mean_reward          | -5.74e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 152000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5810801e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.34e+06      |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | 0.000202      |\n",
      "|    std                  | 0.736         |\n",
      "|    value_loss           | 1.72e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 536    |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 286    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033349432 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.54e+03     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    std                  | 0.714        |\n",
      "|    value_loss           | 6.96e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-9161.91 +/- 8548.73\n",
      "Episode length: 338.62 +/- 28.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -9.16e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011769217 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.8e+05      |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 539    |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 292    |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.978355e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.21e+04     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.6e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-3868.71 +/- 6167.57\n",
      "Episode length: 324.12 +/- 40.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -3.87e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025212332 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04e+03     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 7.23e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 298    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010721167 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.88e+03    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.000173    |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 2.82e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-8701.32 +/- 9818.13\n",
      "Episode length: 333.38 +/- 31.23\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 333           |\n",
      "|    mean_reward          | -8.7e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 164000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0243503e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.55e+05      |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | -0.000347     |\n",
      "|    std                  | 0.718         |\n",
      "|    value_loss           | 1.46e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 304    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 546           |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 307           |\n",
      "|    total_timesteps      | 167936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8454011e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.14e+06      |\n",
      "|    n_updates            | 810           |\n",
      "|    policy_gradient_loss | -0.000144     |\n",
      "|    std                  | 0.718         |\n",
      "|    value_loss           | 1.75e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-8984.06 +/- 10000.79\n",
      "Episode length: 320.25 +/- 34.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -8.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001892483 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.27e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    std                  | 0.717        |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 547    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 310    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-8669.74 +/- 9438.92\n",
      "Episode length: 319.25 +/- 57.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 319           |\n",
      "|    mean_reward          | -8.67e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 172000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024763486 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.9e+05       |\n",
      "|    n_updates            | 830           |\n",
      "|    policy_gradient_loss | -0.000778     |\n",
      "|    std                  | 0.717         |\n",
      "|    value_loss           | 1.99e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 547    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 313    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 550           |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 316           |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018563628 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.28e+05      |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.00094      |\n",
      "|    std                  | 0.717         |\n",
      "|    value_loss           | 1.38e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-8531.28 +/- 9970.63\n",
      "Episode length: 316.62 +/- 27.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 317         |\n",
      "|    mean_reward          | -8.53e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987494 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03e+06    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.000483   |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 1.21e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 551    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 319    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 553         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010569612 |\n",
      "|    clip_fraction        | 0.0319      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.21e+03    |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00091    |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 3.47e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-4376.64 +/- 5583.36\n",
      "Episode length: 285.88 +/- 53.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | -4.38e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 180000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041278256 |\n",
      "|    clip_fraction        | 0.00908      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+05     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 2.44e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 554    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 324    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 557        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 327        |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01972401 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.9e+04    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | 0.00935    |\n",
      "|    std                  | 0.726      |\n",
      "|    value_loss           | 7.1e+04    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-3682.48 +/- 5484.61\n",
      "Episode length: 321.50 +/- 47.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -3.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056421096 |\n",
      "|    clip_fraction        | 0.0725       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.75e+03     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000188    |\n",
      "|    std                  | 0.731        |\n",
      "|    value_loss           | 1.03e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 557    |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 330    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 560           |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 332           |\n",
      "|    total_timesteps      | 186368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029987746 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3e+05         |\n",
      "|    n_updates            | 900           |\n",
      "|    policy_gradient_loss | 0.000625      |\n",
      "|    std                  | 0.73          |\n",
      "|    value_loss           | 1.36e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-8723.22 +/- 7880.51\n",
      "Episode length: 319.25 +/- 40.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -8.72e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.011624e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.45e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000754    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 560    |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 336    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 338          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013632195 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+04     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 5.79e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-1490.79 +/- 208.93\n",
      "Episode length: 301.12 +/- 42.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 301          |\n",
      "|    mean_reward          | -1.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.223819e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.02e+04     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.000243    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 4.45e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 562    |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 342    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 564         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028845433 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6e+03     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 9.42e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-7247.72 +/- 10028.62\n",
      "Episode length: 332.12 +/- 39.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -7.25e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 196000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012039589 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.4e+04      |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | 0.000436     |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 7.67e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 564    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 347    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 566          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 350          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021741404 |\n",
      "|    clip_fraction        | 0.0477       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.25e+03     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | 0.000312     |\n",
      "|    std                  | 0.74         |\n",
      "|    value_loss           | 7.34e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-6053.06 +/- 8855.92\n",
      "Episode length: 326.88 +/- 32.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -6.05e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053633414 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+06      |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.74         |\n",
      "|    value_loss           | 2.04e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 565    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 354    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 567           |\n",
      "|    iterations           | 99            |\n",
      "|    time_elapsed         | 357           |\n",
      "|    total_timesteps      | 202752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032423344 |\n",
      "|    clip_fraction        | 0.00166       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.6e+05       |\n",
      "|    n_updates            | 980           |\n",
      "|    policy_gradient_loss | -0.000849     |\n",
      "|    std                  | 0.74          |\n",
      "|    value_loss           | 1.21e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-7045.99 +/- 7952.93\n",
      "Episode length: 313.75 +/- 45.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 314          |\n",
      "|    mean_reward          | -7.05e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 204000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.188923e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9e+05        |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    std                  | 0.74         |\n",
      "|    value_loss           | 1.85e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 567    |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 360    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 569           |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 363           |\n",
      "|    total_timesteps      | 206848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031907827 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.01e+05      |\n",
      "|    n_updates            | 1000          |\n",
      "|    policy_gradient_loss | -0.000575     |\n",
      "|    std                  | 0.741         |\n",
      "|    value_loss           | 9.16e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-7639.83 +/- 7822.07\n",
      "Episode length: 335.25 +/- 44.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -7.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028287328 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+03     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 9.53e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 569    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 366    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 571         |\n",
      "|    iterations           | 103         |\n",
      "|    time_elapsed         | 368         |\n",
      "|    total_timesteps      | 210944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009347541 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.27e+04    |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | 0.000592    |\n",
      "|    std                  | 0.733       |\n",
      "|    value_loss           | 3e+04       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-7348.61 +/- 6134.04\n",
      "Episode length: 305.38 +/- 57.49\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 305           |\n",
      "|    mean_reward          | -7.35e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 212000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3731783e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.72e+05      |\n",
      "|    n_updates            | 1030          |\n",
      "|    policy_gradient_loss | -0.000538     |\n",
      "|    std                  | 0.732         |\n",
      "|    value_loss           | 9.88e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 572    |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 372    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 573           |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 374           |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019570763 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.28e+04      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | -0.000872     |\n",
      "|    std                  | 0.732         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-11631.43 +/- 8471.23\n",
      "Episode length: 345.50 +/- 52.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | -1.16e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 216000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034074232 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+04     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    std                  | 0.732        |\n",
      "|    value_loss           | 3.11e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 573    |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 378    |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 575          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 380          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012924462 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | 5.35e-05     |\n",
      "|    std                  | 0.731        |\n",
      "|    value_loss           | 7.56e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-7737.13 +/- 7741.62\n",
      "Episode length: 309.38 +/- 59.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 309           |\n",
      "|    mean_reward          | -7.74e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 220000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0045077e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.09e+05      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -0.0003       |\n",
      "|    std                  | 0.731         |\n",
      "|    value_loss           | 2.19e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 575    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 384    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 576          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 387          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010730809 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.15e+05     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 1.21e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-5611.37 +/- 6961.36\n",
      "Episode length: 307.50 +/- 55.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | -5.61e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041896477 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.33e+05     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | 0.00241      |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 577    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 390    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 578          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 392          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014584109 |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.97e+03     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | 0.00235      |\n",
      "|    std                  | 0.721        |\n",
      "|    value_loss           | 8.66e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-2402.21 +/- 2504.24\n",
      "Episode length: 303.25 +/- 44.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 303          |\n",
      "|    mean_reward          | -2.4e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 228000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018544869 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+04      |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | 0.000762     |\n",
      "|    std                  | 0.723        |\n",
      "|    value_loss           | 1.62e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 579    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 396    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 580         |\n",
      "|    iterations           | 113         |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 231424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004175472 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23e+03    |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 2.55e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-4494.58 +/- 5702.70\n",
      "Episode length: 306.88 +/- 28.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -4.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 232000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030549588 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.34e+05     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.000929    |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 3.75e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 581    |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 401    |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 582           |\n",
      "|    iterations           | 115           |\n",
      "|    time_elapsed         | 404           |\n",
      "|    total_timesteps      | 235520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010585453 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.27e+05      |\n",
      "|    n_updates            | 1140          |\n",
      "|    policy_gradient_loss | -0.000414     |\n",
      "|    std                  | 0.723         |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-10318.38 +/- 8954.48\n",
      "Episode length: 353.62 +/- 43.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 354           |\n",
      "|    mean_reward          | -1.03e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 236000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5700585e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.59e+06      |\n",
      "|    n_updates            | 1150          |\n",
      "|    policy_gradient_loss | -0.000138     |\n",
      "|    std                  | 0.723         |\n",
      "|    value_loss           | 1.49e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 582    |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 407    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 584          |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 239616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003269506 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.41e+05     |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.000416    |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 4.82e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-3949.49 +/- 5062.25\n",
      "Episode length: 298.88 +/- 40.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 299         |\n",
      "|    mean_reward          | -3.95e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011278207 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.54e+03    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00267    |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 6.03e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 584    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 413    |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 586         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 243712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009267829 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.57e+03    |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.000539   |\n",
      "|    std                  | 0.709       |\n",
      "|    value_loss           | 1.15e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-5339.62 +/- 5340.83\n",
      "Episode length: 334.88 +/- 32.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -5.34e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 244000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014669921 |\n",
      "|    clip_fraction        | 0.00522      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+05     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.000462    |\n",
      "|    std                  | 0.711        |\n",
      "|    value_loss           | 2.13e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 585    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 419    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 587           |\n",
      "|    iterations           | 121           |\n",
      "|    time_elapsed         | 422           |\n",
      "|    total_timesteps      | 247808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7103273e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.74e+05      |\n",
      "|    n_updates            | 1200          |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    std                  | 0.711         |\n",
      "|    value_loss           | 1.95e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-7627.80 +/- 8390.08\n",
      "Episode length: 328.00 +/- 49.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -7.63e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 248000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061224564 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85e+05     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | 0.000778     |\n",
      "|    std                  | 0.713        |\n",
      "|    value_loss           | 4.92e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 586    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 425    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 588          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 428          |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042239474 |\n",
      "|    clip_fraction        | 0.00859      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.85e+05     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -2.87e-05    |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.18e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-7363.69 +/- 4924.14\n",
      "Episode length: 319.12 +/- 43.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | -7.36e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002486806 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 3.58e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.15e+05    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.000939   |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 1.47e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 588    |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 431    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-5279.32 +/- 5740.45\n",
      "Episode length: 311.12 +/- 50.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 311         |\n",
      "|    mean_reward          | -5.28e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026671909 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.1e+05     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | 0.00772     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 3.89e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 588    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 435    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 589           |\n",
      "|    iterations           | 126           |\n",
      "|    time_elapsed         | 437           |\n",
      "|    total_timesteps      | 258048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4445886e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.38e+06      |\n",
      "|    n_updates            | 1250          |\n",
      "|    policy_gradient_loss | -0.00021      |\n",
      "|    std                  | 0.71          |\n",
      "|    value_loss           | 2.23e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-5521.16 +/- 7523.81\n",
      "Episode length: 283.12 +/- 53.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | -5.52e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006894015 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.37e+05    |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.000726   |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 7.47e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 589    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 441    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 590           |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 443           |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011284696 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.68e+05      |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | -0.00032      |\n",
      "|    std                  | 0.71          |\n",
      "|    value_loss           | 2.48e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-9712.22 +/- 9223.38\n",
      "Episode length: 305.12 +/- 56.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | -9.71e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 264000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003800598 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.46e+05     |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.000502    |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 1.8e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 590    |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 447    |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 449          |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.433226e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07e+06     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.000814    |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 3.56e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-11916.65 +/- 8505.58\n",
      "Episode length: 331.62 +/- 48.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -1.19e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0129073365 |\n",
      "|    clip_fraction        | 0.0996       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.7e+04      |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | 0.000122     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 3.64e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 592    |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 453    |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001649401 |\n",
      "|    clip_fraction        | 0.00894     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.98e+05    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 3.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-6806.08 +/- 7038.52\n",
      "Episode length: 290.50 +/- 49.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -6.81e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024880455 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.47e+04     |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.708        |\n",
      "|    value_loss           | 1.86e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 593    |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 458    |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033711948 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38e+05    |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.00272     |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-16371.41 +/- 11311.09\n",
      "Episode length: 320.00 +/- 41.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -1.64e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 276000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.826732e-05 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56e+05     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.000467    |\n",
      "|    std                  | 0.701        |\n",
      "|    value_loss           | 2.61e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 594    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 464    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 467         |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005418267 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.86e+05    |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    std                  | 0.704       |\n",
      "|    value_loss           | 5.54e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-6641.75 +/- 7365.15\n",
      "Episode length: 339.50 +/- 36.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -6.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 280000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003861372 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.7e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+06     |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.000941    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.9e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 596    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 470    |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 597          |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 473          |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014760443 |\n",
      "|    clip_fraction        | 0.00869      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.93e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.93e+05     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -8.45e-05    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-6398.97 +/- 6046.25\n",
      "Episode length: 325.62 +/- 50.41\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -6.4e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 284000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019924581 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 1.09e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.99e+05      |\n",
      "|    n_updates            | 1380          |\n",
      "|    policy_gradient_loss | -0.00106      |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 2.48e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 597    |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 476    |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 598          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 478          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013772042 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.58e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63e+06     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-6210.39 +/- 8193.84\n",
      "Episode length: 310.50 +/- 30.77\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 310           |\n",
      "|    mean_reward          | -6.21e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 288000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021281908 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 2.09e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.89e+05      |\n",
      "|    n_updates            | 1400          |\n",
      "|    policy_gradient_loss | -0.000469     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 1.96e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 599    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 482    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 600         |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 290816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012274868 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.13e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.26e+05    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 1.01e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-11415.83 +/- 12447.16\n",
      "Episode length: 326.25 +/- 40.41\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -1.14e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 292000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021874969 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 1.85e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.27e+05      |\n",
      "|    n_updates            | 1420          |\n",
      "|    policy_gradient_loss | -0.00189      |\n",
      "|    std                  | 0.703         |\n",
      "|    value_loss           | 2.93e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 600    |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 487    |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 601           |\n",
      "|    iterations           | 144           |\n",
      "|    time_elapsed         | 490           |\n",
      "|    total_timesteps      | 294912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047990467 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 9.54e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.27e+05      |\n",
      "|    n_updates            | 1430          |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 1.8e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-3075.88 +/- 2318.77\n",
      "Episode length: 334.12 +/- 50.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 334         |\n",
      "|    mean_reward          | -3.08e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011514617 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.13e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.59e+04    |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.000125   |\n",
      "|    std                  | 0.704       |\n",
      "|    value_loss           | 1.2e+05     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 601    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 493    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 602          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 495          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.558154e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000304     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+06     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.000107    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 3.37e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-6851.17 +/- 6387.80\n",
      "Episode length: 330.62 +/- 61.22\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 331           |\n",
      "|    mean_reward          | -6.85e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 300000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7044898e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 2.04e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.83e+06      |\n",
      "|    n_updates            | 1460          |\n",
      "|    policy_gradient_loss | -0.000232     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 4.42e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 602    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 499    |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 501          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033856449 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.14e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+05      |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.00272     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-8120.23 +/- 7129.88\n",
      "Episode length: 354.25 +/- 25.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -8.12e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 304000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006647826 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1.61e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.34e+05    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 4.9e+05     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 604    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 504    |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 507          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019518671 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 5.9e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.83e+03     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.00084     |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-4021.97 +/- 6165.52\n",
      "Episode length: 352.62 +/- 32.15\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 353           |\n",
      "|    mean_reward          | -4.02e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 308000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8077088e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.00998       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.76e+05      |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -9.69e-05     |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 9.93e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 605    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 510    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 606           |\n",
      "|    iterations           | 152           |\n",
      "|    time_elapsed         | 513           |\n",
      "|    total_timesteps      | 311296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020950526 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.0138        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.17e+05      |\n",
      "|    n_updates            | 1510          |\n",
      "|    policy_gradient_loss | -0.000236     |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 2.74e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-5304.81 +/- 5627.97\n",
      "Episode length: 331.75 +/- 42.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -5.3e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 312000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028092202 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000224     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.2e+04      |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 6.32e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 606    |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 516    |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 518          |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008785522 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000828     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85e+05     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -0.000437    |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 2.39e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-5517.77 +/- 7094.93\n",
      "Episode length: 328.25 +/- 47.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -5.52e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008107964 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.00696      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36e+06     |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    std                  | 0.701        |\n",
      "|    value_loss           | 2.91e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 522    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 524          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024253628 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000318     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.61e+05     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 5.31e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-7724.94 +/- 7701.88\n",
      "Episode length: 312.38 +/- 39.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -7.72e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009779538 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000384     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.7e+05      |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.000802    |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 1.36e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 609    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 527    |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 529         |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005886602 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 7.78e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.92e+03    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 1.94e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-15217.29 +/- 6219.59\n",
      "Episode length: 329.00 +/- 48.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 329          |\n",
      "|    mean_reward          | -1.52e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003916061 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0178       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.69e+05     |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.000348    |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 2.74e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 610    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 533    |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 612          |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 535          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006421994 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0172       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.85e+05     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 7.46e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-6388.07 +/- 8554.23\n",
      "Episode length: 314.12 +/- 42.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 314          |\n",
      "|    mean_reward          | -6.39e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010521959 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00235      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+06     |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.74e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 611    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 538    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 541          |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033513745 |\n",
      "|    clip_fraction        | 0.00977      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000133     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1e+06        |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.000875    |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 1.03e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-8736.19 +/- 7592.61\n",
      "Episode length: 305.00 +/- 46.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | -8.74e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028359308 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000319     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.63e+05     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.000803    |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 613    |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 544    |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 546          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020965566 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.00019      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.53e+05     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.000565    |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 2.45e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-4133.98 +/- 6189.30\n",
      "Episode length: 321.00 +/- 43.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | -4.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050209453 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 7.8e-05      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+05     |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 1.78e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 614    |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 549    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 615          |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 552          |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022744802 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.91e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+06     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 3.4e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-8845.29 +/- 9696.00\n",
      "Episode length: 315.12 +/- 39.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 315        |\n",
      "|    mean_reward          | -8.85e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 340000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00543148 |\n",
      "|    clip_fraction        | 0.0579     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 3.78e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.78e+05   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.00104   |\n",
      "|    std                  | 0.705      |\n",
      "|    value_loss           | 5.78e+05   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 615    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 555    |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-6497.28 +/- 8928.96\n",
      "Episode length: 294.38 +/- 56.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 294         |\n",
      "|    mean_reward          | -6.5e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 344000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003431833 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00443     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.07e+03    |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 3e+04       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 615    |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 558    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 616          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 561          |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005233368 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00148      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+06     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.000421    |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.1e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-4306.47 +/- 5204.72\n",
      "Episode length: 309.25 +/- 47.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 309           |\n",
      "|    mean_reward          | -4.31e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 348000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00091826357 |\n",
      "|    clip_fraction        | 0.000928      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0.00216       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.25e+06      |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | -0.000277     |\n",
      "|    std                  | 0.708         |\n",
      "|    value_loss           | 9.7e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 616    |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 564    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 566          |\n",
      "|    total_timesteps      | 350208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012192628 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00281      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.58e+04     |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.000773    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 2.32e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-6005.31 +/- 6560.48\n",
      "Episode length: 339.62 +/- 36.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -6.01e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 352000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002287889 |\n",
      "|    clip_fraction        | 0.00244     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00541     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+05    |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 9.02e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 617    |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 570    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 572          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005299675 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00364      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.43e+05     |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000202    |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-8301.87 +/- 9116.41\n",
      "Episode length: 322.88 +/- 35.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -8.3e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 356000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018838182 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -0.0486       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.73e+04      |\n",
      "|    n_updates            | 1730          |\n",
      "|    policy_gradient_loss | -6.98e-05     |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 2.13e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 618    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 575    |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 620           |\n",
      "|    iterations           | 175           |\n",
      "|    time_elapsed         | 578           |\n",
      "|    total_timesteps      | 358400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015657378 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.0208        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.43e+05      |\n",
      "|    n_updates            | 1740          |\n",
      "|    policy_gradient_loss | -0.000488     |\n",
      "|    std                  | 0.702         |\n",
      "|    value_loss           | 1.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-8813.77 +/- 6294.95\n",
      "Episode length: 305.62 +/- 44.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | -8.81e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011872547 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.01         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+06      |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.000567    |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 3.67e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 620    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 581    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 621          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 583          |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022679353 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000295     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+05     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | 0.000704     |\n",
      "|    std                  | 0.695        |\n",
      "|    value_loss           | 9.94e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-3757.59 +/- 6159.49\n",
      "Episode length: 306.75 +/- 50.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | -3.76e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 364000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001563704 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.000161    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+04    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.000356   |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 621    |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 586    |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 589          |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005099919 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000482     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+06     |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.000607    |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 2.98e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-8330.87 +/- 6406.83\n",
      "Episode length: 308.00 +/- 62.29\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -8.33e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 368000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088054384 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0.000199      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+05      |\n",
      "|    n_updates            | 1790          |\n",
      "|    policy_gradient_loss | -0.00154      |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 2.52e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 621    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 592    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 595          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012120033 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0132       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93e+05     |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 7.2e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-6399.07 +/- 8297.67\n",
      "Episode length: 335.88 +/- 34.82\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 336           |\n",
      "|    mean_reward          | -6.4e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 372000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012780784 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0.00893       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.45e+04      |\n",
      "|    n_updates            | 1810          |\n",
      "|    policy_gradient_loss | -0.000784     |\n",
      "|    std                  | 0.694         |\n",
      "|    value_loss           | 4.32e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 599    |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 601          |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014513314 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00905      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.46e+04     |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.695        |\n",
      "|    value_loss           | 1.89e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-7972.29 +/- 9944.68\n",
      "Episode length: 326.50 +/- 48.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -7.97e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 376000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013034774 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.00048      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5e+05      |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 1.97e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 605    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 607          |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043762065 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000117     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.64e+05     |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 1.92e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-4339.41 +/- 5021.28\n",
      "Episode length: 305.75 +/- 39.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | -4.34e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057630776 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 3.52e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.33e+05     |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.00054     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 610    |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 612          |\n",
      "|    total_timesteps      | 382976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031585936 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000111     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.81e+04     |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 8.37e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-4293.22 +/- 4751.91\n",
      "Episode length: 313.25 +/- 35.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 313          |\n",
      "|    mean_reward          | -4.29e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.870799e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.013        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.15e+06     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 1.33e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 624    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 616    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 625           |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 618           |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053186546 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.0172        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.77e+05      |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | -0.000563     |\n",
      "|    std                  | 0.695         |\n",
      "|    value_loss           | 7.71e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-17795.79 +/- 13845.03\n",
      "Episode length: 325.12 +/- 47.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | -1.78e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 388000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001158168 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0099       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+06      |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.00031     |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 1.52e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 625    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 622    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 626          |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 624          |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015011968 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00122      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.29e+05     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 1.9e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-11390.60 +/- 7840.15\n",
      "Episode length: 313.00 +/- 42.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | -1.14e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 392000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008351488 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.000192    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.87e+05    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 3.62e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 626    |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 627    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 193         |\n",
      "|    time_elapsed         | 630         |\n",
      "|    total_timesteps      | 395264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005884272 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0031      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.89e+04    |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 1.42e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-7745.81 +/- 8094.17\n",
      "Episode length: 345.38 +/- 38.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -7.75e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 396000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010604414 |\n",
      "|    clip_fraction        | 0.00161      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000414     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.91e+05     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.709        |\n",
      "|    value_loss           | 2.03e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 627    |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 633    |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 195        |\n",
      "|    time_elapsed         | 635        |\n",
      "|    total_timesteps      | 399360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00606095 |\n",
      "|    clip_fraction        | 0.0324     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 9.49e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.54e+04   |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.00178   |\n",
      "|    std                  | 0.704      |\n",
      "|    value_loss           | 8.33e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-9563.62 +/- 10329.44\n",
      "Episode length: 311.50 +/- 35.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -9.56e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011841148 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000633     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.85e+03     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00057     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 2.34e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 628    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 639    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 629          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 641          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017267686 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0287       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36e+04     |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 1.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-11064.93 +/- 13048.01\n",
      "Episode length: 319.75 +/- 46.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -1.11e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 404000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013109748 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0239       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.67e+05     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 8.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 198    |\n",
      "|    time_elapsed    | 644    |\n",
      "|    total_timesteps | 405504 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 629           |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 646           |\n",
      "|    total_timesteps      | 407552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018110915 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0.00738       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.94e+05      |\n",
      "|    n_updates            | 1980          |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-9621.73 +/- 10563.25\n",
      "Episode length: 323.88 +/- 48.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -9.62e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 408000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015522634 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0619       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+04     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.000752    |\n",
      "|    std                  | 0.709        |\n",
      "|    value_loss           | 2.62e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 200    |\n",
      "|    time_elapsed    | 650    |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 630           |\n",
      "|    iterations           | 201           |\n",
      "|    time_elapsed         | 652           |\n",
      "|    total_timesteps      | 411648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084643066 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 0.0195        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.47e+06      |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -0.000257     |\n",
      "|    std                  | 0.71          |\n",
      "|    value_loss           | 3.33e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-16816.65 +/- 13133.95\n",
      "Episode length: 340.00 +/- 56.57\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 340           |\n",
      "|    mean_reward          | -1.68e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 412000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7454483e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -0.00223      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.28e+04      |\n",
      "|    n_updates            | 2010          |\n",
      "|    policy_gradient_loss | -0.000112     |\n",
      "|    std                  | 0.706         |\n",
      "|    value_loss           | 2.87e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 630    |\n",
      "|    iterations      | 202    |\n",
      "|    time_elapsed    | 656    |\n",
      "|    total_timesteps | 413696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 203          |\n",
      "|    time_elapsed         | 658          |\n",
      "|    total_timesteps      | 415744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001175864 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0145       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+05     |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 1.7e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-4489.45 +/- 5353.48\n",
      "Episode length: 327.88 +/- 42.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -4.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013650216 |\n",
      "|    clip_fraction        | 0.00352      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00347      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.97e+04     |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 5.86e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 204    |\n",
      "|    time_elapsed    | 662    |\n",
      "|    total_timesteps | 417792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 664         |\n",
      "|    total_timesteps      | 419840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005355091 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.00281     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.48e+05    |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 1.43e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-10331.68 +/- 9131.04\n",
      "Episode length: 324.88 +/- 42.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061893715 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000179     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+06     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 2.82e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 206    |\n",
      "|    time_elapsed    | 667    |\n",
      "|    total_timesteps | 421888 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 632          |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 669          |\n",
      "|    total_timesteps      | 423936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055659055 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 6.35e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+05     |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -3.6e-05     |\n",
      "|    std                  | 0.689        |\n",
      "|    value_loss           | 6.75e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-12559.14 +/- 9531.22\n",
      "Episode length: 292.62 +/- 12.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | -1.26e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 424000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003391005 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 3.79e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.09e+05    |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.000428    |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 1.01e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 208    |\n",
      "|    time_elapsed    | 673    |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-6453.78 +/- 7048.30\n",
      "Episode length: 335.25 +/- 49.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -6.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 428000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041948482 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.000362     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.67e+04     |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    std                  | 0.678        |\n",
      "|    value_loss           | 2.91e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 209    |\n",
      "|    time_elapsed    | 676    |\n",
      "|    total_timesteps | 428032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 210          |\n",
      "|    time_elapsed         | 678          |\n",
      "|    total_timesteps      | 430080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003592775 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.00651      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.52e+05     |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | -0.000717    |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-5132.19 +/- 5146.59\n",
      "Episode length: 306.25 +/- 23.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029179754 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.000574     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.29e+05     |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    std                  | 0.678        |\n",
      "|    value_loss           | 6.44e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 211    |\n",
      "|    time_elapsed    | 681    |\n",
      "|    total_timesteps | 432128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 634          |\n",
      "|    iterations           | 212          |\n",
      "|    time_elapsed         | 684          |\n",
      "|    total_timesteps      | 434176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021113588 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.000113     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.33e+05     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    std                  | 0.689        |\n",
      "|    value_loss           | 1.37e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-5927.90 +/- 7853.12\n",
      "Episode length: 325.88 +/- 42.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -5.93e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 436000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012766182 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.000141     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.7e+05      |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -2.62e-05    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 8.19e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 634    |\n",
      "|    iterations      | 213    |\n",
      "|    time_elapsed    | 687    |\n",
      "|    total_timesteps | 436224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 689         |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002567915 |\n",
      "|    clip_fraction        | 0.0061      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.000196    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.94e+06    |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -2.58e-05   |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 2.45e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-2549.41 +/- 2629.27\n",
      "Episode length: 310.50 +/- 45.69\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 310           |\n",
      "|    mean_reward          | -2.55e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 440000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021518773 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 6.94e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+04      |\n",
      "|    n_updates            | 2140          |\n",
      "|    policy_gradient_loss | 5.3e-05       |\n",
      "|    std                  | 0.681         |\n",
      "|    value_loss           | 3.94e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 635    |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 692    |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 636          |\n",
      "|    iterations           | 216          |\n",
      "|    time_elapsed         | 695          |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.300984e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.023        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.27e+05     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -0.000514    |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-8566.51 +/- 13703.52\n",
      "Episode length: 288.50 +/- 47.34\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 288           |\n",
      "|    mean_reward          | -8.57e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 444000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060207583 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0148        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.97e+04      |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | -0.000881     |\n",
      "|    std                  | 0.681         |\n",
      "|    value_loss           | 7.17e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 636    |\n",
      "|    iterations      | 217    |\n",
      "|    time_elapsed    | 698    |\n",
      "|    total_timesteps | 444416 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 637           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 700           |\n",
      "|    total_timesteps      | 446464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068304245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0164        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.93e+05      |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -0.000903     |\n",
      "|    std                  | 0.679         |\n",
      "|    value_loss           | 1.29e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-13399.60 +/- 16306.11\n",
      "Episode length: 302.75 +/- 47.85\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 303           |\n",
      "|    mean_reward          | -1.34e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 448000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040318948 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.00892       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.24e+04      |\n",
      "|    n_updates            | 2180          |\n",
      "|    policy_gradient_loss | -0.000469     |\n",
      "|    std                  | 0.678         |\n",
      "|    value_loss           | 1.65e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 637    |\n",
      "|    iterations      | 219    |\n",
      "|    time_elapsed    | 703    |\n",
      "|    total_timesteps | 448512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003033264 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.00298     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03e+06    |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 2.06e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-13364.31 +/- 12923.35\n",
      "Episode length: 329.62 +/- 53.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 330          |\n",
      "|    mean_reward          | -1.34e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 452000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059664007 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.000293     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+06      |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.00799     |\n",
      "|    std                  | 0.672        |\n",
      "|    value_loss           | 3.44e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 637    |\n",
      "|    iterations      | 221    |\n",
      "|    time_elapsed    | 709    |\n",
      "|    total_timesteps | 452608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 711          |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060858596 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 7.97e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.33e+05     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 5.07e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-9223.74 +/- 9730.27\n",
      "Episode length: 327.62 +/- 42.11\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 328           |\n",
      "|    mean_reward          | -9.22e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 456000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043770976 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.00611       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.65e+04      |\n",
      "|    n_updates            | 2220          |\n",
      "|    policy_gradient_loss | -0.000477     |\n",
      "|    std                  | 0.668         |\n",
      "|    value_loss           | 1.05e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 638    |\n",
      "|    iterations      | 223    |\n",
      "|    time_elapsed    | 715    |\n",
      "|    total_timesteps | 456704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 639          |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 717          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.984174e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0121       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57e+06     |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.000102    |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-3326.38 +/- 4537.02\n",
      "Episode length: 323.12 +/- 47.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -3.33e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 460000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049240305 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.01         |\n",
      "|    explained_variance   | 0.00548       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.71e+05      |\n",
      "|    n_updates            | 2240          |\n",
      "|    policy_gradient_loss | -0.000458     |\n",
      "|    std                  | 0.662         |\n",
      "|    value_loss           | 1.45e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 639    |\n",
      "|    iterations      | 225    |\n",
      "|    time_elapsed    | 720    |\n",
      "|    total_timesteps | 460800 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 640           |\n",
      "|    iterations           | 226           |\n",
      "|    time_elapsed         | 722           |\n",
      "|    total_timesteps      | 462848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059241324 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.00115       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.66e+04      |\n",
      "|    n_updates            | 2250          |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    std                  | 0.66          |\n",
      "|    value_loss           | 6.76e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-7290.94 +/- 7857.03\n",
      "Episode length: 349.50 +/- 24.52\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 350           |\n",
      "|    mean_reward          | -7.29e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 464000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3671433e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.0191        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.58e+05      |\n",
      "|    n_updates            | 2260          |\n",
      "|    policy_gradient_loss | -5.91e-05     |\n",
      "|    std                  | 0.661         |\n",
      "|    value_loss           | 3.85e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 640    |\n",
      "|    iterations      | 227    |\n",
      "|    time_elapsed    | 726    |\n",
      "|    total_timesteps | 464896 |\n",
      "-------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 640            |\n",
      "|    iterations           | 228            |\n",
      "|    time_elapsed         | 728            |\n",
      "|    total_timesteps      | 466944         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000119021686 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.01          |\n",
      "|    explained_variance   | -0.0543        |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 2.71e+04       |\n",
      "|    n_updates            | 2270           |\n",
      "|    policy_gradient_loss | -7.01e-06      |\n",
      "|    std                  | 0.661          |\n",
      "|    value_loss           | 2.05e+04       |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-8830.67 +/- 7312.14\n",
      "Episode length: 313.38 +/- 40.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 313          |\n",
      "|    mean_reward          | -8.83e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 468000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003677429 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0381       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.45e+05     |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.000815    |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 1.18e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 640    |\n",
      "|    iterations      | 229    |\n",
      "|    time_elapsed    | 731    |\n",
      "|    total_timesteps | 468992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 734          |\n",
      "|    total_timesteps      | 471040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.691074e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0106       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+06     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.000178    |\n",
      "|    std                  | 0.657        |\n",
      "|    value_loss           | 1.85e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-6514.01 +/- 8357.59\n",
      "Episode length: 326.38 +/- 59.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -6.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 472000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006830502 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.0196       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.95e+03     |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.000341    |\n",
      "|    std                  | 0.653        |\n",
      "|    value_loss           | 1.14e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 641    |\n",
      "|    iterations      | 231    |\n",
      "|    time_elapsed    | 737    |\n",
      "|    total_timesteps | 473088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 739          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007530847 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.993       |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.42e+04     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.000473    |\n",
      "|    std                  | 0.653        |\n",
      "|    value_loss           | 3.5e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-10568.58 +/- 12151.02\n",
      "Episode length: 324.12 +/- 43.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -1.06e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 476000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.492546e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.991       |\n",
      "|    explained_variance   | 0.0276       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.17e+05     |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.000362    |\n",
      "|    std                  | 0.651        |\n",
      "|    value_loss           | 8.58e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 642    |\n",
      "|    iterations      | 233    |\n",
      "|    time_elapsed    | 743    |\n",
      "|    total_timesteps | 477184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 234          |\n",
      "|    time_elapsed         | 745          |\n",
      "|    total_timesteps      | 479232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.667812e-05 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.992       |\n",
      "|    explained_variance   | 0.0782       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.22e+03     |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -0.000119    |\n",
      "|    std                  | 0.653        |\n",
      "|    value_loss           | 1.16e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-11994.05 +/- 12574.83\n",
      "Episode length: 299.25 +/- 48.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 299           |\n",
      "|    mean_reward          | -1.2e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 480000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021587069 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.994        |\n",
      "|    explained_variance   | 0.0326        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.36e+06      |\n",
      "|    n_updates            | 2340          |\n",
      "|    policy_gradient_loss | -9.77e-06     |\n",
      "|    std                  | 0.654         |\n",
      "|    value_loss           | 3.42e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 642    |\n",
      "|    iterations      | 235    |\n",
      "|    time_elapsed    | 748    |\n",
      "|    total_timesteps | 481280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 643          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 751          |\n",
      "|    total_timesteps      | 483328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009346382 |\n",
      "|    clip_fraction        | 0.00532      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.999       |\n",
      "|    explained_variance   | 0.00265      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97e+05     |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 2.59e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-8804.10 +/- 9538.43\n",
      "Episode length: 304.62 +/- 54.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 305           |\n",
      "|    mean_reward          | -8.8e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 484000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068354135 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.0116        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.46e+06      |\n",
      "|    n_updates            | 2360          |\n",
      "|    policy_gradient_loss | -0.000715     |\n",
      "|    std                  | 0.66          |\n",
      "|    value_loss           | 1.17e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 643    |\n",
      "|    iterations      | 237    |\n",
      "|    time_elapsed    | 754    |\n",
      "|    total_timesteps | 485376 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 643           |\n",
      "|    iterations           | 238           |\n",
      "|    time_elapsed         | 756           |\n",
      "|    total_timesteps      | 487424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026411648 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.0135        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.83e+05      |\n",
      "|    n_updates            | 2370          |\n",
      "|    policy_gradient_loss | -0.000208     |\n",
      "|    std                  | 0.655         |\n",
      "|    value_loss           | 1.34e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-3862.98 +/- 4059.99\n",
      "Episode length: 311.00 +/- 51.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -3.86e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 488000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027451208 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.00533      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.7e+04      |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    std                  | 0.658        |\n",
      "|    value_loss           | 1.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 643    |\n",
      "|    iterations      | 239    |\n",
      "|    time_elapsed    | 760    |\n",
      "|    total_timesteps | 489472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 762          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029265685 |\n",
      "|    clip_fraction        | 0.00459      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0148       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45e+06     |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 2.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-12008.48 +/- 11080.04\n",
      "Episode length: 334.25 +/- 11.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 334         |\n",
      "|    mean_reward          | -1.2e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 492000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004147946 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.000859    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.44e+05    |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.000677   |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 8.79e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 241    |\n",
      "|    time_elapsed    | 765    |\n",
      "|    total_timesteps | 493568 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007278183 |\n",
      "|    clip_fraction        | 0.0615      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.000575    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.08e+05    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    std                  | 0.673       |\n",
      "|    value_loss           | 2.75e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-7916.24 +/- 8334.43\n",
      "Episode length: 312.88 +/- 40.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | -7.92e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001238007 |\n",
      "|    clip_fraction        | 4.88e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0173      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.29e+04    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.000793   |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 2.5e+04     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 243    |\n",
      "|    time_elapsed    | 771    |\n",
      "|    total_timesteps | 497664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 773          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021705911 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0318       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.673        |\n",
      "|    value_loss           | 8.71e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-5467.74 +/- 6486.61\n",
      "Episode length: 313.88 +/- 47.27\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 314           |\n",
      "|    mean_reward          | -5.47e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 500000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1790546e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.019         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.85e+04      |\n",
      "|    n_updates            | 2440          |\n",
      "|    policy_gradient_loss | 0.000127      |\n",
      "|    std                  | 0.672         |\n",
      "|    value_loss           | 1.06e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 245    |\n",
      "|    time_elapsed    | 777    |\n",
      "|    total_timesteps | 501760 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 246           |\n",
      "|    time_elapsed         | 780           |\n",
      "|    total_timesteps      | 503808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9061655e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.012         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.18e+04      |\n",
      "|    n_updates            | 2450          |\n",
      "|    policy_gradient_loss | -0.00088      |\n",
      "|    std                  | 0.672         |\n",
      "|    value_loss           | 9.84e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-12242.61 +/- 10452.24\n",
      "Episode length: 355.75 +/- 45.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | -1.22e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.78002e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0174      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2e+06     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.000136   |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 9.04e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 247    |\n",
      "|    time_elapsed    | 784    |\n",
      "|    total_timesteps | 505856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 248          |\n",
      "|    time_elapsed         | 787          |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.415974e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.00903      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | -0.000285    |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-11706.01 +/- 9312.96\n",
      "Episode length: 337.38 +/- 44.35\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 337           |\n",
      "|    mean_reward          | -1.17e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 508000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9380765e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.0169        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.67e+05      |\n",
      "|    n_updates            | 2480          |\n",
      "|    policy_gradient_loss | -0.00038      |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 1.18e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 249    |\n",
      "|    time_elapsed    | 790    |\n",
      "|    total_timesteps | 509952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-16456.94 +/- 11927.83\n",
      "Episode length: 323.38 +/- 47.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 323         |\n",
      "|    mean_reward          | -1.65e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 512000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001113868 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0228      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.43e+05    |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.000895   |\n",
      "|    std                  | 0.666       |\n",
      "|    value_loss           | 1.41e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 250    |\n",
      "|    time_elapsed    | 794    |\n",
      "|    total_timesteps | 512000 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 251          |\n",
      "|    time_elapsed         | 796          |\n",
      "|    total_timesteps      | 514048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006036103 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.01e+04     |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.000642    |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 6.72e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-5172.88 +/- 6153.74\n",
      "Episode length: 335.62 +/- 35.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | -5.17e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016149068 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.00899      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.22e+05     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.000973    |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 5.41e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 252    |\n",
      "|    time_elapsed    | 800    |\n",
      "|    total_timesteps | 516096 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 253          |\n",
      "|    time_elapsed         | 803          |\n",
      "|    total_timesteps      | 518144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013220061 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0183       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.53e+05     |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.000182    |\n",
      "|    std                  | 0.673        |\n",
      "|    value_loss           | 9.47e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-10540.72 +/- 9169.95\n",
      "Episode length: 354.00 +/- 31.75\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 354           |\n",
      "|    mean_reward          | -1.05e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 520000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048492916 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.0243        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.51e+05      |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 6.8e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 254    |\n",
      "|    time_elapsed    | 807    |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 809           |\n",
      "|    total_timesteps      | 522240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4681105e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0108        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.7e+06       |\n",
      "|    n_updates            | 2540          |\n",
      "|    policy_gradient_loss | -0.000159     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 1.38e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-10666.15 +/- 8985.74\n",
      "Episode length: 290.38 +/- 44.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -1.07e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 524000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033250328 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -0.00474     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+04     |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 3.15e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 256    |\n",
      "|    time_elapsed    | 812    |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 257           |\n",
      "|    time_elapsed         | 814           |\n",
      "|    total_timesteps      | 526336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019615245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0189        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.49e+05      |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | 5.14e-05      |\n",
      "|    std                  | 0.679         |\n",
      "|    value_loss           | 1.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-9436.88 +/- 10101.61\n",
      "Episode length: 310.62 +/- 48.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -9.44e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 528000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033229566 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.00512      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.26e+05     |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 1.98e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 258    |\n",
      "|    time_elapsed    | 818    |\n",
      "|    total_timesteps | 528384 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 646          |\n",
      "|    iterations           | 259          |\n",
      "|    time_elapsed         | 820          |\n",
      "|    total_timesteps      | 530432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016031365 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00116      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+06     |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 2.33e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-14929.16 +/- 8774.30\n",
      "Episode length: 294.50 +/- 41.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 294          |\n",
      "|    mean_reward          | -1.49e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 532000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060796365 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000488     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+03     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 3.87e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 646    |\n",
      "|    iterations      | 260    |\n",
      "|    time_elapsed    | 824    |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 646          |\n",
      "|    iterations           | 261          |\n",
      "|    time_elapsed         | 826          |\n",
      "|    total_timesteps      | 534528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.662206e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.026        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.94e+04     |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.000582    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.03e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-8519.50 +/- 8879.42\n",
      "Episode length: 346.62 +/- 33.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | -8.52e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 536000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008597288 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0105       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.687        |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 646    |\n",
      "|    iterations      | 262    |\n",
      "|    time_elapsed    | 830    |\n",
      "|    total_timesteps | 536576 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 647          |\n",
      "|    iterations           | 263          |\n",
      "|    time_elapsed         | 832          |\n",
      "|    total_timesteps      | 538624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017002192 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.00582      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09e+06     |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.000295    |\n",
      "|    std                  | 0.686        |\n",
      "|    value_loss           | 1.57e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-7066.22 +/- 7347.87\n",
      "Episode length: 323.12 +/- 26.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 323          |\n",
      "|    mean_reward          | -7.07e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 540000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072895116 |\n",
      "|    clip_fraction        | 0.0679       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.00387      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5e+05      |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    std                  | 0.687        |\n",
      "|    value_loss           | 9.58e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 646    |\n",
      "|    iterations      | 264    |\n",
      "|    time_elapsed    | 835    |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005975243 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00619     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.42e+05    |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 1.93e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-13549.84 +/- 9530.32\n",
      "Episode length: 307.62 +/- 32.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | -1.35e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003818682 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.000295    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.56e+06    |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 1.12e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 647    |\n",
      "|    iterations      | 266    |\n",
      "|    time_elapsed    | 841    |\n",
      "|    total_timesteps | 544768 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 267          |\n",
      "|    time_elapsed         | 843          |\n",
      "|    total_timesteps      | 546816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061101206 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.998       |\n",
      "|    explained_variance   | 0.00723      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.84e+04     |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 0.657        |\n",
      "|    value_loss           | 1.04e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-4864.37 +/- 6746.14\n",
      "Episode length: 302.50 +/- 55.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 302           |\n",
      "|    mean_reward          | -4.86e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 548000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029154736 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.997        |\n",
      "|    explained_variance   | 0.0145        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.92e+05      |\n",
      "|    n_updates            | 2670          |\n",
      "|    policy_gradient_loss | -0.000414     |\n",
      "|    std                  | 0.655         |\n",
      "|    value_loss           | 1.52e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 647    |\n",
      "|    iterations      | 268    |\n",
      "|    time_elapsed    | 847    |\n",
      "|    total_timesteps | 548864 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 269          |\n",
      "|    time_elapsed         | 849          |\n",
      "|    total_timesteps      | 550912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036573848 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.00832      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.88e+05     |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    std                  | 0.654        |\n",
      "|    value_loss           | 1.89e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-7460.35 +/- 6351.72\n",
      "Episode length: 302.00 +/- 58.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 302          |\n",
      "|    mean_reward          | -7.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 552000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045484924 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.999       |\n",
      "|    explained_variance   | 0.000996     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+05     |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    std                  | 0.656        |\n",
      "|    value_loss           | 6.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 648    |\n",
      "|    iterations      | 270    |\n",
      "|    time_elapsed    | 853    |\n",
      "|    total_timesteps | 552960 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 648           |\n",
      "|    iterations           | 271           |\n",
      "|    time_elapsed         | 855           |\n",
      "|    total_timesteps      | 555008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054192706 |\n",
      "|    clip_fraction        | 0.00249       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.993        |\n",
      "|    explained_variance   | 0.00487       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.16e+05      |\n",
      "|    n_updates            | 2700          |\n",
      "|    policy_gradient_loss | -0.00165      |\n",
      "|    std                  | 0.65          |\n",
      "|    value_loss           | 8e+05         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-16021.09 +/- 10823.86\n",
      "Episode length: 329.38 +/- 48.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | -1.6e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 556000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006599878 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.00339     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.69e+05    |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    std                  | 0.65        |\n",
      "|    value_loss           | 2.14e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 648    |\n",
      "|    iterations      | 272    |\n",
      "|    time_elapsed    | 859    |\n",
      "|    total_timesteps | 557056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 273          |\n",
      "|    time_elapsed         | 861          |\n",
      "|    total_timesteps      | 559104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034676427 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.988       |\n",
      "|    explained_variance   | 0.00386      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.08e+04     |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    std                  | 0.647        |\n",
      "|    value_loss           | 1.29e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-5419.04 +/- 6938.71\n",
      "Episode length: 307.88 +/- 27.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | -5.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006092505 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.984       |\n",
      "|    explained_variance   | 0.0277       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+05     |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.000329    |\n",
      "|    std                  | 0.647        |\n",
      "|    value_loss           | 3.55e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 648    |\n",
      "|    iterations      | 274    |\n",
      "|    time_elapsed    | 864    |\n",
      "|    total_timesteps | 561152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 275          |\n",
      "|    time_elapsed         | 866          |\n",
      "|    total_timesteps      | 563200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.236444e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.982       |\n",
      "|    explained_variance   | 0.0194       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.000328    |\n",
      "|    std                  | 0.646        |\n",
      "|    value_loss           | 6.96e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-15174.24 +/- 13044.03\n",
      "Episode length: 338.38 +/- 50.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 338           |\n",
      "|    mean_reward          | -1.52e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 564000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057970424 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.979        |\n",
      "|    explained_variance   | 0.0219        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.62e+05      |\n",
      "|    n_updates            | 2750          |\n",
      "|    policy_gradient_loss | -0.00065      |\n",
      "|    std                  | 0.644         |\n",
      "|    value_loss           | 1.05e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 649    |\n",
      "|    iterations      | 276    |\n",
      "|    time_elapsed    | 870    |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 649           |\n",
      "|    iterations           | 277           |\n",
      "|    time_elapsed         | 872           |\n",
      "|    total_timesteps      | 567296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025506504 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.979        |\n",
      "|    explained_variance   | 0.0228        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.32e+05      |\n",
      "|    n_updates            | 2760          |\n",
      "|    policy_gradient_loss | -0.000239     |\n",
      "|    std                  | 0.644         |\n",
      "|    value_loss           | 4.5e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-15822.22 +/- 12020.85\n",
      "Episode length: 318.50 +/- 50.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -1.58e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 568000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013714844 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.978       |\n",
      "|    explained_variance   | 0.0811       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.34e+05     |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 3.39e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 649    |\n",
      "|    iterations      | 278    |\n",
      "|    time_elapsed    | 876    |\n",
      "|    total_timesteps | 569344 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 650           |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 878           |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7649297e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.977        |\n",
      "|    explained_variance   | 0.0302        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | 3.64e-05      |\n",
      "|    std                  | 0.643         |\n",
      "|    value_loss           | 1.11e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=-8551.49 +/- 8817.49\n",
      "Episode length: 289.50 +/- 29.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -8.55e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 572000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.164566e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.977       |\n",
      "|    explained_variance   | 0.0148       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72e+06     |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.54e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 650    |\n",
      "|    iterations      | 280    |\n",
      "|    time_elapsed    | 881    |\n",
      "|    total_timesteps | 573440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 281          |\n",
      "|    time_elapsed         | 883          |\n",
      "|    total_timesteps      | 575488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032997793 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.976       |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.53e+05     |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.000765    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.21e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-10748.40 +/- 9379.97\n",
      "Episode length: 344.25 +/- 24.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -1.07e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 576000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040525254 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.966       |\n",
      "|    explained_variance   | 0.00703      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.8e+05      |\n",
      "|    n_updates            | 2810         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.63         |\n",
      "|    value_loss           | 1.81e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 650    |\n",
      "|    iterations      | 282    |\n",
      "|    time_elapsed    | 887    |\n",
      "|    total_timesteps | 577536 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 283          |\n",
      "|    time_elapsed         | 889          |\n",
      "|    total_timesteps      | 579584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033275718 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.957       |\n",
      "|    explained_variance   | 0.00254      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.63e+04     |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 1.51e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-9475.61 +/- 10153.82\n",
      "Episode length: 335.38 +/- 30.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -9.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 580000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.718926e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.959       |\n",
      "|    explained_variance   | 0.00821      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.71e+04     |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -4.31e-05    |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 6.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 651    |\n",
      "|    iterations      | 284    |\n",
      "|    time_elapsed    | 893    |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 895          |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.473341e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.0293       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.69e+04     |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -6.19e-05    |\n",
      "|    std                  | 0.632        |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-11957.76 +/- 11978.30\n",
      "Episode length: 330.62 +/- 38.03\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 331           |\n",
      "|    mean_reward          | -1.2e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 584000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049422064 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.962        |\n",
      "|    explained_variance   | 0.0295        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.48e+04      |\n",
      "|    n_updates            | 2850          |\n",
      "|    policy_gradient_loss | -0.000329     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 3.94e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 651    |\n",
      "|    iterations      | 286    |\n",
      "|    time_elapsed    | 898    |\n",
      "|    total_timesteps | 585728 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 287          |\n",
      "|    time_elapsed         | 900          |\n",
      "|    total_timesteps      | 587776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010901398 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.964       |\n",
      "|    explained_variance   | 0.0446       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.75e+05     |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.000782    |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 2.54e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-12688.28 +/- 14870.17\n",
      "Episode length: 308.00 +/- 65.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -1.27e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 588000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013048529 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.965        |\n",
      "|    explained_variance   | 0.0137        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.02e+05      |\n",
      "|    n_updates            | 2870          |\n",
      "|    policy_gradient_loss | -0.000132     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 1.5e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 652    |\n",
      "|    iterations      | 288    |\n",
      "|    time_elapsed    | 904    |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 906          |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029450473 |\n",
      "|    clip_fraction        | 0.0062       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.963       |\n",
      "|    explained_variance   | 0.0105       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.31e+05     |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 1.65e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-8379.36 +/- 7595.19\n",
      "Episode length: 315.50 +/- 55.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 316           |\n",
      "|    mean_reward          | -8.38e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 592000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027296916 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.966        |\n",
      "|    explained_variance   | 0.00336       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.44e+05      |\n",
      "|    n_updates            | 2890          |\n",
      "|    policy_gradient_loss | -0.000821     |\n",
      "|    std                  | 0.636         |\n",
      "|    value_loss           | 1.18e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 652    |\n",
      "|    iterations      | 290    |\n",
      "|    time_elapsed    | 909    |\n",
      "|    total_timesteps | 593920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 911         |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005353211 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.000365    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.26e+03    |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    std                  | 0.636       |\n",
      "|    value_loss           | 1.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-6402.55 +/- 7609.88\n",
      "Episode length: 281.00 +/- 49.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 281           |\n",
      "|    mean_reward          | -6.4e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 596000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015472111 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.968        |\n",
      "|    explained_variance   | -0.0987       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.79e+04      |\n",
      "|    n_updates            | 2910          |\n",
      "|    policy_gradient_loss | 0.000239      |\n",
      "|    std                  | 0.64          |\n",
      "|    value_loss           | 3.2e+04       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 292    |\n",
      "|    time_elapsed    | 915    |\n",
      "|    total_timesteps | 598016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-8353.02 +/- 12645.68\n",
      "Episode length: 321.38 +/- 42.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 321         |\n",
      "|    mean_reward          | -8.35e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.78435e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.0204      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26e+04    |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.00013    |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 1.11e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 293    |\n",
      "|    time_elapsed    | 918    |\n",
      "|    total_timesteps | 600064 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 654           |\n",
      "|    iterations           | 294           |\n",
      "|    time_elapsed         | 920           |\n",
      "|    total_timesteps      | 602112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044501814 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.974        |\n",
      "|    explained_variance   | 0.0155        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.12e+05      |\n",
      "|    n_updates            | 2930          |\n",
      "|    policy_gradient_loss | -0.000142     |\n",
      "|    std                  | 0.642         |\n",
      "|    value_loss           | 3.02e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=-7120.11 +/- 6845.32\n",
      "Episode length: 319.88 +/- 36.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -7.12e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 604000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0218544e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.976        |\n",
      "|    explained_variance   | 0.0435        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.32e+05      |\n",
      "|    n_updates            | 2940          |\n",
      "|    policy_gradient_loss | -0.00018      |\n",
      "|    std                  | 0.643         |\n",
      "|    value_loss           | 3e+06         |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 295    |\n",
      "|    time_elapsed    | 923    |\n",
      "|    total_timesteps | 604160 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 654          |\n",
      "|    iterations           | 296          |\n",
      "|    time_elapsed         | 926          |\n",
      "|    total_timesteps      | 606208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016751048 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.974       |\n",
      "|    explained_variance   | 0.0176       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.38e+04     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000888    |\n",
      "|    std                  | 0.638        |\n",
      "|    value_loss           | 2.48e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=-4775.57 +/- 6818.79\n",
      "Episode length: 328.75 +/- 40.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 329          |\n",
      "|    mean_reward          | -4.78e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 608000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045843786 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.969       |\n",
      "|    explained_variance   | -0.00771     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.49e+03     |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    std                  | 0.636        |\n",
      "|    value_loss           | 2.7e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 297    |\n",
      "|    time_elapsed    | 929    |\n",
      "|    total_timesteps | 608256 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 654           |\n",
      "|    iterations           | 298           |\n",
      "|    time_elapsed         | 932           |\n",
      "|    total_timesteps      | 610304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024371682 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.967        |\n",
      "|    explained_variance   | 0.0417        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 2970          |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    std                  | 0.637         |\n",
      "|    value_loss           | 2.15e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-7597.05 +/- 7996.07\n",
      "Episode length: 318.38 +/- 39.03\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 318           |\n",
      "|    mean_reward          | -7.6e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 612000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045988685 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.969        |\n",
      "|    explained_variance   | 0.0521        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.98e+05      |\n",
      "|    n_updates            | 2980          |\n",
      "|    policy_gradient_loss | -0.00047      |\n",
      "|    std                  | 0.639         |\n",
      "|    value_loss           | 8.36e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 299    |\n",
      "|    time_elapsed    | 935    |\n",
      "|    total_timesteps | 612352 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 654           |\n",
      "|    iterations           | 300           |\n",
      "|    time_elapsed         | 938           |\n",
      "|    total_timesteps      | 614400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094475737 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.973        |\n",
      "|    explained_variance   | 0.041         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.09e+05      |\n",
      "|    n_updates            | 2990          |\n",
      "|    policy_gradient_loss | -0.000486     |\n",
      "|    std                  | 0.642         |\n",
      "|    value_loss           | 6.39e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=-13127.78 +/- 10207.33\n",
      "Episode length: 331.00 +/- 46.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | -1.31e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 616000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009469364 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.975       |\n",
      "|    explained_variance   | 0.0689       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+04     |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -6.74e-05    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 4.78e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 301    |\n",
      "|    time_elapsed    | 941    |\n",
      "|    total_timesteps | 616448 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 655           |\n",
      "|    iterations           | 302           |\n",
      "|    time_elapsed         | 944           |\n",
      "|    total_timesteps      | 618496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9778967e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.973        |\n",
      "|    explained_variance   | 0.0772        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.29e+05      |\n",
      "|    n_updates            | 3010          |\n",
      "|    policy_gradient_loss | -9.76e-05     |\n",
      "|    std                  | 0.639         |\n",
      "|    value_loss           | 3.17e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-5685.10 +/- 7269.76\n",
      "Episode length: 323.12 +/- 39.18\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -5.69e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 620000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010046175 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.971        |\n",
      "|    explained_variance   | 0.0447        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+06      |\n",
      "|    n_updates            | 3020          |\n",
      "|    policy_gradient_loss | -0.00035      |\n",
      "|    std                  | 0.638         |\n",
      "|    value_loss           | 1.31e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 303    |\n",
      "|    time_elapsed    | 947    |\n",
      "|    total_timesteps | 620544 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 655       |\n",
      "|    iterations           | 304       |\n",
      "|    time_elapsed         | 949       |\n",
      "|    total_timesteps      | 622592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0062416 |\n",
      "|    clip_fraction        | 0.0309    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.968    |\n",
      "|    explained_variance   | -0.0285   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.39e+04  |\n",
      "|    n_updates            | 3030      |\n",
      "|    policy_gradient_loss | -0.00235  |\n",
      "|    std                  | 0.635     |\n",
      "|    value_loss           | 2.72e+04  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=-9890.63 +/- 8967.13\n",
      "Episode length: 320.38 +/- 53.28\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -9.89e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 624000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00089257973 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.964        |\n",
      "|    explained_variance   | 0.0532        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.47e+04      |\n",
      "|    n_updates            | 3040          |\n",
      "|    policy_gradient_loss | -0.000298     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 2.41e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 655    |\n",
      "|    iterations      | 305    |\n",
      "|    time_elapsed    | 953    |\n",
      "|    total_timesteps | 624640 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 655           |\n",
      "|    iterations           | 306           |\n",
      "|    time_elapsed         | 955           |\n",
      "|    total_timesteps      | 626688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011848043 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.964        |\n",
      "|    explained_variance   | 0.0452        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.18e+05      |\n",
      "|    n_updates            | 3050          |\n",
      "|    policy_gradient_loss | -7.37e-05     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 4.72e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=-7851.40 +/- 6305.94\n",
      "Episode length: 337.50 +/- 29.29\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 338           |\n",
      "|    mean_reward          | -7.85e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 628000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012691194 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.966        |\n",
      "|    explained_variance   | 0.0569        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.82e+05      |\n",
      "|    n_updates            | 3060          |\n",
      "|    policy_gradient_loss | -0.000436     |\n",
      "|    std                  | 0.637         |\n",
      "|    value_loss           | 1.29e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 655    |\n",
      "|    iterations      | 307    |\n",
      "|    time_elapsed    | 958    |\n",
      "|    total_timesteps | 628736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 308          |\n",
      "|    time_elapsed         | 961          |\n",
      "|    total_timesteps      | 630784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005686477 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.969       |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+05     |\n",
      "|    n_updates            | 3070         |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    std                  | 0.637        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=-12943.94 +/- 12743.20\n",
      "Episode length: 334.25 +/- 52.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 334          |\n",
      "|    mean_reward          | -1.29e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 632000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021916688 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.967       |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.44e+04     |\n",
      "|    n_updates            | 3080         |\n",
      "|    policy_gradient_loss | -0.000672    |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 2.33e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 309    |\n",
      "|    time_elapsed    | 964    |\n",
      "|    total_timesteps | 632832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 310          |\n",
      "|    time_elapsed         | 967          |\n",
      "|    total_timesteps      | 634880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007686389 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.965       |\n",
      "|    explained_variance   | 0.0166       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.04e+05     |\n",
      "|    n_updates            | 3090         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 1.16e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=-8245.04 +/- 6936.24\n",
      "Episode length: 343.62 +/- 29.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -8.25e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 636000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004287958 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.964       |\n",
      "|    explained_variance   | 0.0132       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27e+05     |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | -5.97e-05    |\n",
      "|    std                  | 0.634        |\n",
      "|    value_loss           | 4.93e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 311    |\n",
      "|    time_elapsed    | 970    |\n",
      "|    total_timesteps | 636928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 312          |\n",
      "|    time_elapsed         | 972          |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016922648 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.032        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85e+05     |\n",
      "|    n_updates            | 3110         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 6.21e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-9357.68 +/- 8456.83\n",
      "Episode length: 338.50 +/- 31.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 338          |\n",
      "|    mean_reward          | -9.36e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007668876 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.0638       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.27e+05     |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -0.000115    |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 1.1e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 313    |\n",
      "|    time_elapsed    | 976    |\n",
      "|    total_timesteps | 641024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 656           |\n",
      "|    iterations           | 314           |\n",
      "|    time_elapsed         | 978           |\n",
      "|    total_timesteps      | 643072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7292415e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.961        |\n",
      "|    explained_variance   | 0.0238        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.83e+04      |\n",
      "|    n_updates            | 3130          |\n",
      "|    policy_gradient_loss | -5.94e-05     |\n",
      "|    std                  | 0.631         |\n",
      "|    value_loss           | 9.81e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=-13189.60 +/- 13009.17\n",
      "Episode length: 320.00 +/- 25.18\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -1.32e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 644000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047522812 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.96         |\n",
      "|    explained_variance   | 0.0183        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.31e+05      |\n",
      "|    n_updates            | 3140          |\n",
      "|    policy_gradient_loss | -0.000484     |\n",
      "|    std                  | 0.632         |\n",
      "|    value_loss           | 6.47e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 315    |\n",
      "|    time_elapsed    | 982    |\n",
      "|    total_timesteps | 645120 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 657           |\n",
      "|    iterations           | 316           |\n",
      "|    time_elapsed         | 984           |\n",
      "|    total_timesteps      | 647168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023463079 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.961        |\n",
      "|    explained_variance   | 0.0512        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.45e+05      |\n",
      "|    n_updates            | 3150          |\n",
      "|    policy_gradient_loss | -0.000276     |\n",
      "|    std                  | 0.633         |\n",
      "|    value_loss           | 8.3e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-10348.22 +/- 7746.28\n",
      "Episode length: 309.50 +/- 47.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 310          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 648000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005266466 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.962       |\n",
      "|    explained_variance   | 0.041        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03e+06     |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -0.00061     |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 1.48e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 657    |\n",
      "|    iterations      | 317    |\n",
      "|    time_elapsed    | 987    |\n",
      "|    total_timesteps | 649216 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 657          |\n",
      "|    iterations           | 318          |\n",
      "|    time_elapsed         | 990          |\n",
      "|    total_timesteps      | 651264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.279618e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.963       |\n",
      "|    explained_variance   | 0.0266       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.82e+03     |\n",
      "|    n_updates            | 3170         |\n",
      "|    policy_gradient_loss | -0.000155    |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 8.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-11395.01 +/- 12880.98\n",
      "Episode length: 319.75 +/- 48.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -1.14e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 652000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010862561 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.966        |\n",
      "|    explained_variance   | 0.0198        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+05      |\n",
      "|    n_updates            | 3180          |\n",
      "|    policy_gradient_loss | -8.51e-05     |\n",
      "|    std                  | 0.637         |\n",
      "|    value_loss           | 7.27e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 657    |\n",
      "|    iterations      | 319    |\n",
      "|    time_elapsed    | 993    |\n",
      "|    total_timesteps | 653312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 995         |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001540784 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0315      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.28e+04    |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    std                  | 0.637       |\n",
      "|    value_loss           | 4.93e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-6727.03 +/- 7877.21\n",
      "Episode length: 296.25 +/- 55.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 296          |\n",
      "|    mean_reward          | -6.73e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009466948 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.972       |\n",
      "|    explained_variance   | 0.0402       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.78e+05     |\n",
      "|    n_updates            | 3200         |\n",
      "|    policy_gradient_loss | -0.000641    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 657    |\n",
      "|    iterations      | 321    |\n",
      "|    time_elapsed    | 999    |\n",
      "|    total_timesteps | 657408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 1001        |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002496759 |\n",
      "|    clip_fraction        | 0.00146     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.978      |\n",
      "|    explained_variance   | 0.0197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.94e+05    |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.000857   |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 1.51e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-6834.06 +/- 6912.56\n",
      "Episode length: 319.88 +/- 37.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -6.83e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023697722 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.983       |\n",
      "|    explained_variance   | 0.0193       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+05     |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.648        |\n",
      "|    value_loss           | 7.28e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 323    |\n",
      "|    time_elapsed    | 1005   |\n",
      "|    total_timesteps | 661504 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 324          |\n",
      "|    time_elapsed         | 1007         |\n",
      "|    total_timesteps      | 663552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017855235 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.986       |\n",
      "|    explained_variance   | 0.0115       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.13e+05     |\n",
      "|    n_updates            | 3230         |\n",
      "|    policy_gradient_loss | -0.000537    |\n",
      "|    std                  | 0.649        |\n",
      "|    value_loss           | 8.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-10444.59 +/- 9270.79\n",
      "Episode length: 316.62 +/- 46.73\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 317           |\n",
      "|    mean_reward          | -1.04e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 664000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029307592 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.981        |\n",
      "|    explained_variance   | 0.0126        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.27e+04      |\n",
      "|    n_updates            | 3240          |\n",
      "|    policy_gradient_loss | -0.000286     |\n",
      "|    std                  | 0.64          |\n",
      "|    value_loss           | 3.85e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 325    |\n",
      "|    time_elapsed    | 1010   |\n",
      "|    total_timesteps | 665600 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 326          |\n",
      "|    time_elapsed         | 1013         |\n",
      "|    total_timesteps      | 667648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.991641e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.973       |\n",
      "|    explained_variance   | 0.0327       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+06     |\n",
      "|    n_updates            | 3250         |\n",
      "|    policy_gradient_loss | -0.000138    |\n",
      "|    std                  | 0.64         |\n",
      "|    value_loss           | 1.84e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-4542.47 +/- 5970.22\n",
      "Episode length: 290.00 +/- 36.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -4.54e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 668000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075372495 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | 0.0253       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.06e+04     |\n",
      "|    n_updates            | 3260         |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    std                  | 0.632        |\n",
      "|    value_loss           | 3.39e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 327    |\n",
      "|    time_elapsed    | 1016   |\n",
      "|    total_timesteps | 669696 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 328           |\n",
      "|    time_elapsed         | 1018          |\n",
      "|    total_timesteps      | 671744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031904742 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.96         |\n",
      "|    explained_variance   | 0.0874        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.92e+05      |\n",
      "|    n_updates            | 3270          |\n",
      "|    policy_gradient_loss | -0.00152      |\n",
      "|    std                  | 0.631         |\n",
      "|    value_loss           | 5.12e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-12838.18 +/- 8026.79\n",
      "Episode length: 328.62 +/- 43.83\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 329           |\n",
      "|    mean_reward          | -1.28e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 672000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015971079 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.96         |\n",
      "|    explained_variance   | 0.0199        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.07e+05      |\n",
      "|    n_updates            | 3280          |\n",
      "|    policy_gradient_loss | -0.00061      |\n",
      "|    std                  | 0.633         |\n",
      "|    value_loss           | 2.01e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 329    |\n",
      "|    time_elapsed    | 1021   |\n",
      "|    total_timesteps | 673792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 659         |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 1024        |\n",
      "|    total_timesteps      | 675840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004043618 |\n",
      "|    clip_fraction        | 0.00869     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97e+04    |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    std                  | 0.632       |\n",
      "|    value_loss           | 5.46e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=-13536.27 +/- 9523.61\n",
      "Episode length: 332.50 +/- 53.97\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 332           |\n",
      "|    mean_reward          | -1.35e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 676000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040161016 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.961        |\n",
      "|    explained_variance   | 0.0775        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.87e+05      |\n",
      "|    n_updates            | 3300          |\n",
      "|    policy_gradient_loss | -0.000432     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 9.89e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 331    |\n",
      "|    time_elapsed    | 1027   |\n",
      "|    total_timesteps | 677888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 1029        |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000448161 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.0868      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35e+05    |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.000425   |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 1.67e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-3287.91 +/- 5132.63\n",
      "Episode length: 284.75 +/- 32.66\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 285           |\n",
      "|    mean_reward          | -3.29e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 680000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030257914 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.959        |\n",
      "|    explained_variance   | 0.0193        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.27e+04      |\n",
      "|    n_updates            | 3320          |\n",
      "|    policy_gradient_loss | -0.000507     |\n",
      "|    std                  | 0.631         |\n",
      "|    value_loss           | 5.23e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 333    |\n",
      "|    time_elapsed    | 1033   |\n",
      "|    total_timesteps | 681984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-12865.74 +/- 9340.72\n",
      "Episode length: 298.62 +/- 49.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | -1.29e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 684000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005893209 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.955       |\n",
      "|    explained_variance   | 0.0548       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.13e+04     |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -0.000652    |\n",
      "|    std                  | 0.627        |\n",
      "|    value_loss           | 5.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 334    |\n",
      "|    time_elapsed    | 1036   |\n",
      "|    total_timesteps | 684032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 335          |\n",
      "|    time_elapsed         | 1038         |\n",
      "|    total_timesteps      | 686080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018892423 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+04     |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.626        |\n",
      "|    value_loss           | 8.67e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-6992.20 +/- 7985.57\n",
      "Episode length: 322.75 +/- 32.98\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -6.99e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 688000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056287245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.95         |\n",
      "|    explained_variance   | 0.0769        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.85e+05      |\n",
      "|    n_updates            | 3350          |\n",
      "|    policy_gradient_loss | -0.000333     |\n",
      "|    std                  | 0.625         |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 336    |\n",
      "|    time_elapsed    | 1042   |\n",
      "|    total_timesteps | 688128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 337          |\n",
      "|    time_elapsed         | 1044         |\n",
      "|    total_timesteps      | 690176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009600342 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+05     |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.000793    |\n",
      "|    std                  | 0.624        |\n",
      "|    value_loss           | 1.55e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=-6212.45 +/- 8277.85\n",
      "Episode length: 337.62 +/- 40.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 338           |\n",
      "|    mean_reward          | -6.21e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 692000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068882364 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.948        |\n",
      "|    explained_variance   | 0.0344        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.93e+05      |\n",
      "|    n_updates            | 3370          |\n",
      "|    policy_gradient_loss | -0.000628     |\n",
      "|    std                  | 0.625         |\n",
      "|    value_loss           | 1.67e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 338    |\n",
      "|    time_elapsed    | 1047   |\n",
      "|    total_timesteps | 692224 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 339           |\n",
      "|    time_elapsed         | 1050          |\n",
      "|    total_timesteps      | 694272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048444077 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.946        |\n",
      "|    explained_variance   | 0.0287        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8e+04         |\n",
      "|    n_updates            | 3380          |\n",
      "|    policy_gradient_loss | 1.54e-05      |\n",
      "|    std                  | 0.623         |\n",
      "|    value_loss           | 1.25e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-15248.16 +/- 8651.65\n",
      "Episode length: 345.50 +/- 56.06\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 346           |\n",
      "|    mean_reward          | -1.52e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 696000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011783565 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.945        |\n",
      "|    explained_variance   | 0.0365        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.33e+04      |\n",
      "|    n_updates            | 3390          |\n",
      "|    policy_gradient_loss | -0.000355     |\n",
      "|    std                  | 0.623         |\n",
      "|    value_loss           | 3.79e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 340    |\n",
      "|    time_elapsed    | 1053   |\n",
      "|    total_timesteps | 696320 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 341           |\n",
      "|    time_elapsed         | 1055          |\n",
      "|    total_timesteps      | 698368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9455746e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.946        |\n",
      "|    explained_variance   | 0.0485        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.65e+06      |\n",
      "|    n_updates            | 3400          |\n",
      "|    policy_gradient_loss | -0.000239     |\n",
      "|    std                  | 0.623         |\n",
      "|    value_loss           | 1.41e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-4238.70 +/- 6855.10\n",
      "Episode length: 335.75 +/- 36.26\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 336           |\n",
      "|    mean_reward          | -4.24e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 700000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8857215e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.944        |\n",
      "|    explained_variance   | 0.0431        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+05      |\n",
      "|    n_updates            | 3410          |\n",
      "|    policy_gradient_loss | -0.000195     |\n",
      "|    std                  | 0.621         |\n",
      "|    value_loss           | 6.44e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 342    |\n",
      "|    time_elapsed    | 1059   |\n",
      "|    total_timesteps | 700416 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 343          |\n",
      "|    time_elapsed         | 1061         |\n",
      "|    total_timesteps      | 702464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.966119e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.942       |\n",
      "|    explained_variance   | 0.051        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.13e+04     |\n",
      "|    n_updates            | 3420         |\n",
      "|    policy_gradient_loss | -0.000196    |\n",
      "|    std                  | 0.621        |\n",
      "|    value_loss           | 5.09e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=-8439.54 +/- 7546.47\n",
      "Episode length: 315.88 +/- 28.79\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 316           |\n",
      "|    mean_reward          | -8.44e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 704000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2571152e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.943        |\n",
      "|    explained_variance   | 0.0372        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.08e+05      |\n",
      "|    n_updates            | 3430          |\n",
      "|    policy_gradient_loss | -4.42e-05     |\n",
      "|    std                  | 0.621         |\n",
      "|    value_loss           | 1.08e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 344    |\n",
      "|    time_elapsed    | 1064   |\n",
      "|    total_timesteps | 704512 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 345          |\n",
      "|    time_elapsed         | 1067         |\n",
      "|    total_timesteps      | 706560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012662278 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.942       |\n",
      "|    explained_variance   | 0.0445       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+05     |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    std                  | 0.62         |\n",
      "|    value_loss           | 1.55e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-10457.76 +/- 10870.27\n",
      "Episode length: 325.50 +/- 63.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -1.05e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 708000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5535192e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.0676        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.31e+05      |\n",
      "|    n_updates            | 3450          |\n",
      "|    policy_gradient_loss | -2.64e-06     |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 4.6e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 346    |\n",
      "|    time_elapsed    | 1070   |\n",
      "|    total_timesteps | 708608 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 662           |\n",
      "|    iterations           | 347           |\n",
      "|    time_elapsed         | 1072          |\n",
      "|    total_timesteps      | 710656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8043305e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.0294        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.13e+05      |\n",
      "|    n_updates            | 3460          |\n",
      "|    policy_gradient_loss | -1.4e-05      |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 1.12e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=-5381.10 +/- 6844.10\n",
      "Episode length: 312.12 +/- 51.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 312           |\n",
      "|    mean_reward          | -5.38e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 712000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015865035 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.0202        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.68e+05      |\n",
      "|    n_updates            | 3470          |\n",
      "|    policy_gradient_loss | -0.000402     |\n",
      "|    std                  | 0.618         |\n",
      "|    value_loss           | 7.92e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 348    |\n",
      "|    time_elapsed    | 1075   |\n",
      "|    total_timesteps | 712704 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 662           |\n",
      "|    iterations           | 349           |\n",
      "|    time_elapsed         | 1078          |\n",
      "|    total_timesteps      | 714752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5353409e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.027         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.01e+05      |\n",
      "|    n_updates            | 3480          |\n",
      "|    policy_gradient_loss | -0.000147     |\n",
      "|    std                  | 0.62          |\n",
      "|    value_loss           | 2.56e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=-14310.25 +/- 14153.50\n",
      "Episode length: 342.62 +/- 29.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | -1.43e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 716000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017376039 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.0224       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.89e+04     |\n",
      "|    n_updates            | 3490         |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.618        |\n",
      "|    value_loss           | 5.61e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 350    |\n",
      "|    time_elapsed    | 1081   |\n",
      "|    total_timesteps | 716800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 351          |\n",
      "|    time_elapsed         | 1083         |\n",
      "|    total_timesteps      | 718848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025489654 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.934       |\n",
      "|    explained_variance   | 0.046        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+05     |\n",
      "|    n_updates            | 3500         |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 5.97e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-7186.84 +/- 8227.53\n",
      "Episode length: 343.62 +/- 39.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -7.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.953965e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | 0.00965      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11e+05     |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.000382    |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 5.07e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 352    |\n",
      "|    time_elapsed    | 1087   |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 353          |\n",
      "|    time_elapsed         | 1089         |\n",
      "|    total_timesteps      | 722944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071901083 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.93        |\n",
      "|    explained_variance   | 0.0391       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.79e+04     |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 1.75e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=-8791.12 +/- 13453.42\n",
      "Episode length: 292.00 +/- 51.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 292          |\n",
      "|    mean_reward          | -8.79e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 724000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020409594 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.0637       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.44e+04     |\n",
      "|    n_updates            | 3530         |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    std                  | 0.613        |\n",
      "|    value_loss           | 1.14e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 354    |\n",
      "|    time_elapsed    | 1092   |\n",
      "|    total_timesteps | 724992 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 663           |\n",
      "|    iterations           | 355           |\n",
      "|    time_elapsed         | 1095          |\n",
      "|    total_timesteps      | 727040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046307108 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.929        |\n",
      "|    explained_variance   | 0.0492        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.63e+05      |\n",
      "|    n_updates            | 3540          |\n",
      "|    policy_gradient_loss | -0.000529     |\n",
      "|    std                  | 0.613         |\n",
      "|    value_loss           | 7.19e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=-9483.32 +/- 5932.76\n",
      "Episode length: 311.62 +/- 41.16\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 312           |\n",
      "|    mean_reward          | -9.48e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 728000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8839563e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.928        |\n",
      "|    explained_variance   | 0.0561        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.14e+05      |\n",
      "|    n_updates            | 3550          |\n",
      "|    policy_gradient_loss | -0.000688     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 2.03e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 356    |\n",
      "|    time_elapsed    | 1098   |\n",
      "|    total_timesteps | 729088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 357          |\n",
      "|    time_elapsed         | 1100         |\n",
      "|    total_timesteps      | 731136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010288721 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.932       |\n",
      "|    explained_variance   | 0.0367       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.23e+05     |\n",
      "|    n_updates            | 3560         |\n",
      "|    policy_gradient_loss | -0.000686    |\n",
      "|    std                  | 0.616        |\n",
      "|    value_loss           | 8.32e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-10312.07 +/- 9232.80\n",
      "Episode length: 316.88 +/- 30.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 732000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002050134 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.937       |\n",
      "|    explained_variance   | 0.0196       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.76e+05     |\n",
      "|    n_updates            | 3570         |\n",
      "|    policy_gradient_loss | -8.78e-05    |\n",
      "|    std                  | 0.619        |\n",
      "|    value_loss           | 5.24e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 358    |\n",
      "|    time_elapsed    | 1104   |\n",
      "|    total_timesteps | 733184 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 664           |\n",
      "|    iterations           | 359           |\n",
      "|    time_elapsed         | 1106          |\n",
      "|    total_timesteps      | 735232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013422921 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.94         |\n",
      "|    explained_variance   | 0.0582        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+06      |\n",
      "|    n_updates            | 3580          |\n",
      "|    policy_gradient_loss | -0.000273     |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 1.42e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=-8445.90 +/- 9243.03\n",
      "Episode length: 274.00 +/- 42.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 274           |\n",
      "|    mean_reward          | -8.45e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 736000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039487373 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.94         |\n",
      "|    explained_variance   | 0.0261        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.21e+05      |\n",
      "|    n_updates            | 3590          |\n",
      "|    policy_gradient_loss | -0.000904     |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 1.13e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 360    |\n",
      "|    time_elapsed    | 1109   |\n",
      "|    total_timesteps | 737280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 361          |\n",
      "|    time_elapsed         | 1111         |\n",
      "|    total_timesteps      | 739328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.051387e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.0412       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.55e+05     |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -7.11e-05    |\n",
      "|    std                  | 0.618        |\n",
      "|    value_loss           | 5.16e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-7366.82 +/- 10576.07\n",
      "Episode length: 328.25 +/- 60.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 328        |\n",
      "|    mean_reward          | -7.37e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00131999 |\n",
      "|    clip_fraction        | 4.88e-05   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | -0.00261   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.25e+05   |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | -0.000654  |\n",
      "|    std                  | 0.616      |\n",
      "|    value_loss           | 1.92e+05   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 362    |\n",
      "|    time_elapsed    | 1115   |\n",
      "|    total_timesteps | 741376 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 665           |\n",
      "|    iterations           | 363           |\n",
      "|    time_elapsed         | 1117          |\n",
      "|    total_timesteps      | 743424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020281017 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.935        |\n",
      "|    explained_variance   | -0.0162       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.24e+04      |\n",
      "|    n_updates            | 3620          |\n",
      "|    policy_gradient_loss | 2.44e-05      |\n",
      "|    std                  | 0.616         |\n",
      "|    value_loss           | 2.6e+04       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=-9110.13 +/- 8522.36\n",
      "Episode length: 315.50 +/- 50.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | -9.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 744000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004965195 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | 0.0601       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.74e+05     |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.000789    |\n",
      "|    std                  | 0.614        |\n",
      "|    value_loss           | 2.2e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 364    |\n",
      "|    time_elapsed    | 1121   |\n",
      "|    total_timesteps | 745472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 665          |\n",
      "|    iterations           | 365          |\n",
      "|    time_elapsed         | 1123         |\n",
      "|    total_timesteps      | 747520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.526239e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | 0.0345       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+05     |\n",
      "|    n_updates            | 3640         |\n",
      "|    policy_gradient_loss | -0.00014     |\n",
      "|    std                  | 0.611        |\n",
      "|    value_loss           | 2.15e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=-13529.44 +/- 11219.13\n",
      "Episode length: 325.38 +/- 54.38\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 325           |\n",
      "|    mean_reward          | -1.35e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 748000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6162473e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.927        |\n",
      "|    explained_variance   | 0.0486        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.48e+05      |\n",
      "|    n_updates            | 3650          |\n",
      "|    policy_gradient_loss | -0.000166     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 1.94e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 366    |\n",
      "|    time_elapsed    | 1126   |\n",
      "|    total_timesteps | 749568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 665          |\n",
      "|    iterations           | 367          |\n",
      "|    time_elapsed         | 1129         |\n",
      "|    total_timesteps      | 751616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032826755 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.0843       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.000932    |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 2.69e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=-2437.01 +/- 1795.12\n",
      "Episode length: 302.50 +/- 34.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 302          |\n",
      "|    mean_reward          | -2.44e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 752000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001927518 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.93        |\n",
      "|    explained_variance   | 0.0904       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.48e+05     |\n",
      "|    n_updates            | 3670         |\n",
      "|    policy_gradient_loss | -0.000592    |\n",
      "|    std                  | 0.614        |\n",
      "|    value_loss           | 7.6e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 368    |\n",
      "|    time_elapsed    | 1132   |\n",
      "|    total_timesteps | 753664 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 369           |\n",
      "|    time_elapsed         | 1134          |\n",
      "|    total_timesteps      | 755712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023184823 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.931        |\n",
      "|    explained_variance   | 0.0751        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7e+05         |\n",
      "|    n_updates            | 3680          |\n",
      "|    policy_gradient_loss | -0.000219     |\n",
      "|    std                  | 0.614         |\n",
      "|    value_loss           | 1.35e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-5834.98 +/- 5462.78\n",
      "Episode length: 320.50 +/- 37.50\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -5.83e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 756000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5065849e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.93         |\n",
      "|    explained_variance   | 0.0443        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.58e+05      |\n",
      "|    n_updates            | 3690          |\n",
      "|    policy_gradient_loss | -7.55e-05     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 1.73e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 370    |\n",
      "|    time_elapsed    | 1138   |\n",
      "|    total_timesteps | 757760 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 1140        |\n",
      "|    total_timesteps      | 759808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005339535 |\n",
      "|    clip_fraction        | 0.01        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.0378      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.84e+04    |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 5.65e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-11887.60 +/- 8612.98\n",
      "Episode length: 306.75 +/- 44.27\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 307           |\n",
      "|    mean_reward          | -1.19e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 760000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066271884 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.929        |\n",
      "|    explained_variance   | 0.0312        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+04      |\n",
      "|    n_updates            | 3710          |\n",
      "|    policy_gradient_loss | -0.00036      |\n",
      "|    std                  | 0.613         |\n",
      "|    value_loss           | 5.17e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 372    |\n",
      "|    time_elapsed    | 1143   |\n",
      "|    total_timesteps | 761856 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 373           |\n",
      "|    time_elapsed         | 1146          |\n",
      "|    total_timesteps      | 763904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044695704 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.931        |\n",
      "|    explained_variance   | 0.0492        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.15e+04      |\n",
      "|    n_updates            | 3720          |\n",
      "|    policy_gradient_loss | -4.09e-05     |\n",
      "|    std                  | 0.614         |\n",
      "|    value_loss           | 7.2e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=-11951.21 +/- 11525.33\n",
      "Episode length: 332.38 +/- 47.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | -1.2e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 764000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000488832 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.0485      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.48e+03    |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.000294   |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 8.36e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 374    |\n",
      "|    time_elapsed    | 1149   |\n",
      "|    total_timesteps | 765952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-4346.11 +/- 5606.42\n",
      "Episode length: 294.88 +/- 34.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | -4.35e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 768000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010336225 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.923       |\n",
      "|    explained_variance   | 0.048        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.58e+04     |\n",
      "|    n_updates            | 3740         |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    std                  | 0.607        |\n",
      "|    value_loss           | 1.63e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 375    |\n",
      "|    time_elapsed    | 1152   |\n",
      "|    total_timesteps | 768000 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 376           |\n",
      "|    time_elapsed         | 1155          |\n",
      "|    total_timesteps      | 770048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011841064 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.92         |\n",
      "|    explained_variance   | 0.0594        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.01e+05      |\n",
      "|    n_updates            | 3750          |\n",
      "|    policy_gradient_loss | -0.000369     |\n",
      "|    std                  | 0.607         |\n",
      "|    value_loss           | 1.07e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=-11646.70 +/- 11022.79\n",
      "Episode length: 313.75 +/- 39.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 314           |\n",
      "|    mean_reward          | -1.16e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 772000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8558421e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.921        |\n",
      "|    explained_variance   | 0.089         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.17e+05      |\n",
      "|    n_updates            | 3760          |\n",
      "|    policy_gradient_loss | -0.000214     |\n",
      "|    std                  | 0.609         |\n",
      "|    value_loss           | 1.24e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 377    |\n",
      "|    time_elapsed    | 1158   |\n",
      "|    total_timesteps | 772096 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 1161        |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004055001 |\n",
      "|    clip_fraction        | 0.00552     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | -0.0121     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.76e+04    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 7.1e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=-6229.65 +/- 8128.46\n",
      "Episode length: 318.62 +/- 64.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -6.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 776000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008413332 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | 0.0532       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+06     |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.000925    |\n",
      "|    std                  | 0.606        |\n",
      "|    value_loss           | 3.42e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 379    |\n",
      "|    time_elapsed    | 1164   |\n",
      "|    total_timesteps | 776192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 380          |\n",
      "|    time_elapsed         | 1167         |\n",
      "|    total_timesteps      | 778240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022699977 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.0518       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.96e+05     |\n",
      "|    n_updates            | 3790         |\n",
      "|    policy_gradient_loss | -0.000746    |\n",
      "|    std                  | 0.608        |\n",
      "|    value_loss           | 8.59e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-6886.89 +/- 7744.18\n",
      "Episode length: 312.50 +/- 47.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -6.89e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 780000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041607018 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.921       |\n",
      "|    explained_variance   | -0.189       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.19e+04     |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.607        |\n",
      "|    value_loss           | 4.76e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 381    |\n",
      "|    time_elapsed    | 1170   |\n",
      "|    total_timesteps | 780288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 1172        |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005365661 |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.0966      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97e+04    |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 1.27e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=-10173.49 +/- 11441.13\n",
      "Episode length: 313.88 +/- 30.75\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 314           |\n",
      "|    mean_reward          | -1.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 784000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023582758 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.924        |\n",
      "|    explained_variance   | 0.0731        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.37e+05      |\n",
      "|    n_updates            | 3820          |\n",
      "|    policy_gradient_loss | -0.000177     |\n",
      "|    std                  | 0.608         |\n",
      "|    value_loss           | 3.64e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 383    |\n",
      "|    time_elapsed    | 1175   |\n",
      "|    total_timesteps | 784384 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 384          |\n",
      "|    time_elapsed         | 1178         |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016898849 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.0856       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47e+05     |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -0.000897    |\n",
      "|    std                  | 0.607        |\n",
      "|    value_loss           | 9.49e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=-7244.10 +/- 9967.73\n",
      "Episode length: 307.50 +/- 37.90\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -7.24e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 788000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032482797 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.921        |\n",
      "|    explained_variance   | 0.0539        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.18e+04      |\n",
      "|    n_updates            | 3840          |\n",
      "|    policy_gradient_loss | -0.000397     |\n",
      "|    std                  | 0.609         |\n",
      "|    value_loss           | 7.01e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 385    |\n",
      "|    time_elapsed    | 1181   |\n",
      "|    total_timesteps | 788480 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 667           |\n",
      "|    iterations           | 386           |\n",
      "|    time_elapsed         | 1183          |\n",
      "|    total_timesteps      | 790528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074512826 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.922        |\n",
      "|    explained_variance   | 0.0702        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.25e+06      |\n",
      "|    n_updates            | 3850          |\n",
      "|    policy_gradient_loss | -0.000405     |\n",
      "|    std                  | 0.609         |\n",
      "|    value_loss           | 2.34e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=-7336.50 +/- 8672.79\n",
      "Episode length: 363.25 +/- 41.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 363          |\n",
      "|    mean_reward          | -7.34e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 792000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014969034 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.924       |\n",
      "|    explained_variance   | 0.0566       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.59e+05     |\n",
      "|    n_updates            | 3860         |\n",
      "|    policy_gradient_loss | -0.00077     |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 4.94e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 387    |\n",
      "|    time_elapsed    | 1187   |\n",
      "|    total_timesteps | 792576 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 667           |\n",
      "|    iterations           | 388           |\n",
      "|    time_elapsed         | 1189          |\n",
      "|    total_timesteps      | 794624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012797232 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.926        |\n",
      "|    explained_variance   | 0.0578        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.72e+05      |\n",
      "|    n_updates            | 3870          |\n",
      "|    policy_gradient_loss | -0.000405     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 7.46e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=-7382.65 +/- 8404.15\n",
      "Episode length: 324.00 +/- 52.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | -7.38e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 796000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003685697 |\n",
      "|    clip_fraction        | 0.00303     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.0598      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.84e+05    |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 7.84e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 389    |\n",
      "|    time_elapsed    | 1192   |\n",
      "|    total_timesteps | 796672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 390          |\n",
      "|    time_elapsed         | 1195         |\n",
      "|    total_timesteps      | 798720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025102235 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.0494       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.27e+04     |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    std                  | 0.611        |\n",
      "|    value_loss           | 7.92e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-5262.91 +/- 6744.25\n",
      "Episode length: 364.50 +/- 45.22\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 364           |\n",
      "|    mean_reward          | -5.26e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 800000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014301453 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.928        |\n",
      "|    explained_variance   | 0.0604        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+05      |\n",
      "|    n_updates            | 3900          |\n",
      "|    policy_gradient_loss | -0.000172     |\n",
      "|    std                  | 0.614         |\n",
      "|    value_loss           | 4.74e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 668    |\n",
      "|    iterations      | 391    |\n",
      "|    time_elapsed    | 1198   |\n",
      "|    total_timesteps | 800768 |\n",
      "-------------------------------\n",
      "Before training: mean_reward:-7803.66 +/- 8093.65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#vec_env = VecNormalize(env, norm_obs=True, norm_reward=True,clip_obs=1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=800000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Before training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b31937afa3a071",
   "metadata": {},
   "source": [
    "### 再训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdc253940e03362",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard/ppo_run_1713689561.763529_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 352       |\n",
      "|    ep_rew_mean     | -2.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 858       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-1569.59 +/- 155.36\n",
      "Episode length: 316.62 +/- 26.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | -1.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058822203 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.78        |\n",
      "|    explained_variance   | 0.0603       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1e+05        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00406     |\n",
      "|    std                  | 0.529        |\n",
      "|    value_loss           | 6.77e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 361       |\n",
      "|    ep_rew_mean     | -2.16e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 524       |\n",
      "|    iterations      | 2         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 4096      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 368          |\n",
      "|    ep_rew_mean          | -2.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063983416 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.783       |\n",
      "|    explained_variance   | -0.0104      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.39e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    std                  | 0.526        |\n",
      "|    value_loss           | 2.1e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-2452.67 +/- 2379.23\n",
      "Episode length: 327.25 +/- 45.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -2.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063312966 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.773       |\n",
      "|    explained_variance   | -0.0402      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.64e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000994    |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | -1.6e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 492      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 397          |\n",
      "|    ep_rew_mean          | -2.25e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 508          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028919028 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.762       |\n",
      "|    explained_variance   | 0.0419       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.65e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00055     |\n",
      "|    std                  | 0.517        |\n",
      "|    value_loss           | 1.36e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-3188.37 +/- 2634.17\n",
      "Episode length: 326.12 +/- 42.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -3.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026175128 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.76        |\n",
      "|    explained_variance   | 0.0451       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.17e+04     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.518        |\n",
      "|    value_loss           | 2.35e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 346       |\n",
      "|    ep_rew_mean     | -1.96e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 482       |\n",
      "|    iterations      | 6         |\n",
      "|    time_elapsed    | 25        |\n",
      "|    total_timesteps | 12288     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 411        |\n",
      "|    ep_rew_mean          | -2.43e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00311386 |\n",
      "|    clip_fraction        | 0.0195     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.758     |\n",
      "|    explained_variance   | 0.0323     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.55e+03   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00161   |\n",
      "|    std                  | 0.515      |\n",
      "|    value_loss           | 1.37e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-4534.24 +/- 4297.53\n",
      "Episode length: 328.62 +/- 47.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | -4.53e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005017961 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.0837      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.91e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 9.62e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 524       |\n",
      "|    ep_rew_mean     | -2.44e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 477       |\n",
      "|    iterations      | 8         |\n",
      "|    time_elapsed    | 34        |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 406           |\n",
      "|    ep_rew_mean          | -2.69e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 486           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012775813 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.753        |\n",
      "|    explained_variance   | 0.0955        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.4e+06       |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -8.15e-05     |\n",
      "|    std                  | 0.514         |\n",
      "|    value_loss           | 2.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-4994.20 +/- 9029.20\n",
      "Episode length: 307.38 +/- 29.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -4.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008600367 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.753       |\n",
      "|    explained_variance   | 0.0535       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.69e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    std                  | 0.513        |\n",
      "|    value_loss           | 1.32e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 420       |\n",
      "|    ep_rew_mean     | -4.22e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 474       |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 20480     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 403          |\n",
      "|    ep_rew_mean          | -1.25e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026726848 |\n",
      "|    clip_fraction        | 0.00166      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.75        |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 2.14e-05     |\n",
      "|    std                  | 0.512        |\n",
      "|    value_loss           | 2.44e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-6483.63 +/- 5428.66\n",
      "Episode length: 311.00 +/- 33.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -6.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013369606 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.0831       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.72e+04     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    std                  | 0.512        |\n",
      "|    value_loss           | 8.83e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 479       |\n",
      "|    ep_rew_mean     | -2.77e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 12        |\n",
      "|    time_elapsed    | 51        |\n",
      "|    total_timesteps | 24576     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 392          |\n",
      "|    ep_rew_mean          | -2.22e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014479053 |\n",
      "|    clip_fraction        | 0.00337      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.752       |\n",
      "|    explained_variance   | 0.0649       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    std                  | 0.515        |\n",
      "|    value_loss           | 8.88e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-4733.43 +/- 5620.84\n",
      "Episode length: 304.00 +/- 40.87\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 304            |\n",
      "|    mean_reward          | -4.73e+03      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 28000          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000119711054 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.755         |\n",
      "|    explained_variance   | 0.0845         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.19e+06       |\n",
      "|    n_updates            | 130            |\n",
      "|    policy_gradient_loss | 4.11e-05       |\n",
      "|    std                  | 0.514          |\n",
      "|    value_loss           | 1.76e+06       |\n",
      "--------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 322       |\n",
      "|    ep_rew_mean     | -1.31e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 14        |\n",
      "|    time_elapsed    | 60        |\n",
      "|    total_timesteps | 28672     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 292         |\n",
      "|    ep_rew_mean          | -1.68e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002850104 |\n",
      "|    clip_fraction        | 0.00977     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.0565      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.92e+06    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000516   |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 2.25e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-6638.54 +/- 9644.13\n",
      "Episode length: 339.62 +/- 49.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -6.64e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003285362 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.75       |\n",
      "|    explained_variance   | 0.0552      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.77e+05    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    std                  | 0.513       |\n",
      "|    value_loss           | 5.24e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 635       |\n",
      "|    ep_rew_mean     | -3.72e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 473       |\n",
      "|    iterations      | 16        |\n",
      "|    time_elapsed    | 69        |\n",
      "|    total_timesteps | 32768     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 353          |\n",
      "|    ep_rew_mean          | -9.34e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031896797 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.57e+03     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000455    |\n",
      "|    std                  | 0.51         |\n",
      "|    value_loss           | 7.13e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-4783.72 +/- 8316.22\n",
      "Episode length: 324.25 +/- 52.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | -4.78e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004111281 |\n",
      "|    clip_fraction        | 0.0173      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.748      |\n",
      "|    explained_variance   | 0.0789      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.6e+04     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.000195   |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 5.49e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 724       |\n",
      "|    ep_rew_mean     | -4.68e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 471       |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 78        |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 364          |\n",
      "|    ep_rew_mean          | -2.12e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021906798 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+04     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 3.01e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-6056.64 +/- 4535.46\n",
      "Episode length: 346.75 +/- 29.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | -6.06e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002600375 |\n",
      "|    clip_fraction        | 0.00444     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.68e+05    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.506       |\n",
      "|    value_loss           | 8.03e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 233       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 469       |\n",
      "|    iterations      | 20        |\n",
      "|    time_elapsed    | 87        |\n",
      "|    total_timesteps | 40960     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 382        |\n",
      "|    ep_rew_mean          | -2.58e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00662597 |\n",
      "|    clip_fraction        | 0.0614     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.74      |\n",
      "|    explained_variance   | 0.0919     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.93e+05   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00276   |\n",
      "|    std                  | 0.508      |\n",
      "|    value_loss           | 4.56e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-5072.15 +/- 6037.74\n",
      "Episode length: 318.62 +/- 52.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -5.07e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014548573 |\n",
      "|    clip_fraction        | 0.00542      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.741       |\n",
      "|    explained_variance   | 0.0816       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52e+06     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 1.38e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 302       |\n",
      "|    ep_rew_mean     | -7.62e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 468       |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 96        |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 342          |\n",
      "|    ep_rew_mean          | -1.96e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023457012 |\n",
      "|    clip_fraction        | 0.00903      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.741       |\n",
      "|    explained_variance   | 0.0752       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 0.000231     |\n",
      "|    std                  | 0.507        |\n",
      "|    value_loss           | 6.98e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-4011.85 +/- 4455.01\n",
      "Episode length: 323.75 +/- 36.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 324           |\n",
      "|    mean_reward          | -4.01e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 48000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015237654 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.741        |\n",
      "|    explained_variance   | 0.0937        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.73e+05      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000385     |\n",
      "|    std                  | 0.509         |\n",
      "|    value_loss           | 1.72e+06      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 466       |\n",
      "|    ep_rew_mean     | -2.95e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 466       |\n",
      "|    iterations      | 24        |\n",
      "|    time_elapsed    | 105       |\n",
      "|    total_timesteps | 49152     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 452          |\n",
      "|    ep_rew_mean          | -2.73e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015266044 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.747       |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 4.15e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-5991.44 +/- 8010.79\n",
      "Episode length: 335.38 +/- 50.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -5.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021688156 |\n",
      "|    clip_fraction        | 0.00693      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.42e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000893    |\n",
      "|    std                  | 0.511        |\n",
      "|    value_loss           | 5.36e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 343       |\n",
      "|    ep_rew_mean     | -1.74e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 465       |\n",
      "|    iterations      | 26        |\n",
      "|    time_elapsed    | 114       |\n",
      "|    total_timesteps | 53248     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 331          |\n",
      "|    ep_rew_mean          | -1.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021966058 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.743       |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81e+05     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    std                  | 0.506        |\n",
      "|    value_loss           | 6.86e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-5078.20 +/- 8298.60\n",
      "Episode length: 329.75 +/- 45.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 330          |\n",
      "|    mean_reward          | -5.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015028131 |\n",
      "|    clip_fraction        | 0.0041       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.737       |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+05     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.505        |\n",
      "|    value_loss           | 3.04e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 223       |\n",
      "|    ep_rew_mean     | -1.27e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 464       |\n",
      "|    iterations      | 28        |\n",
      "|    time_elapsed    | 123       |\n",
      "|    total_timesteps | 57344     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 401          |\n",
      "|    ep_rew_mean          | -1.1e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 467          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018775349 |\n",
      "|    clip_fraction        | 0.00972      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.722       |\n",
      "|    explained_variance   | -0.0303      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57e+04     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    std                  | 0.489        |\n",
      "|    value_loss           | 2.05e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-1811.23 +/- 445.14\n",
      "Episode length: 325.25 +/- 33.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 325           |\n",
      "|    mean_reward          | -1.81e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020371772 |\n",
      "|    clip_fraction        | 0.00239       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.701        |\n",
      "|    explained_variance   | 0.0989        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.56e+05      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000872     |\n",
      "|    std                  | 0.487         |\n",
      "|    value_loss           | 1.13e+06      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 385       |\n",
      "|    ep_rew_mean     | -2.25e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 464       |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 132       |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 312           |\n",
      "|    ep_rew_mean          | -1.81e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 467           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 135           |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094628107 |\n",
      "|    clip_fraction        | 0.00488       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.703        |\n",
      "|    explained_variance   | 0.1           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.68e+05      |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000677     |\n",
      "|    std                  | 0.49          |\n",
      "|    value_loss           | 9.36e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-3390.58 +/- 3454.42\n",
      "Episode length: 345.38 +/- 43.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -3.39e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037301802 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | -0.0557      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.16e+03     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 0.000359     |\n",
      "|    std                  | 0.489        |\n",
      "|    value_loss           | 2.03e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 370       |\n",
      "|    ep_rew_mean     | -2.14e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 141       |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 387          |\n",
      "|    ep_rew_mean          | -3.01e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007198333 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | 0.111        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+06     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000788    |\n",
      "|    std                  | 0.49         |\n",
      "|    value_loss           | 1.86e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-2206.49 +/- 1420.76\n",
      "Episode length: 320.00 +/- 31.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 320         |\n",
      "|    mean_reward          | -2.21e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002094997 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.69e+04    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 5.15e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 387       |\n",
      "|    ep_rew_mean     | -2.35e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 34        |\n",
      "|    time_elapsed    | 150       |\n",
      "|    total_timesteps | 69632     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 260         |\n",
      "|    ep_rew_mean          | -1.47e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004254721 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.58e+05    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 8.91e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-3539.73 +/- 3615.82\n",
      "Episode length: 335.62 +/- 54.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 336         |\n",
      "|    mean_reward          | -3.54e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000949232 |\n",
      "|    clip_fraction        | 0.00454     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.712      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.54e+05    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 8.1e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 373       |\n",
      "|    ep_rew_mean     | -6.48e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 36        |\n",
      "|    time_elapsed    | 159       |\n",
      "|    total_timesteps | 73728     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 339          |\n",
      "|    ep_rew_mean          | -2.05e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009412368 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.713       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.36e+05     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000307    |\n",
      "|    std                  | 0.494        |\n",
      "|    value_loss           | 2.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-4980.19 +/- 5270.31\n",
      "Episode length: 350.62 +/- 39.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -4.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057352306 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.716       |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+06     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 1.31e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 357       |\n",
      "|    ep_rew_mean     | -2.11e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 38        |\n",
      "|    time_elapsed    | 168       |\n",
      "|    total_timesteps | 77824     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 415           |\n",
      "|    ep_rew_mean          | -2.62e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 465           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 171           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00082522136 |\n",
      "|    clip_fraction        | 0.0229        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.723        |\n",
      "|    explained_variance   | -0.104        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+04       |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | 0.000619      |\n",
      "|    std                  | 0.504         |\n",
      "|    value_loss           | 1.47e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-4732.66 +/- 4393.47\n",
      "Episode length: 307.38 +/- 45.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -4.73e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017169648 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.733       |\n",
      "|    explained_variance   | -0.0345      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+04     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | 0.000135     |\n",
      "|    std                  | 0.503        |\n",
      "|    value_loss           | 1.76e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 323       |\n",
      "|    ep_rew_mean     | -1.91e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 462       |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 176       |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 384          |\n",
      "|    ep_rew_mean          | -2.34e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021565063 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.737       |\n",
      "|    explained_variance   | -0.000626    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.76e+03     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.506        |\n",
      "|    value_loss           | 1.17e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-10185.04 +/- 8363.27\n",
      "Episode length: 328.12 +/- 24.97\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 328           |\n",
      "|    mean_reward          | -1.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 84000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033728796 |\n",
      "|    clip_fraction        | 0.00269       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.739        |\n",
      "|    explained_variance   | 0.115         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.84e+04      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000907     |\n",
      "|    std                  | 0.507         |\n",
      "|    value_loss           | 3.54e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 331       |\n",
      "|    ep_rew_mean     | -2.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 462       |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 185       |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-3078.67 +/- 3788.96\n",
      "Episode length: 321.88 +/- 40.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -3.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013052234 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.74        |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+06     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 3.06e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 303       |\n",
      "|    ep_rew_mean     | -1.89e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 43        |\n",
      "|    time_elapsed    | 191       |\n",
      "|    total_timesteps | 88064     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 432          |\n",
      "|    ep_rew_mean          | -6.6e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019128479 |\n",
      "|    clip_fraction        | 0.0085       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.74        |\n",
      "|    explained_variance   | -0.269       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.24e+03     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.000115     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 1.28e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-2644.02 +/- 2602.99\n",
      "Episode length: 334.38 +/- 38.07\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 334           |\n",
      "|    mean_reward          | -2.64e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 92000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016598165 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.741        |\n",
      "|    explained_variance   | 0.0759        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.81e+05      |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | 0.000104      |\n",
      "|    std                  | 0.508         |\n",
      "|    value_loss           | 1.1e+06       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 310       |\n",
      "|    ep_rew_mean     | -1.84e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 45        |\n",
      "|    time_elapsed    | 200       |\n",
      "|    total_timesteps | 92160     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 494        |\n",
      "|    ep_rew_mean          | -2.63e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 203        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00532081 |\n",
      "|    clip_fraction        | 0.0347     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.741     |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.29e+04   |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00418   |\n",
      "|    std                  | 0.508      |\n",
      "|    value_loss           | 1.32e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-2748.72 +/- 2913.96\n",
      "Episode length: 317.00 +/- 44.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 317         |\n",
      "|    mean_reward          | -2.75e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000838241 |\n",
      "|    clip_fraction        | 0.00151     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.742      |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.97e+05    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.000539   |\n",
      "|    std                  | 0.508       |\n",
      "|    value_loss           | 1.79e+06    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 259       |\n",
      "|    ep_rew_mean     | -1.55e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 209       |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 348          |\n",
      "|    ep_rew_mean          | -1.39e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014608537 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.739       |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 0.505        |\n",
      "|    value_loss           | 1.21e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-1529.32 +/- 243.81\n",
      "Episode length: 296.75 +/- 42.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 297          |\n",
      "|    mean_reward          | -1.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026470548 |\n",
      "|    clip_fraction        | 0.00522      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.732       |\n",
      "|    explained_variance   | 0.18         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.23e+04     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000927    |\n",
      "|    std                  | 0.501        |\n",
      "|    value_loss           | 3.42e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 367       |\n",
      "|    ep_rew_mean     | -5.79e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 218       |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 417          |\n",
      "|    ep_rew_mean          | -2.58e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 221          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012000814 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.161        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+04     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    std                  | 0.499        |\n",
      "|    value_loss           | 4.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-3982.99 +/- 4602.49\n",
      "Episode length: 331.00 +/- 42.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | -3.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032454222 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.723       |\n",
      "|    explained_variance   | -0.0159      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.03e+03     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000908    |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 2.19e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 519       |\n",
      "|    ep_rew_mean     | -3.11e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 51        |\n",
      "|    time_elapsed    | 226       |\n",
      "|    total_timesteps | 104448    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 378          |\n",
      "|    ep_rew_mean          | -3.92e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 230          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003424179 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.718       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+05     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -6.14e-06    |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 1.06e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-4345.40 +/- 5266.47\n",
      "Episode length: 321.50 +/- 48.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -4.35e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010046931 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.09e+05     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 9.42e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 555       |\n",
      "|    ep_rew_mean     | -2.13e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 53        |\n",
      "|    time_elapsed    | 236       |\n",
      "|    total_timesteps | 108544    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 302          |\n",
      "|    ep_rew_mean          | -1.8e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 239          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036690203 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+04     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 2.05e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-2110.87 +/- 824.01\n",
      "Episode length: 343.25 +/- 54.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | -2.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016387075 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.27e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00065     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 4.28e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 429       |\n",
      "|    ep_rew_mean     | -2.03e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 55        |\n",
      "|    time_elapsed    | 244       |\n",
      "|    total_timesteps | 112640    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 279          |\n",
      "|    ep_rew_mean          | -1.7e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004078106 |\n",
      "|    clip_fraction        | 0.00269      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.723       |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000656    |\n",
      "|    std                  | 0.499        |\n",
      "|    value_loss           | 6.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-3452.41 +/- 2400.15\n",
      "Episode length: 339.50 +/- 34.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -3.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065763537 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.33e+05     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 7.35e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 452       |\n",
      "|    ep_rew_mean     | -2.94e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 57        |\n",
      "|    time_elapsed    | 253       |\n",
      "|    total_timesteps | 116736    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 371          |\n",
      "|    ep_rew_mean          | -3.98e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058004893 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.704       |\n",
      "|    explained_variance   | -0.0608      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+03     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000732    |\n",
      "|    std                  | 0.482        |\n",
      "|    value_loss           | 1.58e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-3384.25 +/- 4677.14\n",
      "Episode length: 298.88 +/- 44.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | -3.38e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062474683 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.37e+03     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    std                  | 0.478        |\n",
      "|    value_loss           | 2.52e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 453       |\n",
      "|    ep_rew_mean     | -2.92e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 59        |\n",
      "|    time_elapsed    | 262       |\n",
      "|    total_timesteps | 120832    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 391          |\n",
      "|    ep_rew_mean          | -2.44e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 266          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019836836 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.677       |\n",
      "|    explained_variance   | -0.033       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+04     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000193    |\n",
      "|    std                  | 0.474        |\n",
      "|    value_loss           | 1.29e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-3181.15 +/- 3541.85\n",
      "Episode length: 349.25 +/- 42.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -3.18e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068063186 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.671       |\n",
      "|    explained_variance   | -0.00436     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.8e+03      |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000485    |\n",
      "|    std                  | 0.473        |\n",
      "|    value_loss           | 1.3e+04      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 471       |\n",
      "|    ep_rew_mean     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 61        |\n",
      "|    time_elapsed    | 271       |\n",
      "|    total_timesteps | 124928    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 417           |\n",
      "|    ep_rew_mean          | -2.55e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 461           |\n",
      "|    iterations           | 62            |\n",
      "|    time_elapsed         | 275           |\n",
      "|    total_timesteps      | 126976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0648727e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.67         |\n",
      "|    explained_variance   | 0.0882        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.65e+05      |\n",
      "|    n_updates            | 610           |\n",
      "|    policy_gradient_loss | 0.000562      |\n",
      "|    std                  | 0.473         |\n",
      "|    value_loss           | 1.67e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-3213.53 +/- 3929.27\n",
      "Episode length: 319.50 +/- 39.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -3.21e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025742967 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.49e+05     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.000959    |\n",
      "|    std                  | 0.471        |\n",
      "|    value_loss           | 6.54e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 414       |\n",
      "|    ep_rew_mean     | -2.59e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 63        |\n",
      "|    time_elapsed    | 280       |\n",
      "|    total_timesteps | 129024    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 280          |\n",
      "|    ep_rew_mean          | -1.63e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 283          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003508969 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.665       |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.28e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | 0.000251     |\n",
      "|    std                  | 0.47         |\n",
      "|    value_loss           | 4.43e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-1659.15 +/- 377.03\n",
      "Episode length: 306.62 +/- 62.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -1.66e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031317542 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.662       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+04     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.468        |\n",
      "|    value_loss           | 1.42e+06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 409      |\n",
      "|    ep_rew_mean     | -4e+03   |\n",
      "| time/              |          |\n",
      "|    fps             | 460      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 239          |\n",
      "|    ep_rew_mean          | -1.38e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 292          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065083606 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.31e+05     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | 0.000413     |\n",
      "|    std                  | 0.466        |\n",
      "|    value_loss           | 5.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-4984.59 +/- 5326.30\n",
      "Episode length: 367.88 +/- 26.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 368          |\n",
      "|    mean_reward          | -4.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013248659 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.98e+05     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    std                  | 0.469        |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 346       |\n",
      "|    ep_rew_mean     | -2.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 67        |\n",
      "|    time_elapsed    | 298       |\n",
      "|    total_timesteps | 137216    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 327          |\n",
      "|    ep_rew_mean          | -2.03e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 302          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020134463 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.657       |\n",
      "|    explained_variance   | -0.0203      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.86e+04     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.000152     |\n",
      "|    std                  | 0.462        |\n",
      "|    value_loss           | 1.82e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-2305.89 +/- 2014.16\n",
      "Episode length: 302.75 +/- 51.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 303         |\n",
      "|    mean_reward          | -2.31e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006426132 |\n",
      "|    clip_fraction        | 0.0581      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.22e+03    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    std                  | 0.462       |\n",
      "|    value_loss           | 8.69e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 440       |\n",
      "|    ep_rew_mean     | -2.73e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 69        |\n",
      "|    time_elapsed    | 307       |\n",
      "|    total_timesteps | 141312    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 406          |\n",
      "|    ep_rew_mean          | -2.17e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 311          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018653625 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.31e+05     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.0007      |\n",
      "|    std                  | 0.461        |\n",
      "|    value_loss           | 1.5e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-2635.58 +/- 1510.05\n",
      "Episode length: 345.50 +/- 57.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | -2.64e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006501942 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.28e+06    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.46        |\n",
      "|    value_loss           | 1.67e+06    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 369       |\n",
      "|    ep_rew_mean     | -2.48e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 71        |\n",
      "|    time_elapsed    | 316       |\n",
      "|    total_timesteps | 145408    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 343          |\n",
      "|    ep_rew_mean          | -2.06e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 320          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030993344 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.644       |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+04     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    std                  | 0.46         |\n",
      "|    value_loss           | 5.77e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-2654.33 +/- 1653.66\n",
      "Episode length: 351.75 +/- 36.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | -2.65e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 148000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003922368 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | -0.0563     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.73e+03    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.000808   |\n",
      "|    std                  | 0.452       |\n",
      "|    value_loss           | 2.05e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 377       |\n",
      "|    ep_rew_mean     | -2.49e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 73        |\n",
      "|    time_elapsed    | 325       |\n",
      "|    total_timesteps | 149504    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 388         |\n",
      "|    ep_rew_mean          | -2.39e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007848597 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.42e+05    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    std                  | 0.451       |\n",
      "|    value_loss           | 2.81e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-3270.78 +/- 3478.32\n",
      "Episode length: 349.75 +/- 24.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 350         |\n",
      "|    mean_reward          | -3.27e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008639485 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.01e+04    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 0.452       |\n",
      "|    value_loss           | 1.06e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 398       |\n",
      "|    ep_rew_mean     | -6.89e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 75        |\n",
      "|    time_elapsed    | 334       |\n",
      "|    total_timesteps | 153600    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | -5.22e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007765404 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+06     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 2.42e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-2616.08 +/- 2162.93\n",
      "Episode length: 319.50 +/- 41.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -2.62e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037877806 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03e+06     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    std                  | 0.451        |\n",
      "|    value_loss           | 1.13e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 372       |\n",
      "|    ep_rew_mean     | -2.34e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 77        |\n",
      "|    time_elapsed    | 339       |\n",
      "|    total_timesteps | 157696    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 465          |\n",
      "|    ep_rew_mean          | -2.94e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 342          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029011655 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.45         |\n",
      "|    value_loss           | 7.37e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-1900.30 +/- 298.08\n",
      "Episode length: 341.75 +/- 50.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | -1.9e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018599881 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | 0.000144     |\n",
      "|    std                  | 0.453        |\n",
      "|    value_loss           | 4.27e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 398       |\n",
      "|    ep_rew_mean     | -2.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 467       |\n",
      "|    iterations      | 79        |\n",
      "|    time_elapsed    | 345       |\n",
      "|    total_timesteps | 161792    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 385          |\n",
      "|    ep_rew_mean          | -2.35e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055852616 |\n",
      "|    clip_fraction        | 0.0505       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.621       |\n",
      "|    explained_variance   | 0.24         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.31e+04     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    std                  | 0.447        |\n",
      "|    value_loss           | 1.22e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-5514.28 +/- 7146.12\n",
      "Episode length: 369.38 +/- 24.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 369          |\n",
      "|    mean_reward          | -5.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030756707 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.619       |\n",
      "|    explained_variance   | 0.0508       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.87e+04     |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.451        |\n",
      "|    value_loss           | 2.77e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 375       |\n",
      "|    ep_rew_mean     | -2.34e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 471       |\n",
      "|    iterations      | 81        |\n",
      "|    time_elapsed    | 351       |\n",
      "|    total_timesteps | 165888    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 438          |\n",
      "|    ep_rew_mean          | -2.73e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019392079 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | 0.0475       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+04     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.000763    |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 7.7e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-2453.65 +/- 1944.10\n",
      "Episode length: 320.75 +/- 45.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | -2.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036279024 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.453        |\n",
      "|    value_loss           | 9.56e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 271       |\n",
      "|    ep_rew_mean     | -1.61e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 83        |\n",
      "|    time_elapsed    | 357       |\n",
      "|    total_timesteps | 169984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-3602.20 +/- 2545.44\n",
      "Episode length: 339.25 +/- 25.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -3.6e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015975062 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | -0.134       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.21e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 1.43e-05     |\n",
      "|    std                  | 0.449        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 452      |\n",
      "|    ep_rew_mean     | -3e+03   |\n",
      "| time/              |          |\n",
      "|    fps             | 476      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 379          |\n",
      "|    ep_rew_mean          | -8.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 363          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017928347 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | -0.0665      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.32e+03     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000568    |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-4650.15 +/- 7330.35\n",
      "Episode length: 347.25 +/- 42.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | -4.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020712775 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.104        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.3e+03      |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 1.15e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 377       |\n",
      "|    ep_rew_mean     | -7.85e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 479       |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 367       |\n",
      "|    total_timesteps | 176128    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 397          |\n",
      "|    ep_rew_mean          | -2.38e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 369          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039405394 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.79e+05     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.453        |\n",
      "|    value_loss           | 8.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7757.26 +/- 11281.43\n",
      "Episode length: 358.38 +/- 56.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 358         |\n",
      "|    mean_reward          | -7.76e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004128854 |\n",
      "|    clip_fraction        | 0.013       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.27e+04    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 8.04e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 357       |\n",
      "|    ep_rew_mean     | -2.18e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 482       |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 373       |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | -2.44e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 375          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067914985 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | 0.361        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.51e+04     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.447        |\n",
      "|    value_loss           | 1.39e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-2931.95 +/- 1938.21\n",
      "Episode length: 335.38 +/- 56.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -2.93e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011658419 |\n",
      "|    clip_fraction        | 0.00361      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.614       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.98e+04     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000791    |\n",
      "|    std                  | 0.448        |\n",
      "|    value_loss           | 2.34e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 329       |\n",
      "|    ep_rew_mean     | -1.77e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 485       |\n",
      "|    iterations      | 90        |\n",
      "|    time_elapsed    | 379       |\n",
      "|    total_timesteps | 184320    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 296          |\n",
      "|    ep_rew_mean          | -1.78e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 381          |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025584698 |\n",
      "|    clip_fraction        | 0.00249      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | 0.175        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.74e+04     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.444        |\n",
      "|    value_loss           | 5.54e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-4456.28 +/- 7293.13\n",
      "Episode length: 318.00 +/- 42.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -4.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020203653 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.11e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00073     |\n",
      "|    std                  | 0.442        |\n",
      "|    value_loss           | 1.61e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 426       |\n",
      "|    ep_rew_mean     | -2.58e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 489       |\n",
      "|    iterations      | 92        |\n",
      "|    time_elapsed    | 385       |\n",
      "|    total_timesteps | 188416    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 376          |\n",
      "|    ep_rew_mean          | -2.29e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 387          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044838246 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.6         |\n",
      "|    explained_variance   | -0.176       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+03     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.439        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-7507.91 +/- 14105.16\n",
      "Episode length: 343.75 +/- 49.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -7.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030315695 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.06e+05     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.438        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 536       |\n",
      "|    ep_rew_mean     | -2.48e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 492       |\n",
      "|    iterations      | 94        |\n",
      "|    time_elapsed    | 391       |\n",
      "|    total_timesteps | 192512    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 375          |\n",
      "|    ep_rew_mean          | -2.34e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 393          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018106019 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+06     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    std                  | 0.439        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-4204.83 +/- 6520.28\n",
      "Episode length: 317.62 +/- 35.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 318         |\n",
      "|    mean_reward          | -4.2e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004321959 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | -0.0274     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63e+04    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 3.49e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 857       |\n",
      "|    ep_rew_mean     | -9.72e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 495       |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 397       |\n",
      "|    total_timesteps | 196608    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 346          |\n",
      "|    ep_rew_mean          | -2.15e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 399          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038970914 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.0855       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.48e+04     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.000572    |\n",
      "|    std                  | 0.428        |\n",
      "|    value_loss           | 6.48e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-3776.74 +/- 4861.09\n",
      "Episode length: 340.88 +/- 32.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | -3.78e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069252644 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+03     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    std                  | 0.422        |\n",
      "|    value_loss           | 1.68e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 613       |\n",
      "|    ep_rew_mean     | -3.83e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 498       |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 402       |\n",
      "|    total_timesteps | 200704    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f2c70296790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "model.set_parameters(\"last_model\")\n",
    "model.learn(\n",
    "    total_timesteps=200000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de305199a42f8d",
   "metadata": {},
   "source": [
    "### 绘图检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c11d5f894713807",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 total reward: -2062.3142465040733\n",
      "Episode 2 total reward: -10985.751324923982\n",
      "Episode 3 total reward: -2078.7413577303773\n",
      "Episode 4 total reward: -19475.349703063184\n",
      "Episode 5 total reward: -1900.8387239426065\n",
      "Episode 6 total reward: -6478.355662527488\n",
      "Episode 7 total reward: -12652.29847841948\n",
      "Episode 8 total reward: -1639.8355671203385\n",
      "Episode 9 total reward: -1548.4761987104637\n",
      "Episode 10 total reward: -6382.508278333867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个包含10个子图的窗口\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axs = axs.ravel()  # 将二维数组展平，方便通过索引访问每个子图\n",
    "\n",
    "env = env_test1.DroneEnv()\n",
    "for i in range(10):\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    trajectory_x = [env.xy_p[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_y = [env.xy_p[1]]  # 存储无人机路径的y坐标\n",
    "    trajectory_ex = [env.xy_e[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_ey = [env.xy_e[1]]  # 存储无人机路径的y坐标\n",
    "    \n",
    "    # 在第i个子图中绘制环境和障碍物\n",
    "    axs[i].scatter(env.xy_e[0], env.xy_e[1], marker='x', color='green', label='Goal')\n",
    "    for k in env.obstacles:\n",
    "        obstacle_circle = plt.Circle(k, env.r_obstacles, color='lightblue', fill=True)\n",
    "        axs[i].add_patch(obstacle_circle)\n",
    "    axs[i].set_xlim(env.space1.low[0], env.space1.high[0])\n",
    "    axs[i].set_ylim(env.space1.low[1], env.space1.high[1])\n",
    "    axs[i].set_xlabel('X')\n",
    "    axs[i].set_ylabel('Y')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Episode {i+1}')\n",
    "\n",
    "    # 通过预训练模型控制无人机执行任务并绘制路径\n",
    "    model = PPO.load(\"best_model\") \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        count += 1\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        next_state, reward, done, t, info = env.step(action)\n",
    "        #if reward < -10:\n",
    "            #print(state, action, reward)\n",
    "        if count > 500:\n",
    "            done = True\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory_x.append(env.xy_p[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_y.append(env.xy_p[1])  # 更新无人机路径的y坐标\n",
    "        trajectory_ex.append(env.xy_e[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_ey.append(env.xy_e[1])  # 更新无人机路径的y坐标\n",
    "\n",
    "    # 绘制无人机路径\n",
    "    axs[i].plot(trajectory_x, trajectory_y, color='red', linewidth=0.5)\n",
    "    axs[i].plot(trajectory_ex, trajectory_ey, color='red', linewidth=0.5)\n",
    "\n",
    "    # 打印每个episode的总奖励\n",
    "    print(f'Episode {i+1} total reward:', total_reward)\n",
    "\n",
    "# 显示子图窗口\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b679efa-70da-4b5f-acb5-227aff558c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
