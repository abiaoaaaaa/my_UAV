{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-14T06:59:15.823458300Z",
     "start_time": "2024-05-14T06:59:11.695732600Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env_test1\n",
    "env = env_test1.DroneEnv()\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c32894c611bf66",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cd539b-67c5-4dc7-988b-6963b7385b11",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-14T06:59:22.708781200Z",
     "start_time": "2024-05-14T06:59:22.690674900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation callback\n",
    "callbacks = []\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=8,\n",
    "    best_model_save_path=\".\",\n",
    "    log_path=\".\",\n",
    "    eval_freq=4000,\n",
    ")\n",
    "\n",
    "callbacks.append(eval_callback)\n",
    "kwargs = {}\n",
    "kwargs[\"callback\"] = callbacks\n",
    "\n",
    "log_name = \"ppo_run_\" + str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6364eb784437bc7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T06:23:33.541022200Z"
    },
    "collapsed": false,
    "is_executing": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard/ppo_run_1713687832.6596043_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 817  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=-10835.17 +/- 9321.84\n",
      "Episode length: 326.25 +/- 23.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -1.08e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029642964 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.26e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.96e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.47e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 514  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019682671 |\n",
      "|    clip_fraction        | 0.00464      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.000147    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.78e+04     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 2.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-9893.71 +/- 11594.84\n",
      "Episode length: 315.62 +/- 31.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | -9.89e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051553133 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -2.43e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.92e+04     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 9.18e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 487  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 503           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035032988 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.21e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000522     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.45e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-9405.71 +/- 9162.89\n",
      "Episode length: 311.12 +/- 57.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -9.41e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022548572 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 2.52e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.18e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 3.23e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 481   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036205414 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 3.31e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.32e+04     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00378     |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 1.17e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-15252.01 +/- 11600.48\n",
      "Episode length: 322.88 +/- 39.08\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -1.53e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 16000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012638094 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 7.93e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.2e+06       |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000355     |\n",
      "|    std                  | 0.96          |\n",
      "|    value_loss           | 3.14e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.708814e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 9.18e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.56e+05     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000488    |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 1.56e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-7832.35 +/- 6737.75\n",
      "Episode length: 308.00 +/- 19.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | -7.83e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011396175 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.000102   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 2.41e+04    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 488           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 46            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018408825 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 4.17e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.55e+06      |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.00082      |\n",
      "|    std                  | 0.926         |\n",
      "|    value_loss           | 2.54e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-5568.49 +/- 4689.38\n",
      "Episode length: 295.00 +/- 41.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | -5.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006882199 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 3.64e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.46e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.948        |\n",
      "|    value_loss           | 1.42e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 54         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00077356 |\n",
      "|    clip_fraction        | 0.000391   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 2.15e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.58e+05   |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00141   |\n",
      "|    std                  | 0.939      |\n",
      "|    value_loss           | 1.5e+06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-4482.65 +/- 6473.99\n",
      "Episode length: 304.00 +/- 54.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 304         |\n",
      "|    mean_reward          | -4.48e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002663623 |\n",
      "|    clip_fraction        | 0.00381     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 1.73e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+05    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.513277e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.91e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+06     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    std                  | 0.928        |\n",
      "|    value_loss           | 2.33e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-4481.26 +/- 5169.56\n",
      "Episode length: 295.25 +/- 50.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | -4.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.958303e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.31e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.18e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00017     |\n",
      "|    std                  | 0.931        |\n",
      "|    value_loss           | 2.02e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012509142 |\n",
      "|    clip_fraction        | 0.00112      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.27e+06     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.911        |\n",
      "|    value_loss           | 2.11e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-2940.11 +/- 3271.43\n",
      "Episode length: 272.62 +/- 34.99\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 273           |\n",
      "|    mean_reward          | -2.94e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 36000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0794373e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.33         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.73e+05      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000192     |\n",
      "|    std                  | 0.911         |\n",
      "|    value_loss           | 3.28e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 480   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-7452.87 +/- 6283.88\n",
      "Episode length: 277.62 +/- 49.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 278         |\n",
      "|    mean_reward          | -7.45e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011675134 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 1.67e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.876       |\n",
      "|    value_loss           | 1.2e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 480           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 106           |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070773734 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.67e+05      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.00204      |\n",
      "|    std                  | 0.871         |\n",
      "|    value_loss           | 2.08e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-4801.48 +/- 4996.01\n",
      "Episode length: 304.38 +/- 47.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 304          |\n",
      "|    mean_reward          | -4.8e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032076808 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11e+04     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.871        |\n",
      "|    value_loss           | 9.98e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 111   |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023766654 |\n",
      "|    clip_fraction        | 0.00273      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08e+05     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000599    |\n",
      "|    std                  | 0.861        |\n",
      "|    value_loss           | 6.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-4748.83 +/- 6275.80\n",
      "Episode length: 296.38 +/- 24.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 296          |\n",
      "|    mean_reward          | -4.75e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009095837 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+06     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000842    |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 4.05e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 120   |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025086985 |\n",
      "|    clip_fraction        | 0.00488      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72e+06     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 2.2e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-6194.54 +/- 6429.77\n",
      "Episode length: 326.75 +/- 38.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 327         |\n",
      "|    mean_reward          | -6.19e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004128187 |\n",
      "|    clip_fraction        | 0.00801     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.81e+05    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000745   |\n",
      "|    std                  | 0.857       |\n",
      "|    value_loss           | 1.45e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 128   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032602432 |\n",
      "|    clip_fraction        | 0.00996      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+06     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    std                  | 0.858        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-5005.86 +/- 6512.74\n",
      "Episode length: 307.50 +/- 41.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | -5.01e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004895909 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.04e+05     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00043     |\n",
      "|    std                  | 0.858        |\n",
      "|    value_loss           | 1.04e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 137   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00592896 |\n",
      "|    clip_fraction        | 0.0385     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.39e+05   |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00343   |\n",
      "|    std                  | 0.846      |\n",
      "|    value_loss           | 5.14e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-2744.57 +/- 3510.51\n",
      "Episode length: 300.25 +/- 50.93\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 300           |\n",
      "|    mean_reward          | -2.74e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 68000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033695478 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.25         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.9e+05       |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00105      |\n",
      "|    std                  | 0.849         |\n",
      "|    value_loss           | 2.37e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 145   |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002645306 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71e+05    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000438   |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 4.45e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-14222.48 +/- 12429.50\n",
      "Episode length: 338.50 +/- 36.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 338          |\n",
      "|    mean_reward          | -1.42e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042363526 |\n",
      "|    clip_fraction        | 0.00928      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+05     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 8.8e+05      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 154   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002177515 |\n",
      "|    clip_fraction        | 0.00288     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.33e+05    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 7.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-2717.48 +/- 2300.58\n",
      "Episode length: 305.50 +/- 77.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | -2.72e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000860849 |\n",
      "|    clip_fraction        | 0.000195    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.19e+06    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.000855   |\n",
      "|    std                  | 0.84        |\n",
      "|    value_loss           | 2.46e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054182815 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.3e+04      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    std                  | 0.843        |\n",
      "|    value_loss           | 1.67e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-3534.54 +/- 2550.11\n",
      "Episode length: 311.50 +/- 42.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -3.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021265207 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.52e+05     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 172   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006487474 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.5e+05     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0029     |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 8.47e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-9585.38 +/- 12697.34\n",
      "Episode length: 310.62 +/- 44.92\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 311           |\n",
      "|    mean_reward          | -9.59e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 84000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060662813 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.25         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.11e+05      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000111     |\n",
      "|    std                  | 0.847         |\n",
      "|    value_loss           | 2.85e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 475   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 180   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-7147.57 +/- 7431.32\n",
      "Episode length: 318.50 +/- 50.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -7.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061098724 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18e+05     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    std                  | 0.834        |\n",
      "|    value_loss           | 1.29e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 185   |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010241703 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+06     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.836        |\n",
      "|    value_loss           | 1.44e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-9868.51 +/- 8733.03\n",
      "Episode length: 325.62 +/- 46.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -9.87e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040867487 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.47e+05     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.836        |\n",
      "|    value_loss           | 8.44e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 194   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067133605 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.24e+06     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    std                  | 0.835        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-9468.72 +/- 8086.48\n",
      "Episode length: 324.50 +/- 48.12\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 324           |\n",
      "|    mean_reward          | -9.47e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 96000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049809995 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.24         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.35e+06      |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    std                  | 0.832         |\n",
      "|    value_loss           | 2.63e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 203   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 206          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029520364 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.89e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    std                  | 0.828        |\n",
      "|    value_loss           | 2.59e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-5456.12 +/- 5755.34\n",
      "Episode length: 342.75 +/- 35.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 343         |\n",
      "|    mean_reward          | -5.46e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008011256 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22e+06    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 1.51e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 474    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 211    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002263059 |\n",
      "|    clip_fraction        | 0.00957     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35e+05    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.000524   |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 2.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-15059.54 +/- 8463.57\n",
      "Episode length: 294.62 +/- 46.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | -1.51e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001040586 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.3e+05     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000277   |\n",
      "|    std                  | 0.805       |\n",
      "|    value_loss           | 1.21e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 480    |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 217    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 219          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015816229 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64e+06     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    std                  | 0.803        |\n",
      "|    value_loss           | 2.57e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-9461.25 +/- 6898.32\n",
      "Episode length: 316.88 +/- 37.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | -9.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010044618 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+06     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    std                  | 0.8          |\n",
      "|    value_loss           | 1.25e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 486    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 223    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 225          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064495634 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.792        |\n",
      "|    value_loss           | 7.44e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-3042.31 +/- 4124.76\n",
      "Episode length: 307.00 +/- 67.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -3.04e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037936212 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    std                  | 0.794        |\n",
      "|    value_loss           | 2.15e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 492    |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 228    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 496         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008169918 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51e+03    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 6.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-9992.32 +/- 9333.05\n",
      "Episode length: 328.38 +/- 35.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -9.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022639134 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.43e+03     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 9.25e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 498    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 234    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 236          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039139865 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.16e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 0.758        |\n",
      "|    value_loss           | 1.13e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-8277.49 +/- 6988.22\n",
      "Episode length: 321.75 +/- 45.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -8.28e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024414326 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+05     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 0.757        |\n",
      "|    value_loss           | 5.05e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 503    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 240    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 242          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008507621 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.86e+05     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000867    |\n",
      "|    std                  | 0.758        |\n",
      "|    value_loss           | 1.63e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-6649.90 +/- 7834.69\n",
      "Episode length: 327.12 +/- 34.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -6.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023046248 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.96e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00091     |\n",
      "|    std                  | 0.762        |\n",
      "|    value_loss           | 6.44e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 507    |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 245    |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044672783 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+03     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    std                  | 0.734        |\n",
      "|    value_loss           | 6.01e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-4656.59 +/- 5426.40\n",
      "Episode length: 336.38 +/- 32.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 336         |\n",
      "|    mean_reward          | -4.66e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004483409 |\n",
      "|    clip_fraction        | 0.0123      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.06e+04    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    std                  | 0.734       |\n",
      "|    value_loss           | 8.76e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 512    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 251    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 515           |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 254           |\n",
      "|    total_timesteps      | 131072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015887554 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.24e+05      |\n",
      "|    n_updates            | 630           |\n",
      "|    policy_gradient_loss | -0.00027      |\n",
      "|    std                  | 0.733         |\n",
      "|    value_loss           | 1.81e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-5893.15 +/- 7859.29\n",
      "Episode length: 324.00 +/- 38.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | -5.89e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 132000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002590755 |\n",
      "|    clip_fraction        | 0.00264     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.14e+04    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    std                  | 0.729       |\n",
      "|    value_loss           | 5.57e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 517    |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 257    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017305536 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.19e+03    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00499     |\n",
      "|    std                  | 0.737       |\n",
      "|    value_loss           | 5.4e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-9234.84 +/- 9807.40\n",
      "Episode length: 335.88 +/- 60.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | -9.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007086221 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.04e+04     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000761    |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 4.98e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 520    |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 263    |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 523           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 265           |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.2756055e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.71e+05      |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.000671     |\n",
      "|    std                  | 0.738         |\n",
      "|    value_loss           | 1.84e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-11819.57 +/- 10237.29\n",
      "Episode length: 341.00 +/- 38.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | -1.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045081833 |\n",
      "|    clip_fraction        | 0.0601       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+05     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | 8.88e-05     |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 2.12e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 524    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 269    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 527            |\n",
      "|    iterations           | 70             |\n",
      "|    time_elapsed         | 271            |\n",
      "|    total_timesteps      | 143360         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.40390475e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.12          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.42e+06       |\n",
      "|    n_updates            | 690            |\n",
      "|    policy_gradient_loss | 8.74e-05       |\n",
      "|    std                  | 0.738          |\n",
      "|    value_loss           | 1.94e+06       |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-5682.43 +/- 10532.51\n",
      "Episode length: 286.00 +/- 40.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | -5.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040413076 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.89e+05     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | 0.00124      |\n",
      "|    std                  | 0.737        |\n",
      "|    value_loss           | 7.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 529    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 274    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 532          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 277          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007798376 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.49e+04     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 7.29e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-4296.06 +/- 6498.71\n",
      "Episode length: 307.75 +/- 43.87\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -4.3e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 148000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031309653 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.34e+05      |\n",
      "|    n_updates            | 720           |\n",
      "|    policy_gradient_loss | -0.000658     |\n",
      "|    std                  | 0.737         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 533    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 280    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 536          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 282          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067641987 |\n",
      "|    clip_fraction        | 0.218        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+04     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | 0.00606      |\n",
      "|    std                  | 0.736        |\n",
      "|    value_loss           | 8.69e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-5744.04 +/- 8033.90\n",
      "Episode length: 334.50 +/- 46.92\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 334           |\n",
      "|    mean_reward          | -5.74e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 152000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5810801e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.34e+06      |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | 0.000202      |\n",
      "|    std                  | 0.736         |\n",
      "|    value_loss           | 1.72e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 536    |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 286    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033349432 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.54e+03     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    std                  | 0.714        |\n",
      "|    value_loss           | 6.96e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-9161.91 +/- 8548.73\n",
      "Episode length: 338.62 +/- 28.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -9.16e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011769217 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.8e+05      |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 539    |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 292    |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.978355e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.21e+04     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.6e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-3868.71 +/- 6167.57\n",
      "Episode length: 324.12 +/- 40.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -3.87e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025212332 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04e+03     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 7.23e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 542    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 298    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010721167 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.88e+03    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.000173    |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 2.82e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-8701.32 +/- 9818.13\n",
      "Episode length: 333.38 +/- 31.23\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 333           |\n",
      "|    mean_reward          | -8.7e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 164000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0243503e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.55e+05      |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | -0.000347     |\n",
      "|    std                  | 0.718         |\n",
      "|    value_loss           | 1.46e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 543    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 304    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 546           |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 307           |\n",
      "|    total_timesteps      | 167936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8454011e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.14e+06      |\n",
      "|    n_updates            | 810           |\n",
      "|    policy_gradient_loss | -0.000144     |\n",
      "|    std                  | 0.718         |\n",
      "|    value_loss           | 1.75e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-8984.06 +/- 10000.79\n",
      "Episode length: 320.25 +/- 34.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -8.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001892483 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.27e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    std                  | 0.717        |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 547    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 310    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-8669.74 +/- 9438.92\n",
      "Episode length: 319.25 +/- 57.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 319           |\n",
      "|    mean_reward          | -8.67e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 172000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024763486 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.9e+05       |\n",
      "|    n_updates            | 830           |\n",
      "|    policy_gradient_loss | -0.000778     |\n",
      "|    std                  | 0.717         |\n",
      "|    value_loss           | 1.99e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 547    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 313    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 550           |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 316           |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018563628 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.28e+05      |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.00094      |\n",
      "|    std                  | 0.717         |\n",
      "|    value_loss           | 1.38e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-8531.28 +/- 9970.63\n",
      "Episode length: 316.62 +/- 27.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 317         |\n",
      "|    mean_reward          | -8.53e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987494 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03e+06    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.000483   |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 1.21e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 551    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 319    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 553         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010569612 |\n",
      "|    clip_fraction        | 0.0319      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.21e+03    |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00091    |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 3.47e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-4376.64 +/- 5583.36\n",
      "Episode length: 285.88 +/- 53.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | -4.38e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 180000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041278256 |\n",
      "|    clip_fraction        | 0.00908      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+05     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 2.44e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 554    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 324    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 557        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 327        |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01972401 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.9e+04    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | 0.00935    |\n",
      "|    std                  | 0.726      |\n",
      "|    value_loss           | 7.1e+04    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-3682.48 +/- 5484.61\n",
      "Episode length: 321.50 +/- 47.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -3.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056421096 |\n",
      "|    clip_fraction        | 0.0725       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.75e+03     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000188    |\n",
      "|    std                  | 0.731        |\n",
      "|    value_loss           | 1.03e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 557    |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 330    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 560           |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 332           |\n",
      "|    total_timesteps      | 186368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029987746 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3e+05         |\n",
      "|    n_updates            | 900           |\n",
      "|    policy_gradient_loss | 0.000625      |\n",
      "|    std                  | 0.73          |\n",
      "|    value_loss           | 1.36e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-8723.22 +/- 7880.51\n",
      "Episode length: 319.25 +/- 40.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -8.72e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.011624e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.45e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000754    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 560    |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 336    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 338          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013632195 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+04     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 5.79e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-1490.79 +/- 208.93\n",
      "Episode length: 301.12 +/- 42.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 301          |\n",
      "|    mean_reward          | -1.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.223819e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.02e+04     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.000243    |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 4.45e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 562    |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 342    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 564         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028845433 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6e+03     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 9.42e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-7247.72 +/- 10028.62\n",
      "Episode length: 332.12 +/- 39.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -7.25e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 196000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012039589 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.4e+04      |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | 0.000436     |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 7.67e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 564    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 347    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 566          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 350          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021741404 |\n",
      "|    clip_fraction        | 0.0477       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.25e+03     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | 0.000312     |\n",
      "|    std                  | 0.74         |\n",
      "|    value_loss           | 7.34e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-6053.06 +/- 8855.92\n",
      "Episode length: 326.88 +/- 32.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -6.05e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053633414 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+06      |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.74         |\n",
      "|    value_loss           | 2.04e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 565    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 354    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 567           |\n",
      "|    iterations           | 99            |\n",
      "|    time_elapsed         | 357           |\n",
      "|    total_timesteps      | 202752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032423344 |\n",
      "|    clip_fraction        | 0.00166       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.6e+05       |\n",
      "|    n_updates            | 980           |\n",
      "|    policy_gradient_loss | -0.000849     |\n",
      "|    std                  | 0.74          |\n",
      "|    value_loss           | 1.21e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-7045.99 +/- 7952.93\n",
      "Episode length: 313.75 +/- 45.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 314          |\n",
      "|    mean_reward          | -7.05e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 204000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.188923e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9e+05        |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    std                  | 0.74         |\n",
      "|    value_loss           | 1.85e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 567    |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 360    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 569           |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 363           |\n",
      "|    total_timesteps      | 206848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031907827 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.01e+05      |\n",
      "|    n_updates            | 1000          |\n",
      "|    policy_gradient_loss | -0.000575     |\n",
      "|    std                  | 0.741         |\n",
      "|    value_loss           | 9.16e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-7639.83 +/- 7822.07\n",
      "Episode length: 335.25 +/- 44.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -7.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028287328 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+03     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 9.53e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 569    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 366    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 571         |\n",
      "|    iterations           | 103         |\n",
      "|    time_elapsed         | 368         |\n",
      "|    total_timesteps      | 210944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009347541 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.27e+04    |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | 0.000592    |\n",
      "|    std                  | 0.733       |\n",
      "|    value_loss           | 3e+04       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-7348.61 +/- 6134.04\n",
      "Episode length: 305.38 +/- 57.49\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 305           |\n",
      "|    mean_reward          | -7.35e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 212000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3731783e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.72e+05      |\n",
      "|    n_updates            | 1030          |\n",
      "|    policy_gradient_loss | -0.000538     |\n",
      "|    std                  | 0.732         |\n",
      "|    value_loss           | 9.88e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 572    |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 372    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 573           |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 374           |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019570763 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.28e+04      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | -0.000872     |\n",
      "|    std                  | 0.732         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-11631.43 +/- 8471.23\n",
      "Episode length: 345.50 +/- 52.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | -1.16e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 216000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034074232 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+04     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    std                  | 0.732        |\n",
      "|    value_loss           | 3.11e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 573    |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 378    |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 575          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 380          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012924462 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | 5.35e-05     |\n",
      "|    std                  | 0.731        |\n",
      "|    value_loss           | 7.56e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-7737.13 +/- 7741.62\n",
      "Episode length: 309.38 +/- 59.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 309           |\n",
      "|    mean_reward          | -7.74e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 220000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0045077e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.09e+05      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -0.0003       |\n",
      "|    std                  | 0.731         |\n",
      "|    value_loss           | 2.19e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 575    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 384    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 576          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 387          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010730809 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.15e+05     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 1.21e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-5611.37 +/- 6961.36\n",
      "Episode length: 307.50 +/- 55.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | -5.61e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041896477 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.33e+05     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | 0.00241      |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 577    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 390    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 578          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 392          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014584109 |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.97e+03     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | 0.00235      |\n",
      "|    std                  | 0.721        |\n",
      "|    value_loss           | 8.66e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-2402.21 +/- 2504.24\n",
      "Episode length: 303.25 +/- 44.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 303          |\n",
      "|    mean_reward          | -2.4e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 228000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018544869 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+04      |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | 0.000762     |\n",
      "|    std                  | 0.723        |\n",
      "|    value_loss           | 1.62e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 579    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 396    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 580         |\n",
      "|    iterations           | 113         |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 231424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004175472 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23e+03    |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 2.55e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-4494.58 +/- 5702.70\n",
      "Episode length: 306.88 +/- 28.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -4.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 232000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030549588 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.34e+05     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.000929    |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 3.75e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 581    |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 401    |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 582           |\n",
      "|    iterations           | 115           |\n",
      "|    time_elapsed         | 404           |\n",
      "|    total_timesteps      | 235520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010585453 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.27e+05      |\n",
      "|    n_updates            | 1140          |\n",
      "|    policy_gradient_loss | -0.000414     |\n",
      "|    std                  | 0.723         |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-10318.38 +/- 8954.48\n",
      "Episode length: 353.62 +/- 43.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 354           |\n",
      "|    mean_reward          | -1.03e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 236000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5700585e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.59e+06      |\n",
      "|    n_updates            | 1150          |\n",
      "|    policy_gradient_loss | -0.000138     |\n",
      "|    std                  | 0.723         |\n",
      "|    value_loss           | 1.49e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 582    |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 407    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 584          |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 239616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003269506 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.41e+05     |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.000416    |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 4.82e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-3949.49 +/- 5062.25\n",
      "Episode length: 298.88 +/- 40.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 299         |\n",
      "|    mean_reward          | -3.95e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011278207 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.54e+03    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00267    |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 6.03e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 584    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 413    |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 586         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 243712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009267829 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.57e+03    |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.000539   |\n",
      "|    std                  | 0.709       |\n",
      "|    value_loss           | 1.15e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-5339.62 +/- 5340.83\n",
      "Episode length: 334.88 +/- 32.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -5.34e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 244000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014669921 |\n",
      "|    clip_fraction        | 0.00522      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+05     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.000462    |\n",
      "|    std                  | 0.711        |\n",
      "|    value_loss           | 2.13e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 585    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 419    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 587           |\n",
      "|    iterations           | 121           |\n",
      "|    time_elapsed         | 422           |\n",
      "|    total_timesteps      | 247808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7103273e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.74e+05      |\n",
      "|    n_updates            | 1200          |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    std                  | 0.711         |\n",
      "|    value_loss           | 1.95e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-7627.80 +/- 8390.08\n",
      "Episode length: 328.00 +/- 49.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -7.63e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 248000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061224564 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85e+05     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | 0.000778     |\n",
      "|    std                  | 0.713        |\n",
      "|    value_loss           | 4.92e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 586    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 425    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 588          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 428          |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042239474 |\n",
      "|    clip_fraction        | 0.00859      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.85e+05     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -2.87e-05    |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.18e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-7363.69 +/- 4924.14\n",
      "Episode length: 319.12 +/- 43.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | -7.36e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002486806 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 3.58e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.15e+05    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.000939   |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 1.47e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 588    |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 431    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-5279.32 +/- 5740.45\n",
      "Episode length: 311.12 +/- 50.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 311         |\n",
      "|    mean_reward          | -5.28e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026671909 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.1e+05     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | 0.00772     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 3.89e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 588    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 435    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 589           |\n",
      "|    iterations           | 126           |\n",
      "|    time_elapsed         | 437           |\n",
      "|    total_timesteps      | 258048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4445886e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.38e+06      |\n",
      "|    n_updates            | 1250          |\n",
      "|    policy_gradient_loss | -0.00021      |\n",
      "|    std                  | 0.71          |\n",
      "|    value_loss           | 2.23e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-5521.16 +/- 7523.81\n",
      "Episode length: 283.12 +/- 53.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | -5.52e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006894015 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.37e+05    |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.000726   |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 7.47e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 589    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 441    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 590           |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 443           |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011284696 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.68e+05      |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | -0.00032      |\n",
      "|    std                  | 0.71          |\n",
      "|    value_loss           | 2.48e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-9712.22 +/- 9223.38\n",
      "Episode length: 305.12 +/- 56.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | -9.71e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 264000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003800598 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.46e+05     |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.000502    |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 1.8e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 590    |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 447    |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 449          |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.433226e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07e+06     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.000814    |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 3.56e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-11916.65 +/- 8505.58\n",
      "Episode length: 331.62 +/- 48.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -1.19e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0129073365 |\n",
      "|    clip_fraction        | 0.0996       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.7e+04      |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | 0.000122     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 3.64e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 592    |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 453    |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001649401 |\n",
      "|    clip_fraction        | 0.00894     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.98e+05    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 3.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-6806.08 +/- 7038.52\n",
      "Episode length: 290.50 +/- 49.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -6.81e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024880455 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.47e+04     |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.708        |\n",
      "|    value_loss           | 1.86e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 593    |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 458    |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033711948 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38e+05    |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.00272     |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-16371.41 +/- 11311.09\n",
      "Episode length: 320.00 +/- 41.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -1.64e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 276000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.826732e-05 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56e+05     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.000467    |\n",
      "|    std                  | 0.701        |\n",
      "|    value_loss           | 2.61e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 594    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 464    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 467         |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005418267 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.86e+05    |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    std                  | 0.704       |\n",
      "|    value_loss           | 5.54e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-6641.75 +/- 7365.15\n",
      "Episode length: 339.50 +/- 36.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -6.64e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 280000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003861372 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.7e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+06     |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.000941    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.9e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 596    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 470    |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 597          |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 473          |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014760443 |\n",
      "|    clip_fraction        | 0.00869      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.93e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.93e+05     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -8.45e-05    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-6398.97 +/- 6046.25\n",
      "Episode length: 325.62 +/- 50.41\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -6.4e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 284000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019924581 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 1.09e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.99e+05      |\n",
      "|    n_updates            | 1380          |\n",
      "|    policy_gradient_loss | -0.00106      |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 2.48e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 597    |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 476    |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 598          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 478          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013772042 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.58e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63e+06     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-6210.39 +/- 8193.84\n",
      "Episode length: 310.50 +/- 30.77\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 310           |\n",
      "|    mean_reward          | -6.21e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 288000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021281908 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 2.09e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.89e+05      |\n",
      "|    n_updates            | 1400          |\n",
      "|    policy_gradient_loss | -0.000469     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 1.96e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 599    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 482    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 600         |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 290816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012274868 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.13e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.26e+05    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 1.01e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-11415.83 +/- 12447.16\n",
      "Episode length: 326.25 +/- 40.41\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -1.14e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 292000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021874969 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 1.85e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.27e+05      |\n",
      "|    n_updates            | 1420          |\n",
      "|    policy_gradient_loss | -0.00189      |\n",
      "|    std                  | 0.703         |\n",
      "|    value_loss           | 2.93e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 600    |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 487    |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 601           |\n",
      "|    iterations           | 144           |\n",
      "|    time_elapsed         | 490           |\n",
      "|    total_timesteps      | 294912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047990467 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 9.54e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.27e+05      |\n",
      "|    n_updates            | 1430          |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 1.8e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-3075.88 +/- 2318.77\n",
      "Episode length: 334.12 +/- 50.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 334         |\n",
      "|    mean_reward          | -3.08e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011514617 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1.13e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.59e+04    |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.000125   |\n",
      "|    std                  | 0.704       |\n",
      "|    value_loss           | 1.2e+05     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 601    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 493    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 602          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 495          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.558154e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000304     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+06     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.000107    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 3.37e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-6851.17 +/- 6387.80\n",
      "Episode length: 330.62 +/- 61.22\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 331           |\n",
      "|    mean_reward          | -6.85e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 300000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7044898e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 2.04e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.83e+06      |\n",
      "|    n_updates            | 1460          |\n",
      "|    policy_gradient_loss | -0.000232     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 4.42e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 602    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 499    |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 501          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033856449 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.14e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+05      |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.00272     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-8120.23 +/- 7129.88\n",
      "Episode length: 354.25 +/- 25.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | -8.12e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 304000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006647826 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1.61e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.34e+05    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 4.9e+05     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 604    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 504    |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 507          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019518671 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 5.9e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.83e+03     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.00084     |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-4021.97 +/- 6165.52\n",
      "Episode length: 352.62 +/- 32.15\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 353           |\n",
      "|    mean_reward          | -4.02e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 308000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8077088e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.00998       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.76e+05      |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -9.69e-05     |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 9.93e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 605    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 510    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 606           |\n",
      "|    iterations           | 152           |\n",
      "|    time_elapsed         | 513           |\n",
      "|    total_timesteps      | 311296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020950526 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.0138        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.17e+05      |\n",
      "|    n_updates            | 1510          |\n",
      "|    policy_gradient_loss | -0.000236     |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 2.74e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-5304.81 +/- 5627.97\n",
      "Episode length: 331.75 +/- 42.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | -5.3e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 312000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028092202 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000224     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.2e+04      |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 6.32e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 606    |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 516    |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 518          |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008785522 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000828     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85e+05     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -0.000437    |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 2.39e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-5517.77 +/- 7094.93\n",
      "Episode length: 328.25 +/- 47.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -5.52e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008107964 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.00696      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36e+06     |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    std                  | 0.701        |\n",
      "|    value_loss           | 2.91e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 522    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 524          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024253628 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000318     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.61e+05     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 5.31e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-7724.94 +/- 7701.88\n",
      "Episode length: 312.38 +/- 39.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -7.72e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009779538 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000384     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.7e+05      |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.000802    |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 1.36e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 609    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 527    |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 529         |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005886602 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 7.78e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.92e+03    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 1.94e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-15217.29 +/- 6219.59\n",
      "Episode length: 329.00 +/- 48.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 329          |\n",
      "|    mean_reward          | -1.52e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003916061 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0178       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.69e+05     |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.000348    |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 2.74e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 610    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 533    |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 612          |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 535          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006421994 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0172       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.85e+05     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 7.46e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-6388.07 +/- 8554.23\n",
      "Episode length: 314.12 +/- 42.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 314          |\n",
      "|    mean_reward          | -6.39e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010521959 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00235      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+06     |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.74e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 611    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 538    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 541          |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033513745 |\n",
      "|    clip_fraction        | 0.00977      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000133     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1e+06        |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.000875    |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 1.03e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-8736.19 +/- 7592.61\n",
      "Episode length: 305.00 +/- 46.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | -8.74e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028359308 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000319     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.63e+05     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.000803    |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 613    |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 544    |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 546          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020965566 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.00019      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.53e+05     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.000565    |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 2.45e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-4133.98 +/- 6189.30\n",
      "Episode length: 321.00 +/- 43.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | -4.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050209453 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 7.8e-05      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+05     |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 1.78e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 614    |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 549    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 615          |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 552          |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022744802 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.91e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+06     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 3.4e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-8845.29 +/- 9696.00\n",
      "Episode length: 315.12 +/- 39.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 315        |\n",
      "|    mean_reward          | -8.85e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 340000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00543148 |\n",
      "|    clip_fraction        | 0.0579     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 3.78e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.78e+05   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.00104   |\n",
      "|    std                  | 0.705      |\n",
      "|    value_loss           | 5.78e+05   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 615    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 555    |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-6497.28 +/- 8928.96\n",
      "Episode length: 294.38 +/- 56.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 294         |\n",
      "|    mean_reward          | -6.5e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 344000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003431833 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00443     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.07e+03    |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 3e+04       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 615    |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 558    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 616          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 561          |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005233368 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00148      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+06     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.000421    |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.1e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-4306.47 +/- 5204.72\n",
      "Episode length: 309.25 +/- 47.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 309           |\n",
      "|    mean_reward          | -4.31e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 348000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00091826357 |\n",
      "|    clip_fraction        | 0.000928      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0.00216       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.25e+06      |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | -0.000277     |\n",
      "|    std                  | 0.708         |\n",
      "|    value_loss           | 9.7e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 616    |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 564    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 566          |\n",
      "|    total_timesteps      | 350208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012192628 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00281      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.58e+04     |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.000773    |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 2.32e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-6005.31 +/- 6560.48\n",
      "Episode length: 339.62 +/- 36.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -6.01e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 352000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002287889 |\n",
      "|    clip_fraction        | 0.00244     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00541     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+05    |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 9.02e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 617    |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 570    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 572          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005299675 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00364      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.43e+05     |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000202    |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-8301.87 +/- 9116.41\n",
      "Episode length: 322.88 +/- 35.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -8.3e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 356000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018838182 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -0.0486       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.73e+04      |\n",
      "|    n_updates            | 1730          |\n",
      "|    policy_gradient_loss | -6.98e-05     |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 2.13e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 618    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 575    |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 620           |\n",
      "|    iterations           | 175           |\n",
      "|    time_elapsed         | 578           |\n",
      "|    total_timesteps      | 358400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015657378 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.0208        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.43e+05      |\n",
      "|    n_updates            | 1740          |\n",
      "|    policy_gradient_loss | -0.000488     |\n",
      "|    std                  | 0.702         |\n",
      "|    value_loss           | 1.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-8813.77 +/- 6294.95\n",
      "Episode length: 305.62 +/- 44.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | -8.81e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011872547 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.01         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+06      |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.000567    |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 3.67e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 620    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 581    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 621          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 583          |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022679353 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000295     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+05     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | 0.000704     |\n",
      "|    std                  | 0.695        |\n",
      "|    value_loss           | 9.94e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-3757.59 +/- 6159.49\n",
      "Episode length: 306.75 +/- 50.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | -3.76e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 364000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001563704 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.000161    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+04    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.000356   |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 621    |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 586    |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 589          |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005099919 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000482     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+06     |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.000607    |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 2.98e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-8330.87 +/- 6406.83\n",
      "Episode length: 308.00 +/- 62.29\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -8.33e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 368000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088054384 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0.000199      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+05      |\n",
      "|    n_updates            | 1790          |\n",
      "|    policy_gradient_loss | -0.00154      |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 2.52e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 621    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 592    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 595          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012120033 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0132       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93e+05     |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 7.2e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-6399.07 +/- 8297.67\n",
      "Episode length: 335.88 +/- 34.82\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 336           |\n",
      "|    mean_reward          | -6.4e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 372000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012780784 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0.00893       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.45e+04      |\n",
      "|    n_updates            | 1810          |\n",
      "|    policy_gradient_loss | -0.000784     |\n",
      "|    std                  | 0.694         |\n",
      "|    value_loss           | 4.32e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 599    |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 601          |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014513314 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00905      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.46e+04     |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.695        |\n",
      "|    value_loss           | 1.89e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-7972.29 +/- 9944.68\n",
      "Episode length: 326.50 +/- 48.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -7.97e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 376000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013034774 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.00048      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5e+05      |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 1.97e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 605    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 607          |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043762065 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000117     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.64e+05     |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 1.92e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-4339.41 +/- 5021.28\n",
      "Episode length: 305.75 +/- 39.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | -4.34e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057630776 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 3.52e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.33e+05     |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.00054     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 610    |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 612          |\n",
      "|    total_timesteps      | 382976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031585936 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000111     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.81e+04     |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 8.37e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-4293.22 +/- 4751.91\n",
      "Episode length: 313.25 +/- 35.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 313          |\n",
      "|    mean_reward          | -4.29e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.870799e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.013        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.15e+06     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 1.33e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 624    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 616    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 625           |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 618           |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053186546 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0.0172        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.77e+05      |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | -0.000563     |\n",
      "|    std                  | 0.695         |\n",
      "|    value_loss           | 7.71e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-17795.79 +/- 13845.03\n",
      "Episode length: 325.12 +/- 47.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | -1.78e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 388000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001158168 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0099       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+06      |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.00031     |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 1.52e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 625    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 622    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 626          |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 624          |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015011968 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00122      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.29e+05     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 1.9e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-11390.60 +/- 7840.15\n",
      "Episode length: 313.00 +/- 42.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | -1.14e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 392000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008351488 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.000192    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.87e+05    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 3.62e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 626    |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 627    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 193         |\n",
      "|    time_elapsed         | 630         |\n",
      "|    total_timesteps      | 395264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005884272 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0031      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.89e+04    |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 1.42e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-7745.81 +/- 8094.17\n",
      "Episode length: 345.38 +/- 38.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -7.75e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 396000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010604414 |\n",
      "|    clip_fraction        | 0.00161      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000414     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.91e+05     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.709        |\n",
      "|    value_loss           | 2.03e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 627    |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 633    |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 195        |\n",
      "|    time_elapsed         | 635        |\n",
      "|    total_timesteps      | 399360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00606095 |\n",
      "|    clip_fraction        | 0.0324     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 9.49e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.54e+04   |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.00178   |\n",
      "|    std                  | 0.704      |\n",
      "|    value_loss           | 8.33e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-9563.62 +/- 10329.44\n",
      "Episode length: 311.50 +/- 35.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -9.56e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011841148 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000633     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.85e+03     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00057     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 2.34e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 628    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 639    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 629          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 641          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017267686 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0287       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36e+04     |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 1.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-11064.93 +/- 13048.01\n",
      "Episode length: 319.75 +/- 46.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -1.11e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 404000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013109748 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0239       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.67e+05     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 8.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 198    |\n",
      "|    time_elapsed    | 644    |\n",
      "|    total_timesteps | 405504 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 629           |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 646           |\n",
      "|    total_timesteps      | 407552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018110915 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0.00738       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.94e+05      |\n",
      "|    n_updates            | 1980          |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-9621.73 +/- 10563.25\n",
      "Episode length: 323.88 +/- 48.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -9.62e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 408000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015522634 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0619       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+04     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.000752    |\n",
      "|    std                  | 0.709        |\n",
      "|    value_loss           | 2.62e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 200    |\n",
      "|    time_elapsed    | 650    |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 630           |\n",
      "|    iterations           | 201           |\n",
      "|    time_elapsed         | 652           |\n",
      "|    total_timesteps      | 411648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084643066 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 0.0195        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.47e+06      |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -0.000257     |\n",
      "|    std                  | 0.71          |\n",
      "|    value_loss           | 3.33e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-16816.65 +/- 13133.95\n",
      "Episode length: 340.00 +/- 56.57\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 340           |\n",
      "|    mean_reward          | -1.68e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 412000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7454483e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -0.00223      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.28e+04      |\n",
      "|    n_updates            | 2010          |\n",
      "|    policy_gradient_loss | -0.000112     |\n",
      "|    std                  | 0.706         |\n",
      "|    value_loss           | 2.87e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 630    |\n",
      "|    iterations      | 202    |\n",
      "|    time_elapsed    | 656    |\n",
      "|    total_timesteps | 413696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 203          |\n",
      "|    time_elapsed         | 658          |\n",
      "|    total_timesteps      | 415744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001175864 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0145       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+05     |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 1.7e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-4489.45 +/- 5353.48\n",
      "Episode length: 327.88 +/- 42.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | -4.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013650216 |\n",
      "|    clip_fraction        | 0.00352      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00347      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.97e+04     |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 0.71         |\n",
      "|    value_loss           | 5.86e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 204    |\n",
      "|    time_elapsed    | 662    |\n",
      "|    total_timesteps | 417792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 664         |\n",
      "|    total_timesteps      | 419840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005355091 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.00281     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.48e+05    |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 1.43e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-10331.68 +/- 9131.04\n",
      "Episode length: 324.88 +/- 42.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061893715 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000179     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+06     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 2.82e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 206    |\n",
      "|    time_elapsed    | 667    |\n",
      "|    total_timesteps | 421888 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 632          |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 669          |\n",
      "|    total_timesteps      | 423936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055659055 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 6.35e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+05     |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -3.6e-05     |\n",
      "|    std                  | 0.689        |\n",
      "|    value_loss           | 6.75e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-12559.14 +/- 9531.22\n",
      "Episode length: 292.62 +/- 12.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | -1.26e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 424000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003391005 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 3.79e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.09e+05    |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.000428    |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 1.01e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 208    |\n",
      "|    time_elapsed    | 673    |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-6453.78 +/- 7048.30\n",
      "Episode length: 335.25 +/- 49.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -6.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 428000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041948482 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.000362     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.67e+04     |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    std                  | 0.678        |\n",
      "|    value_loss           | 2.91e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 209    |\n",
      "|    time_elapsed    | 676    |\n",
      "|    total_timesteps | 428032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 210          |\n",
      "|    time_elapsed         | 678          |\n",
      "|    total_timesteps      | 430080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003592775 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.00651      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.52e+05     |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | -0.000717    |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-5132.19 +/- 5146.59\n",
      "Episode length: 306.25 +/- 23.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029179754 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.000574     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.29e+05     |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    std                  | 0.678        |\n",
      "|    value_loss           | 6.44e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 211    |\n",
      "|    time_elapsed    | 681    |\n",
      "|    total_timesteps | 432128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 634          |\n",
      "|    iterations           | 212          |\n",
      "|    time_elapsed         | 684          |\n",
      "|    total_timesteps      | 434176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021113588 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.000113     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.33e+05     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    std                  | 0.689        |\n",
      "|    value_loss           | 1.37e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-5927.90 +/- 7853.12\n",
      "Episode length: 325.88 +/- 42.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -5.93e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 436000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012766182 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.000141     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.7e+05      |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -2.62e-05    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 8.19e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 634    |\n",
      "|    iterations      | 213    |\n",
      "|    time_elapsed    | 687    |\n",
      "|    total_timesteps | 436224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 689         |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002567915 |\n",
      "|    clip_fraction        | 0.0061      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.000196    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.94e+06    |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -2.58e-05   |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 2.45e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-2549.41 +/- 2629.27\n",
      "Episode length: 310.50 +/- 45.69\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 310           |\n",
      "|    mean_reward          | -2.55e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 440000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021518773 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 6.94e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+04      |\n",
      "|    n_updates            | 2140          |\n",
      "|    policy_gradient_loss | 5.3e-05       |\n",
      "|    std                  | 0.681         |\n",
      "|    value_loss           | 3.94e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 635    |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 692    |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 636          |\n",
      "|    iterations           | 216          |\n",
      "|    time_elapsed         | 695          |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.300984e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.023        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.27e+05     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -0.000514    |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-8566.51 +/- 13703.52\n",
      "Episode length: 288.50 +/- 47.34\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 288           |\n",
      "|    mean_reward          | -8.57e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 444000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060207583 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0148        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.97e+04      |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | -0.000881     |\n",
      "|    std                  | 0.681         |\n",
      "|    value_loss           | 7.17e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 636    |\n",
      "|    iterations      | 217    |\n",
      "|    time_elapsed    | 698    |\n",
      "|    total_timesteps | 444416 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 637           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 700           |\n",
      "|    total_timesteps      | 446464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068304245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0164        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.93e+05      |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -0.000903     |\n",
      "|    std                  | 0.679         |\n",
      "|    value_loss           | 1.29e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-13399.60 +/- 16306.11\n",
      "Episode length: 302.75 +/- 47.85\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 303           |\n",
      "|    mean_reward          | -1.34e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 448000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040318948 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.00892       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.24e+04      |\n",
      "|    n_updates            | 2180          |\n",
      "|    policy_gradient_loss | -0.000469     |\n",
      "|    std                  | 0.678         |\n",
      "|    value_loss           | 1.65e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 637    |\n",
      "|    iterations      | 219    |\n",
      "|    time_elapsed    | 703    |\n",
      "|    total_timesteps | 448512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003033264 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.00298     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03e+06    |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 2.06e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-13364.31 +/- 12923.35\n",
      "Episode length: 329.62 +/- 53.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 330          |\n",
      "|    mean_reward          | -1.34e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 452000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059664007 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.000293     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+06      |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.00799     |\n",
      "|    std                  | 0.672        |\n",
      "|    value_loss           | 3.44e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 637    |\n",
      "|    iterations      | 221    |\n",
      "|    time_elapsed    | 709    |\n",
      "|    total_timesteps | 452608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 711          |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060858596 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 7.97e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.33e+05     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 5.07e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-9223.74 +/- 9730.27\n",
      "Episode length: 327.62 +/- 42.11\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 328           |\n",
      "|    mean_reward          | -9.22e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 456000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043770976 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.00611       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.65e+04      |\n",
      "|    n_updates            | 2220          |\n",
      "|    policy_gradient_loss | -0.000477     |\n",
      "|    std                  | 0.668         |\n",
      "|    value_loss           | 1.05e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 638    |\n",
      "|    iterations      | 223    |\n",
      "|    time_elapsed    | 715    |\n",
      "|    total_timesteps | 456704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 639          |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 717          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.984174e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0121       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57e+06     |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.000102    |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-3326.38 +/- 4537.02\n",
      "Episode length: 323.12 +/- 47.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -3.33e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 460000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049240305 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.01         |\n",
      "|    explained_variance   | 0.00548       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.71e+05      |\n",
      "|    n_updates            | 2240          |\n",
      "|    policy_gradient_loss | -0.000458     |\n",
      "|    std                  | 0.662         |\n",
      "|    value_loss           | 1.45e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 639    |\n",
      "|    iterations      | 225    |\n",
      "|    time_elapsed    | 720    |\n",
      "|    total_timesteps | 460800 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 640           |\n",
      "|    iterations           | 226           |\n",
      "|    time_elapsed         | 722           |\n",
      "|    total_timesteps      | 462848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059241324 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.00115       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.66e+04      |\n",
      "|    n_updates            | 2250          |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    std                  | 0.66          |\n",
      "|    value_loss           | 6.76e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-7290.94 +/- 7857.03\n",
      "Episode length: 349.50 +/- 24.52\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 350           |\n",
      "|    mean_reward          | -7.29e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 464000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3671433e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.0191        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.58e+05      |\n",
      "|    n_updates            | 2260          |\n",
      "|    policy_gradient_loss | -5.91e-05     |\n",
      "|    std                  | 0.661         |\n",
      "|    value_loss           | 3.85e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 640    |\n",
      "|    iterations      | 227    |\n",
      "|    time_elapsed    | 726    |\n",
      "|    total_timesteps | 464896 |\n",
      "-------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 640            |\n",
      "|    iterations           | 228            |\n",
      "|    time_elapsed         | 728            |\n",
      "|    total_timesteps      | 466944         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000119021686 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.01          |\n",
      "|    explained_variance   | -0.0543        |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 2.71e+04       |\n",
      "|    n_updates            | 2270           |\n",
      "|    policy_gradient_loss | -7.01e-06      |\n",
      "|    std                  | 0.661          |\n",
      "|    value_loss           | 2.05e+04       |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-8830.67 +/- 7312.14\n",
      "Episode length: 313.38 +/- 40.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 313          |\n",
      "|    mean_reward          | -8.83e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 468000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003677429 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0381       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.45e+05     |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.000815    |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 1.18e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 640    |\n",
      "|    iterations      | 229    |\n",
      "|    time_elapsed    | 731    |\n",
      "|    total_timesteps | 468992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 734          |\n",
      "|    total_timesteps      | 471040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.691074e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0106       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+06     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.000178    |\n",
      "|    std                  | 0.657        |\n",
      "|    value_loss           | 1.85e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-6514.01 +/- 8357.59\n",
      "Episode length: 326.38 +/- 59.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -6.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 472000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006830502 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.0196       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.95e+03     |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.000341    |\n",
      "|    std                  | 0.653        |\n",
      "|    value_loss           | 1.14e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 641    |\n",
      "|    iterations      | 231    |\n",
      "|    time_elapsed    | 737    |\n",
      "|    total_timesteps | 473088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 739          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007530847 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.993       |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.42e+04     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.000473    |\n",
      "|    std                  | 0.653        |\n",
      "|    value_loss           | 3.5e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-10568.58 +/- 12151.02\n",
      "Episode length: 324.12 +/- 43.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | -1.06e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 476000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.492546e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.991       |\n",
      "|    explained_variance   | 0.0276       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.17e+05     |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.000362    |\n",
      "|    std                  | 0.651        |\n",
      "|    value_loss           | 8.58e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 642    |\n",
      "|    iterations      | 233    |\n",
      "|    time_elapsed    | 743    |\n",
      "|    total_timesteps | 477184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 234          |\n",
      "|    time_elapsed         | 745          |\n",
      "|    total_timesteps      | 479232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.667812e-05 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.992       |\n",
      "|    explained_variance   | 0.0782       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.22e+03     |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -0.000119    |\n",
      "|    std                  | 0.653        |\n",
      "|    value_loss           | 1.16e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-11994.05 +/- 12574.83\n",
      "Episode length: 299.25 +/- 48.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 299           |\n",
      "|    mean_reward          | -1.2e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 480000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021587069 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.994        |\n",
      "|    explained_variance   | 0.0326        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.36e+06      |\n",
      "|    n_updates            | 2340          |\n",
      "|    policy_gradient_loss | -9.77e-06     |\n",
      "|    std                  | 0.654         |\n",
      "|    value_loss           | 3.42e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 642    |\n",
      "|    iterations      | 235    |\n",
      "|    time_elapsed    | 748    |\n",
      "|    total_timesteps | 481280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 643          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 751          |\n",
      "|    total_timesteps      | 483328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009346382 |\n",
      "|    clip_fraction        | 0.00532      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.999       |\n",
      "|    explained_variance   | 0.00265      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97e+05     |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 2.59e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-8804.10 +/- 9538.43\n",
      "Episode length: 304.62 +/- 54.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 305           |\n",
      "|    mean_reward          | -8.8e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 484000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068354135 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.0116        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.46e+06      |\n",
      "|    n_updates            | 2360          |\n",
      "|    policy_gradient_loss | -0.000715     |\n",
      "|    std                  | 0.66          |\n",
      "|    value_loss           | 1.17e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 643    |\n",
      "|    iterations      | 237    |\n",
      "|    time_elapsed    | 754    |\n",
      "|    total_timesteps | 485376 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 643           |\n",
      "|    iterations           | 238           |\n",
      "|    time_elapsed         | 756           |\n",
      "|    total_timesteps      | 487424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026411648 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1            |\n",
      "|    explained_variance   | 0.0135        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.83e+05      |\n",
      "|    n_updates            | 2370          |\n",
      "|    policy_gradient_loss | -0.000208     |\n",
      "|    std                  | 0.655         |\n",
      "|    value_loss           | 1.34e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-3862.98 +/- 4059.99\n",
      "Episode length: 311.00 +/- 51.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -3.86e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 488000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027451208 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.00533      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.7e+04      |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    std                  | 0.658        |\n",
      "|    value_loss           | 1.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 643    |\n",
      "|    iterations      | 239    |\n",
      "|    time_elapsed    | 760    |\n",
      "|    total_timesteps | 489472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 762          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029265685 |\n",
      "|    clip_fraction        | 0.00459      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.0148       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45e+06     |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 2.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-12008.48 +/- 11080.04\n",
      "Episode length: 334.25 +/- 11.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 334         |\n",
      "|    mean_reward          | -1.2e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 492000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004147946 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.000859    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.44e+05    |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.000677   |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 8.79e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 241    |\n",
      "|    time_elapsed    | 765    |\n",
      "|    total_timesteps | 493568 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007278183 |\n",
      "|    clip_fraction        | 0.0615      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.000575    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.08e+05    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    std                  | 0.673       |\n",
      "|    value_loss           | 2.75e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-7916.24 +/- 8334.43\n",
      "Episode length: 312.88 +/- 40.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | -7.92e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001238007 |\n",
      "|    clip_fraction        | 4.88e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0173      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.29e+04    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.000793   |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 2.5e+04     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 243    |\n",
      "|    time_elapsed    | 771    |\n",
      "|    total_timesteps | 497664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 773          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021705911 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0318       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.673        |\n",
      "|    value_loss           | 8.71e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-5467.74 +/- 6486.61\n",
      "Episode length: 313.88 +/- 47.27\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 314           |\n",
      "|    mean_reward          | -5.47e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 500000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1790546e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.019         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.85e+04      |\n",
      "|    n_updates            | 2440          |\n",
      "|    policy_gradient_loss | 0.000127      |\n",
      "|    std                  | 0.672         |\n",
      "|    value_loss           | 1.06e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 245    |\n",
      "|    time_elapsed    | 777    |\n",
      "|    total_timesteps | 501760 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 246           |\n",
      "|    time_elapsed         | 780           |\n",
      "|    total_timesteps      | 503808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9061655e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.012         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.18e+04      |\n",
      "|    n_updates            | 2450          |\n",
      "|    policy_gradient_loss | -0.00088      |\n",
      "|    std                  | 0.672         |\n",
      "|    value_loss           | 9.84e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-12242.61 +/- 10452.24\n",
      "Episode length: 355.75 +/- 45.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | -1.22e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.78002e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0174      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2e+06     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.000136   |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 9.04e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 247    |\n",
      "|    time_elapsed    | 784    |\n",
      "|    total_timesteps | 505856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 248          |\n",
      "|    time_elapsed         | 787          |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.415974e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.00903      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | -0.000285    |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-11706.01 +/- 9312.96\n",
      "Episode length: 337.38 +/- 44.35\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 337           |\n",
      "|    mean_reward          | -1.17e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 508000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9380765e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.0169        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.67e+05      |\n",
      "|    n_updates            | 2480          |\n",
      "|    policy_gradient_loss | -0.00038      |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 1.18e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 249    |\n",
      "|    time_elapsed    | 790    |\n",
      "|    total_timesteps | 509952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-16456.94 +/- 11927.83\n",
      "Episode length: 323.38 +/- 47.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 323         |\n",
      "|    mean_reward          | -1.65e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 512000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001113868 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0228      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.43e+05    |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.000895   |\n",
      "|    std                  | 0.666       |\n",
      "|    value_loss           | 1.41e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 250    |\n",
      "|    time_elapsed    | 794    |\n",
      "|    total_timesteps | 512000 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 251          |\n",
      "|    time_elapsed         | 796          |\n",
      "|    total_timesteps      | 514048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006036103 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.01e+04     |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.000642    |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 6.72e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-5172.88 +/- 6153.74\n",
      "Episode length: 335.62 +/- 35.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | -5.17e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016149068 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.00899      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.22e+05     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.000973    |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 5.41e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 252    |\n",
      "|    time_elapsed    | 800    |\n",
      "|    total_timesteps | 516096 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 253          |\n",
      "|    time_elapsed         | 803          |\n",
      "|    total_timesteps      | 518144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013220061 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0183       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.53e+05     |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.000182    |\n",
      "|    std                  | 0.673        |\n",
      "|    value_loss           | 9.47e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-10540.72 +/- 9169.95\n",
      "Episode length: 354.00 +/- 31.75\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 354           |\n",
      "|    mean_reward          | -1.05e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 520000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048492916 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0.0243        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.51e+05      |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 6.8e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 254    |\n",
      "|    time_elapsed    | 807    |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 809           |\n",
      "|    total_timesteps      | 522240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4681105e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0108        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.7e+06       |\n",
      "|    n_updates            | 2540          |\n",
      "|    policy_gradient_loss | -0.000159     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 1.38e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-10666.15 +/- 8985.74\n",
      "Episode length: 290.38 +/- 44.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -1.07e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 524000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033250328 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -0.00474     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+04     |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 3.15e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 256    |\n",
      "|    time_elapsed    | 812    |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 257           |\n",
      "|    time_elapsed         | 814           |\n",
      "|    total_timesteps      | 526336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019615245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0.0189        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.49e+05      |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | 5.14e-05      |\n",
      "|    std                  | 0.679         |\n",
      "|    value_loss           | 1.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-9436.88 +/- 10101.61\n",
      "Episode length: 310.62 +/- 48.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -9.44e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 528000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033229566 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.00512      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.26e+05     |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 1.98e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 258    |\n",
      "|    time_elapsed    | 818    |\n",
      "|    total_timesteps | 528384 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 646          |\n",
      "|    iterations           | 259          |\n",
      "|    time_elapsed         | 820          |\n",
      "|    total_timesteps      | 530432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016031365 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00116      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+06     |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 2.33e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-14929.16 +/- 8774.30\n",
      "Episode length: 294.50 +/- 41.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 294          |\n",
      "|    mean_reward          | -1.49e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 532000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060796365 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000488     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+03     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 3.87e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 646    |\n",
      "|    iterations      | 260    |\n",
      "|    time_elapsed    | 824    |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 646          |\n",
      "|    iterations           | 261          |\n",
      "|    time_elapsed         | 826          |\n",
      "|    total_timesteps      | 534528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.662206e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.026        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.94e+04     |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.000582    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.03e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-8519.50 +/- 8879.42\n",
      "Episode length: 346.62 +/- 33.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | -8.52e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 536000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008597288 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0105       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.687        |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 646    |\n",
      "|    iterations      | 262    |\n",
      "|    time_elapsed    | 830    |\n",
      "|    total_timesteps | 536576 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 647          |\n",
      "|    iterations           | 263          |\n",
      "|    time_elapsed         | 832          |\n",
      "|    total_timesteps      | 538624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017002192 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.00582      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09e+06     |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.000295    |\n",
      "|    std                  | 0.686        |\n",
      "|    value_loss           | 1.57e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-7066.22 +/- 7347.87\n",
      "Episode length: 323.12 +/- 26.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 323          |\n",
      "|    mean_reward          | -7.07e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 540000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072895116 |\n",
      "|    clip_fraction        | 0.0679       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.00387      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5e+05      |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    std                  | 0.687        |\n",
      "|    value_loss           | 9.58e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 646    |\n",
      "|    iterations      | 264    |\n",
      "|    time_elapsed    | 835    |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005975243 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00619     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.42e+05    |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 1.93e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-13549.84 +/- 9530.32\n",
      "Episode length: 307.62 +/- 32.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | -1.35e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003818682 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.000295    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.56e+06    |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 1.12e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 647    |\n",
      "|    iterations      | 266    |\n",
      "|    time_elapsed    | 841    |\n",
      "|    total_timesteps | 544768 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 267          |\n",
      "|    time_elapsed         | 843          |\n",
      "|    total_timesteps      | 546816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061101206 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.998       |\n",
      "|    explained_variance   | 0.00723      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.84e+04     |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 0.657        |\n",
      "|    value_loss           | 1.04e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-4864.37 +/- 6746.14\n",
      "Episode length: 302.50 +/- 55.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 302           |\n",
      "|    mean_reward          | -4.86e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 548000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029154736 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.997        |\n",
      "|    explained_variance   | 0.0145        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.92e+05      |\n",
      "|    n_updates            | 2670          |\n",
      "|    policy_gradient_loss | -0.000414     |\n",
      "|    std                  | 0.655         |\n",
      "|    value_loss           | 1.52e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 647    |\n",
      "|    iterations      | 268    |\n",
      "|    time_elapsed    | 847    |\n",
      "|    total_timesteps | 548864 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 269          |\n",
      "|    time_elapsed         | 849          |\n",
      "|    total_timesteps      | 550912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036573848 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.00832      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.88e+05     |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    std                  | 0.654        |\n",
      "|    value_loss           | 1.89e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-7460.35 +/- 6351.72\n",
      "Episode length: 302.00 +/- 58.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 302          |\n",
      "|    mean_reward          | -7.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 552000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045484924 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.999       |\n",
      "|    explained_variance   | 0.000996     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+05     |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    std                  | 0.656        |\n",
      "|    value_loss           | 6.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 648    |\n",
      "|    iterations      | 270    |\n",
      "|    time_elapsed    | 853    |\n",
      "|    total_timesteps | 552960 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 648           |\n",
      "|    iterations           | 271           |\n",
      "|    time_elapsed         | 855           |\n",
      "|    total_timesteps      | 555008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054192706 |\n",
      "|    clip_fraction        | 0.00249       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.993        |\n",
      "|    explained_variance   | 0.00487       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.16e+05      |\n",
      "|    n_updates            | 2700          |\n",
      "|    policy_gradient_loss | -0.00165      |\n",
      "|    std                  | 0.65          |\n",
      "|    value_loss           | 8e+05         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-16021.09 +/- 10823.86\n",
      "Episode length: 329.38 +/- 48.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | -1.6e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 556000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006599878 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.00339     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.69e+05    |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    std                  | 0.65        |\n",
      "|    value_loss           | 2.14e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 648    |\n",
      "|    iterations      | 272    |\n",
      "|    time_elapsed    | 859    |\n",
      "|    total_timesteps | 557056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 273          |\n",
      "|    time_elapsed         | 861          |\n",
      "|    total_timesteps      | 559104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034676427 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.988       |\n",
      "|    explained_variance   | 0.00386      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.08e+04     |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    std                  | 0.647        |\n",
      "|    value_loss           | 1.29e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-5419.04 +/- 6938.71\n",
      "Episode length: 307.88 +/- 27.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | -5.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006092505 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.984       |\n",
      "|    explained_variance   | 0.0277       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+05     |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.000329    |\n",
      "|    std                  | 0.647        |\n",
      "|    value_loss           | 3.55e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 648    |\n",
      "|    iterations      | 274    |\n",
      "|    time_elapsed    | 864    |\n",
      "|    total_timesteps | 561152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 275          |\n",
      "|    time_elapsed         | 866          |\n",
      "|    total_timesteps      | 563200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.236444e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.982       |\n",
      "|    explained_variance   | 0.0194       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.000328    |\n",
      "|    std                  | 0.646        |\n",
      "|    value_loss           | 6.96e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-15174.24 +/- 13044.03\n",
      "Episode length: 338.38 +/- 50.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 338           |\n",
      "|    mean_reward          | -1.52e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 564000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057970424 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.979        |\n",
      "|    explained_variance   | 0.0219        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.62e+05      |\n",
      "|    n_updates            | 2750          |\n",
      "|    policy_gradient_loss | -0.00065      |\n",
      "|    std                  | 0.644         |\n",
      "|    value_loss           | 1.05e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 649    |\n",
      "|    iterations      | 276    |\n",
      "|    time_elapsed    | 870    |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 649           |\n",
      "|    iterations           | 277           |\n",
      "|    time_elapsed         | 872           |\n",
      "|    total_timesteps      | 567296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025506504 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.979        |\n",
      "|    explained_variance   | 0.0228        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.32e+05      |\n",
      "|    n_updates            | 2760          |\n",
      "|    policy_gradient_loss | -0.000239     |\n",
      "|    std                  | 0.644         |\n",
      "|    value_loss           | 4.5e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-15822.22 +/- 12020.85\n",
      "Episode length: 318.50 +/- 50.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -1.58e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 568000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013714844 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.978       |\n",
      "|    explained_variance   | 0.0811       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.34e+05     |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 3.39e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 649    |\n",
      "|    iterations      | 278    |\n",
      "|    time_elapsed    | 876    |\n",
      "|    total_timesteps | 569344 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 650           |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 878           |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7649297e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.977        |\n",
      "|    explained_variance   | 0.0302        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | 3.64e-05      |\n",
      "|    std                  | 0.643         |\n",
      "|    value_loss           | 1.11e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=-8551.49 +/- 8817.49\n",
      "Episode length: 289.50 +/- 29.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -8.55e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 572000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.164566e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.977       |\n",
      "|    explained_variance   | 0.0148       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72e+06     |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.54e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 650    |\n",
      "|    iterations      | 280    |\n",
      "|    time_elapsed    | 881    |\n",
      "|    total_timesteps | 573440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 281          |\n",
      "|    time_elapsed         | 883          |\n",
      "|    total_timesteps      | 575488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032997793 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.976       |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.53e+05     |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.000765    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.21e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-10748.40 +/- 9379.97\n",
      "Episode length: 344.25 +/- 24.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -1.07e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 576000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040525254 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.966       |\n",
      "|    explained_variance   | 0.00703      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.8e+05      |\n",
      "|    n_updates            | 2810         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.63         |\n",
      "|    value_loss           | 1.81e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 650    |\n",
      "|    iterations      | 282    |\n",
      "|    time_elapsed    | 887    |\n",
      "|    total_timesteps | 577536 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 283          |\n",
      "|    time_elapsed         | 889          |\n",
      "|    total_timesteps      | 579584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033275718 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.957       |\n",
      "|    explained_variance   | 0.00254      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.63e+04     |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 1.51e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-9475.61 +/- 10153.82\n",
      "Episode length: 335.38 +/- 30.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -9.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 580000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.718926e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.959       |\n",
      "|    explained_variance   | 0.00821      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.71e+04     |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -4.31e-05    |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 6.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 651    |\n",
      "|    iterations      | 284    |\n",
      "|    time_elapsed    | 893    |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 895          |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.473341e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.0293       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.69e+04     |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -6.19e-05    |\n",
      "|    std                  | 0.632        |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-11957.76 +/- 11978.30\n",
      "Episode length: 330.62 +/- 38.03\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 331           |\n",
      "|    mean_reward          | -1.2e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 584000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049422064 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.962        |\n",
      "|    explained_variance   | 0.0295        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.48e+04      |\n",
      "|    n_updates            | 2850          |\n",
      "|    policy_gradient_loss | -0.000329     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 3.94e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 651    |\n",
      "|    iterations      | 286    |\n",
      "|    time_elapsed    | 898    |\n",
      "|    total_timesteps | 585728 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 287          |\n",
      "|    time_elapsed         | 900          |\n",
      "|    total_timesteps      | 587776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010901398 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.964       |\n",
      "|    explained_variance   | 0.0446       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.75e+05     |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.000782    |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 2.54e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-12688.28 +/- 14870.17\n",
      "Episode length: 308.00 +/- 65.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -1.27e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 588000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013048529 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.965        |\n",
      "|    explained_variance   | 0.0137        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.02e+05      |\n",
      "|    n_updates            | 2870          |\n",
      "|    policy_gradient_loss | -0.000132     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 1.5e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 652    |\n",
      "|    iterations      | 288    |\n",
      "|    time_elapsed    | 904    |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 906          |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029450473 |\n",
      "|    clip_fraction        | 0.0062       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.963       |\n",
      "|    explained_variance   | 0.0105       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.31e+05     |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 1.65e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-8379.36 +/- 7595.19\n",
      "Episode length: 315.50 +/- 55.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 316           |\n",
      "|    mean_reward          | -8.38e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 592000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027296916 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.966        |\n",
      "|    explained_variance   | 0.00336       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.44e+05      |\n",
      "|    n_updates            | 2890          |\n",
      "|    policy_gradient_loss | -0.000821     |\n",
      "|    std                  | 0.636         |\n",
      "|    value_loss           | 1.18e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 652    |\n",
      "|    iterations      | 290    |\n",
      "|    time_elapsed    | 909    |\n",
      "|    total_timesteps | 593920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 911         |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005353211 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.000365    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.26e+03    |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    std                  | 0.636       |\n",
      "|    value_loss           | 1.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-6402.55 +/- 7609.88\n",
      "Episode length: 281.00 +/- 49.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 281           |\n",
      "|    mean_reward          | -6.4e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 596000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015472111 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.968        |\n",
      "|    explained_variance   | -0.0987       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.79e+04      |\n",
      "|    n_updates            | 2910          |\n",
      "|    policy_gradient_loss | 0.000239      |\n",
      "|    std                  | 0.64          |\n",
      "|    value_loss           | 3.2e+04       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 292    |\n",
      "|    time_elapsed    | 915    |\n",
      "|    total_timesteps | 598016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-8353.02 +/- 12645.68\n",
      "Episode length: 321.38 +/- 42.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 321         |\n",
      "|    mean_reward          | -8.35e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.78435e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.0204      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26e+04    |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.00013    |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 1.11e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 293    |\n",
      "|    time_elapsed    | 918    |\n",
      "|    total_timesteps | 600064 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 654           |\n",
      "|    iterations           | 294           |\n",
      "|    time_elapsed         | 920           |\n",
      "|    total_timesteps      | 602112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044501814 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.974        |\n",
      "|    explained_variance   | 0.0155        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.12e+05      |\n",
      "|    n_updates            | 2930          |\n",
      "|    policy_gradient_loss | -0.000142     |\n",
      "|    std                  | 0.642         |\n",
      "|    value_loss           | 3.02e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=-7120.11 +/- 6845.32\n",
      "Episode length: 319.88 +/- 36.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -7.12e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 604000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0218544e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.976        |\n",
      "|    explained_variance   | 0.0435        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.32e+05      |\n",
      "|    n_updates            | 2940          |\n",
      "|    policy_gradient_loss | -0.00018      |\n",
      "|    std                  | 0.643         |\n",
      "|    value_loss           | 3e+06         |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 295    |\n",
      "|    time_elapsed    | 923    |\n",
      "|    total_timesteps | 604160 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 654          |\n",
      "|    iterations           | 296          |\n",
      "|    time_elapsed         | 926          |\n",
      "|    total_timesteps      | 606208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016751048 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.974       |\n",
      "|    explained_variance   | 0.0176       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.38e+04     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000888    |\n",
      "|    std                  | 0.638        |\n",
      "|    value_loss           | 2.48e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=-4775.57 +/- 6818.79\n",
      "Episode length: 328.75 +/- 40.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 329          |\n",
      "|    mean_reward          | -4.78e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 608000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045843786 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.969       |\n",
      "|    explained_variance   | -0.00771     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.49e+03     |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    std                  | 0.636        |\n",
      "|    value_loss           | 2.7e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 297    |\n",
      "|    time_elapsed    | 929    |\n",
      "|    total_timesteps | 608256 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 654           |\n",
      "|    iterations           | 298           |\n",
      "|    time_elapsed         | 932           |\n",
      "|    total_timesteps      | 610304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024371682 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.967        |\n",
      "|    explained_variance   | 0.0417        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 2970          |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    std                  | 0.637         |\n",
      "|    value_loss           | 2.15e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-7597.05 +/- 7996.07\n",
      "Episode length: 318.38 +/- 39.03\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 318           |\n",
      "|    mean_reward          | -7.6e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 612000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045988685 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.969        |\n",
      "|    explained_variance   | 0.0521        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.98e+05      |\n",
      "|    n_updates            | 2980          |\n",
      "|    policy_gradient_loss | -0.00047      |\n",
      "|    std                  | 0.639         |\n",
      "|    value_loss           | 8.36e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 299    |\n",
      "|    time_elapsed    | 935    |\n",
      "|    total_timesteps | 612352 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 654           |\n",
      "|    iterations           | 300           |\n",
      "|    time_elapsed         | 938           |\n",
      "|    total_timesteps      | 614400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094475737 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.973        |\n",
      "|    explained_variance   | 0.041         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.09e+05      |\n",
      "|    n_updates            | 2990          |\n",
      "|    policy_gradient_loss | -0.000486     |\n",
      "|    std                  | 0.642         |\n",
      "|    value_loss           | 6.39e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=-13127.78 +/- 10207.33\n",
      "Episode length: 331.00 +/- 46.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | -1.31e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 616000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009469364 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.975       |\n",
      "|    explained_variance   | 0.0689       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+04     |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -6.74e-05    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 4.78e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 301    |\n",
      "|    time_elapsed    | 941    |\n",
      "|    total_timesteps | 616448 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 655           |\n",
      "|    iterations           | 302           |\n",
      "|    time_elapsed         | 944           |\n",
      "|    total_timesteps      | 618496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9778967e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.973        |\n",
      "|    explained_variance   | 0.0772        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.29e+05      |\n",
      "|    n_updates            | 3010          |\n",
      "|    policy_gradient_loss | -9.76e-05     |\n",
      "|    std                  | 0.639         |\n",
      "|    value_loss           | 3.17e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-5685.10 +/- 7269.76\n",
      "Episode length: 323.12 +/- 39.18\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -5.69e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 620000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010046175 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.971        |\n",
      "|    explained_variance   | 0.0447        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+06      |\n",
      "|    n_updates            | 3020          |\n",
      "|    policy_gradient_loss | -0.00035      |\n",
      "|    std                  | 0.638         |\n",
      "|    value_loss           | 1.31e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 654    |\n",
      "|    iterations      | 303    |\n",
      "|    time_elapsed    | 947    |\n",
      "|    total_timesteps | 620544 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 655       |\n",
      "|    iterations           | 304       |\n",
      "|    time_elapsed         | 949       |\n",
      "|    total_timesteps      | 622592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0062416 |\n",
      "|    clip_fraction        | 0.0309    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.968    |\n",
      "|    explained_variance   | -0.0285   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.39e+04  |\n",
      "|    n_updates            | 3030      |\n",
      "|    policy_gradient_loss | -0.00235  |\n",
      "|    std                  | 0.635     |\n",
      "|    value_loss           | 2.72e+04  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=-9890.63 +/- 8967.13\n",
      "Episode length: 320.38 +/- 53.28\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -9.89e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 624000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00089257973 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.964        |\n",
      "|    explained_variance   | 0.0532        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.47e+04      |\n",
      "|    n_updates            | 3040          |\n",
      "|    policy_gradient_loss | -0.000298     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 2.41e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 655    |\n",
      "|    iterations      | 305    |\n",
      "|    time_elapsed    | 953    |\n",
      "|    total_timesteps | 624640 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 655           |\n",
      "|    iterations           | 306           |\n",
      "|    time_elapsed         | 955           |\n",
      "|    total_timesteps      | 626688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011848043 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.964        |\n",
      "|    explained_variance   | 0.0452        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.18e+05      |\n",
      "|    n_updates            | 3050          |\n",
      "|    policy_gradient_loss | -7.37e-05     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 4.72e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=-7851.40 +/- 6305.94\n",
      "Episode length: 337.50 +/- 29.29\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 338           |\n",
      "|    mean_reward          | -7.85e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 628000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012691194 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.966        |\n",
      "|    explained_variance   | 0.0569        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.82e+05      |\n",
      "|    n_updates            | 3060          |\n",
      "|    policy_gradient_loss | -0.000436     |\n",
      "|    std                  | 0.637         |\n",
      "|    value_loss           | 1.29e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 655    |\n",
      "|    iterations      | 307    |\n",
      "|    time_elapsed    | 958    |\n",
      "|    total_timesteps | 628736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 308          |\n",
      "|    time_elapsed         | 961          |\n",
      "|    total_timesteps      | 630784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005686477 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.969       |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+05     |\n",
      "|    n_updates            | 3070         |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    std                  | 0.637        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=-12943.94 +/- 12743.20\n",
      "Episode length: 334.25 +/- 52.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 334          |\n",
      "|    mean_reward          | -1.29e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 632000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021916688 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.967       |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.44e+04     |\n",
      "|    n_updates            | 3080         |\n",
      "|    policy_gradient_loss | -0.000672    |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 2.33e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 309    |\n",
      "|    time_elapsed    | 964    |\n",
      "|    total_timesteps | 632832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 310          |\n",
      "|    time_elapsed         | 967          |\n",
      "|    total_timesteps      | 634880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007686389 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.965       |\n",
      "|    explained_variance   | 0.0166       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.04e+05     |\n",
      "|    n_updates            | 3090         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 1.16e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=-8245.04 +/- 6936.24\n",
      "Episode length: 343.62 +/- 29.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -8.25e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 636000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004287958 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.964       |\n",
      "|    explained_variance   | 0.0132       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27e+05     |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | -5.97e-05    |\n",
      "|    std                  | 0.634        |\n",
      "|    value_loss           | 4.93e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 311    |\n",
      "|    time_elapsed    | 970    |\n",
      "|    total_timesteps | 636928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 312          |\n",
      "|    time_elapsed         | 972          |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016922648 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.032        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85e+05     |\n",
      "|    n_updates            | 3110         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 6.21e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-9357.68 +/- 8456.83\n",
      "Episode length: 338.50 +/- 31.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 338          |\n",
      "|    mean_reward          | -9.36e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007668876 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.0638       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.27e+05     |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -0.000115    |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 1.1e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 313    |\n",
      "|    time_elapsed    | 976    |\n",
      "|    total_timesteps | 641024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 656           |\n",
      "|    iterations           | 314           |\n",
      "|    time_elapsed         | 978           |\n",
      "|    total_timesteps      | 643072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7292415e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.961        |\n",
      "|    explained_variance   | 0.0238        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.83e+04      |\n",
      "|    n_updates            | 3130          |\n",
      "|    policy_gradient_loss | -5.94e-05     |\n",
      "|    std                  | 0.631         |\n",
      "|    value_loss           | 9.81e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=-13189.60 +/- 13009.17\n",
      "Episode length: 320.00 +/- 25.18\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -1.32e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 644000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047522812 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.96         |\n",
      "|    explained_variance   | 0.0183        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.31e+05      |\n",
      "|    n_updates            | 3140          |\n",
      "|    policy_gradient_loss | -0.000484     |\n",
      "|    std                  | 0.632         |\n",
      "|    value_loss           | 6.47e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 315    |\n",
      "|    time_elapsed    | 982    |\n",
      "|    total_timesteps | 645120 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 657           |\n",
      "|    iterations           | 316           |\n",
      "|    time_elapsed         | 984           |\n",
      "|    total_timesteps      | 647168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023463079 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.961        |\n",
      "|    explained_variance   | 0.0512        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.45e+05      |\n",
      "|    n_updates            | 3150          |\n",
      "|    policy_gradient_loss | -0.000276     |\n",
      "|    std                  | 0.633         |\n",
      "|    value_loss           | 8.3e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-10348.22 +/- 7746.28\n",
      "Episode length: 309.50 +/- 47.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 310          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 648000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005266466 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.962       |\n",
      "|    explained_variance   | 0.041        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03e+06     |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -0.00061     |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 1.48e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 657    |\n",
      "|    iterations      | 317    |\n",
      "|    time_elapsed    | 987    |\n",
      "|    total_timesteps | 649216 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 657          |\n",
      "|    iterations           | 318          |\n",
      "|    time_elapsed         | 990          |\n",
      "|    total_timesteps      | 651264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.279618e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.963       |\n",
      "|    explained_variance   | 0.0266       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.82e+03     |\n",
      "|    n_updates            | 3170         |\n",
      "|    policy_gradient_loss | -0.000155    |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 8.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-11395.01 +/- 12880.98\n",
      "Episode length: 319.75 +/- 48.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -1.14e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 652000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010862561 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.966        |\n",
      "|    explained_variance   | 0.0198        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+05      |\n",
      "|    n_updates            | 3180          |\n",
      "|    policy_gradient_loss | -8.51e-05     |\n",
      "|    std                  | 0.637         |\n",
      "|    value_loss           | 7.27e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 657    |\n",
      "|    iterations      | 319    |\n",
      "|    time_elapsed    | 993    |\n",
      "|    total_timesteps | 653312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 995         |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001540784 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0315      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.28e+04    |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    std                  | 0.637       |\n",
      "|    value_loss           | 4.93e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-6727.03 +/- 7877.21\n",
      "Episode length: 296.25 +/- 55.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 296          |\n",
      "|    mean_reward          | -6.73e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009466948 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.972       |\n",
      "|    explained_variance   | 0.0402       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.78e+05     |\n",
      "|    n_updates            | 3200         |\n",
      "|    policy_gradient_loss | -0.000641    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 657    |\n",
      "|    iterations      | 321    |\n",
      "|    time_elapsed    | 999    |\n",
      "|    total_timesteps | 657408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 1001        |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002496759 |\n",
      "|    clip_fraction        | 0.00146     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.978      |\n",
      "|    explained_variance   | 0.0197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.94e+05    |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.000857   |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 1.51e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-6834.06 +/- 6912.56\n",
      "Episode length: 319.88 +/- 37.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -6.83e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023697722 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.983       |\n",
      "|    explained_variance   | 0.0193       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08e+05     |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.648        |\n",
      "|    value_loss           | 7.28e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 323    |\n",
      "|    time_elapsed    | 1005   |\n",
      "|    total_timesteps | 661504 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 324          |\n",
      "|    time_elapsed         | 1007         |\n",
      "|    total_timesteps      | 663552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017855235 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.986       |\n",
      "|    explained_variance   | 0.0115       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.13e+05     |\n",
      "|    n_updates            | 3230         |\n",
      "|    policy_gradient_loss | -0.000537    |\n",
      "|    std                  | 0.649        |\n",
      "|    value_loss           | 8.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-10444.59 +/- 9270.79\n",
      "Episode length: 316.62 +/- 46.73\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 317           |\n",
      "|    mean_reward          | -1.04e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 664000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029307592 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.981        |\n",
      "|    explained_variance   | 0.0126        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.27e+04      |\n",
      "|    n_updates            | 3240          |\n",
      "|    policy_gradient_loss | -0.000286     |\n",
      "|    std                  | 0.64          |\n",
      "|    value_loss           | 3.85e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 325    |\n",
      "|    time_elapsed    | 1010   |\n",
      "|    total_timesteps | 665600 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 326          |\n",
      "|    time_elapsed         | 1013         |\n",
      "|    total_timesteps      | 667648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.991641e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.973       |\n",
      "|    explained_variance   | 0.0327       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+06     |\n",
      "|    n_updates            | 3250         |\n",
      "|    policy_gradient_loss | -0.000138    |\n",
      "|    std                  | 0.64         |\n",
      "|    value_loss           | 1.84e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-4542.47 +/- 5970.22\n",
      "Episode length: 290.00 +/- 36.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | -4.54e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 668000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075372495 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | 0.0253       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.06e+04     |\n",
      "|    n_updates            | 3260         |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    std                  | 0.632        |\n",
      "|    value_loss           | 3.39e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 327    |\n",
      "|    time_elapsed    | 1016   |\n",
      "|    total_timesteps | 669696 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 328           |\n",
      "|    time_elapsed         | 1018          |\n",
      "|    total_timesteps      | 671744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031904742 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.96         |\n",
      "|    explained_variance   | 0.0874        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.92e+05      |\n",
      "|    n_updates            | 3270          |\n",
      "|    policy_gradient_loss | -0.00152      |\n",
      "|    std                  | 0.631         |\n",
      "|    value_loss           | 5.12e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-12838.18 +/- 8026.79\n",
      "Episode length: 328.62 +/- 43.83\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 329           |\n",
      "|    mean_reward          | -1.28e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 672000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015971079 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.96         |\n",
      "|    explained_variance   | 0.0199        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.07e+05      |\n",
      "|    n_updates            | 3280          |\n",
      "|    policy_gradient_loss | -0.00061      |\n",
      "|    std                  | 0.633         |\n",
      "|    value_loss           | 2.01e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 329    |\n",
      "|    time_elapsed    | 1021   |\n",
      "|    total_timesteps | 673792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 659         |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 1024        |\n",
      "|    total_timesteps      | 675840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004043618 |\n",
      "|    clip_fraction        | 0.00869     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97e+04    |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    std                  | 0.632       |\n",
      "|    value_loss           | 5.46e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=-13536.27 +/- 9523.61\n",
      "Episode length: 332.50 +/- 53.97\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 332           |\n",
      "|    mean_reward          | -1.35e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 676000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040161016 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.961        |\n",
      "|    explained_variance   | 0.0775        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.87e+05      |\n",
      "|    n_updates            | 3300          |\n",
      "|    policy_gradient_loss | -0.000432     |\n",
      "|    std                  | 0.634         |\n",
      "|    value_loss           | 9.89e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 331    |\n",
      "|    time_elapsed    | 1027   |\n",
      "|    total_timesteps | 677888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 1029        |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000448161 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.0868      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35e+05    |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.000425   |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 1.67e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-3287.91 +/- 5132.63\n",
      "Episode length: 284.75 +/- 32.66\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 285           |\n",
      "|    mean_reward          | -3.29e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 680000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030257914 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.959        |\n",
      "|    explained_variance   | 0.0193        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.27e+04      |\n",
      "|    n_updates            | 3320          |\n",
      "|    policy_gradient_loss | -0.000507     |\n",
      "|    std                  | 0.631         |\n",
      "|    value_loss           | 5.23e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 333    |\n",
      "|    time_elapsed    | 1033   |\n",
      "|    total_timesteps | 681984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-12865.74 +/- 9340.72\n",
      "Episode length: 298.62 +/- 49.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | -1.29e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 684000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005893209 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.955       |\n",
      "|    explained_variance   | 0.0548       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.13e+04     |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -0.000652    |\n",
      "|    std                  | 0.627        |\n",
      "|    value_loss           | 5.27e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 334    |\n",
      "|    time_elapsed    | 1036   |\n",
      "|    total_timesteps | 684032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 335          |\n",
      "|    time_elapsed         | 1038         |\n",
      "|    total_timesteps      | 686080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018892423 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+04     |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.626        |\n",
      "|    value_loss           | 8.67e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-6992.20 +/- 7985.57\n",
      "Episode length: 322.75 +/- 32.98\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 323           |\n",
      "|    mean_reward          | -6.99e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 688000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056287245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.95         |\n",
      "|    explained_variance   | 0.0769        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.85e+05      |\n",
      "|    n_updates            | 3350          |\n",
      "|    policy_gradient_loss | -0.000333     |\n",
      "|    std                  | 0.625         |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 336    |\n",
      "|    time_elapsed    | 1042   |\n",
      "|    total_timesteps | 688128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 337          |\n",
      "|    time_elapsed         | 1044         |\n",
      "|    total_timesteps      | 690176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009600342 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+05     |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.000793    |\n",
      "|    std                  | 0.624        |\n",
      "|    value_loss           | 1.55e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=-6212.45 +/- 8277.85\n",
      "Episode length: 337.62 +/- 40.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 338           |\n",
      "|    mean_reward          | -6.21e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 692000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068882364 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.948        |\n",
      "|    explained_variance   | 0.0344        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.93e+05      |\n",
      "|    n_updates            | 3370          |\n",
      "|    policy_gradient_loss | -0.000628     |\n",
      "|    std                  | 0.625         |\n",
      "|    value_loss           | 1.67e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 338    |\n",
      "|    time_elapsed    | 1047   |\n",
      "|    total_timesteps | 692224 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 339           |\n",
      "|    time_elapsed         | 1050          |\n",
      "|    total_timesteps      | 694272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048444077 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.946        |\n",
      "|    explained_variance   | 0.0287        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8e+04         |\n",
      "|    n_updates            | 3380          |\n",
      "|    policy_gradient_loss | 1.54e-05      |\n",
      "|    std                  | 0.623         |\n",
      "|    value_loss           | 1.25e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-15248.16 +/- 8651.65\n",
      "Episode length: 345.50 +/- 56.06\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 346           |\n",
      "|    mean_reward          | -1.52e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 696000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011783565 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.945        |\n",
      "|    explained_variance   | 0.0365        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.33e+04      |\n",
      "|    n_updates            | 3390          |\n",
      "|    policy_gradient_loss | -0.000355     |\n",
      "|    std                  | 0.623         |\n",
      "|    value_loss           | 3.79e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 340    |\n",
      "|    time_elapsed    | 1053   |\n",
      "|    total_timesteps | 696320 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 341           |\n",
      "|    time_elapsed         | 1055          |\n",
      "|    total_timesteps      | 698368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9455746e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.946        |\n",
      "|    explained_variance   | 0.0485        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.65e+06      |\n",
      "|    n_updates            | 3400          |\n",
      "|    policy_gradient_loss | -0.000239     |\n",
      "|    std                  | 0.623         |\n",
      "|    value_loss           | 1.41e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-4238.70 +/- 6855.10\n",
      "Episode length: 335.75 +/- 36.26\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 336           |\n",
      "|    mean_reward          | -4.24e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 700000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8857215e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.944        |\n",
      "|    explained_variance   | 0.0431        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+05      |\n",
      "|    n_updates            | 3410          |\n",
      "|    policy_gradient_loss | -0.000195     |\n",
      "|    std                  | 0.621         |\n",
      "|    value_loss           | 6.44e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 342    |\n",
      "|    time_elapsed    | 1059   |\n",
      "|    total_timesteps | 700416 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 343          |\n",
      "|    time_elapsed         | 1061         |\n",
      "|    total_timesteps      | 702464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.966119e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.942       |\n",
      "|    explained_variance   | 0.051        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.13e+04     |\n",
      "|    n_updates            | 3420         |\n",
      "|    policy_gradient_loss | -0.000196    |\n",
      "|    std                  | 0.621        |\n",
      "|    value_loss           | 5.09e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=-8439.54 +/- 7546.47\n",
      "Episode length: 315.88 +/- 28.79\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 316           |\n",
      "|    mean_reward          | -8.44e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 704000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2571152e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.943        |\n",
      "|    explained_variance   | 0.0372        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.08e+05      |\n",
      "|    n_updates            | 3430          |\n",
      "|    policy_gradient_loss | -4.42e-05     |\n",
      "|    std                  | 0.621         |\n",
      "|    value_loss           | 1.08e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 344    |\n",
      "|    time_elapsed    | 1064   |\n",
      "|    total_timesteps | 704512 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 345          |\n",
      "|    time_elapsed         | 1067         |\n",
      "|    total_timesteps      | 706560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012662278 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.942       |\n",
      "|    explained_variance   | 0.0445       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+05     |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    std                  | 0.62         |\n",
      "|    value_loss           | 1.55e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-10457.76 +/- 10870.27\n",
      "Episode length: 325.50 +/- 63.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 326           |\n",
      "|    mean_reward          | -1.05e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 708000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5535192e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.0676        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.31e+05      |\n",
      "|    n_updates            | 3450          |\n",
      "|    policy_gradient_loss | -2.64e-06     |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 4.6e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 346    |\n",
      "|    time_elapsed    | 1070   |\n",
      "|    total_timesteps | 708608 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 662           |\n",
      "|    iterations           | 347           |\n",
      "|    time_elapsed         | 1072          |\n",
      "|    total_timesteps      | 710656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8043305e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.0294        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.13e+05      |\n",
      "|    n_updates            | 3460          |\n",
      "|    policy_gradient_loss | -1.4e-05      |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 1.12e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=-5381.10 +/- 6844.10\n",
      "Episode length: 312.12 +/- 51.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 312           |\n",
      "|    mean_reward          | -5.38e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 712000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015865035 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.0202        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.68e+05      |\n",
      "|    n_updates            | 3470          |\n",
      "|    policy_gradient_loss | -0.000402     |\n",
      "|    std                  | 0.618         |\n",
      "|    value_loss           | 7.92e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 348    |\n",
      "|    time_elapsed    | 1075   |\n",
      "|    total_timesteps | 712704 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 662           |\n",
      "|    iterations           | 349           |\n",
      "|    time_elapsed         | 1078          |\n",
      "|    total_timesteps      | 714752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5353409e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.939        |\n",
      "|    explained_variance   | 0.027         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.01e+05      |\n",
      "|    n_updates            | 3480          |\n",
      "|    policy_gradient_loss | -0.000147     |\n",
      "|    std                  | 0.62          |\n",
      "|    value_loss           | 2.56e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=-14310.25 +/- 14153.50\n",
      "Episode length: 342.62 +/- 29.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | -1.43e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 716000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017376039 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.0224       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.89e+04     |\n",
      "|    n_updates            | 3490         |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.618        |\n",
      "|    value_loss           | 5.61e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 350    |\n",
      "|    time_elapsed    | 1081   |\n",
      "|    total_timesteps | 716800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 351          |\n",
      "|    time_elapsed         | 1083         |\n",
      "|    total_timesteps      | 718848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025489654 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.934       |\n",
      "|    explained_variance   | 0.046        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+05     |\n",
      "|    n_updates            | 3500         |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 5.97e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-7186.84 +/- 8227.53\n",
      "Episode length: 343.62 +/- 39.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -7.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.953965e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | 0.00965      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11e+05     |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.000382    |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 5.07e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 352    |\n",
      "|    time_elapsed    | 1087   |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 353          |\n",
      "|    time_elapsed         | 1089         |\n",
      "|    total_timesteps      | 722944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071901083 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.93        |\n",
      "|    explained_variance   | 0.0391       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.79e+04     |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 1.75e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=-8791.12 +/- 13453.42\n",
      "Episode length: 292.00 +/- 51.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 292          |\n",
      "|    mean_reward          | -8.79e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 724000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020409594 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.0637       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.44e+04     |\n",
      "|    n_updates            | 3530         |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    std                  | 0.613        |\n",
      "|    value_loss           | 1.14e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 354    |\n",
      "|    time_elapsed    | 1092   |\n",
      "|    total_timesteps | 724992 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 663           |\n",
      "|    iterations           | 355           |\n",
      "|    time_elapsed         | 1095          |\n",
      "|    total_timesteps      | 727040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046307108 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.929        |\n",
      "|    explained_variance   | 0.0492        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.63e+05      |\n",
      "|    n_updates            | 3540          |\n",
      "|    policy_gradient_loss | -0.000529     |\n",
      "|    std                  | 0.613         |\n",
      "|    value_loss           | 7.19e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=-9483.32 +/- 5932.76\n",
      "Episode length: 311.62 +/- 41.16\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 312           |\n",
      "|    mean_reward          | -9.48e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 728000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8839563e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.928        |\n",
      "|    explained_variance   | 0.0561        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.14e+05      |\n",
      "|    n_updates            | 3550          |\n",
      "|    policy_gradient_loss | -0.000688     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 2.03e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 356    |\n",
      "|    time_elapsed    | 1098   |\n",
      "|    total_timesteps | 729088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 357          |\n",
      "|    time_elapsed         | 1100         |\n",
      "|    total_timesteps      | 731136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010288721 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.932       |\n",
      "|    explained_variance   | 0.0367       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.23e+05     |\n",
      "|    n_updates            | 3560         |\n",
      "|    policy_gradient_loss | -0.000686    |\n",
      "|    std                  | 0.616        |\n",
      "|    value_loss           | 8.32e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-10312.07 +/- 9232.80\n",
      "Episode length: 316.88 +/- 30.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | -1.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 732000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002050134 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.937       |\n",
      "|    explained_variance   | 0.0196       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.76e+05     |\n",
      "|    n_updates            | 3570         |\n",
      "|    policy_gradient_loss | -8.78e-05    |\n",
      "|    std                  | 0.619        |\n",
      "|    value_loss           | 5.24e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 358    |\n",
      "|    time_elapsed    | 1104   |\n",
      "|    total_timesteps | 733184 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 664           |\n",
      "|    iterations           | 359           |\n",
      "|    time_elapsed         | 1106          |\n",
      "|    total_timesteps      | 735232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013422921 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.94         |\n",
      "|    explained_variance   | 0.0582        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+06      |\n",
      "|    n_updates            | 3580          |\n",
      "|    policy_gradient_loss | -0.000273     |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 1.42e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=-8445.90 +/- 9243.03\n",
      "Episode length: 274.00 +/- 42.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 274           |\n",
      "|    mean_reward          | -8.45e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 736000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039487373 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.94         |\n",
      "|    explained_variance   | 0.0261        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.21e+05      |\n",
      "|    n_updates            | 3590          |\n",
      "|    policy_gradient_loss | -0.000904     |\n",
      "|    std                  | 0.619         |\n",
      "|    value_loss           | 1.13e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 360    |\n",
      "|    time_elapsed    | 1109   |\n",
      "|    total_timesteps | 737280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 361          |\n",
      "|    time_elapsed         | 1111         |\n",
      "|    total_timesteps      | 739328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.051387e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.0412       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.55e+05     |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -7.11e-05    |\n",
      "|    std                  | 0.618        |\n",
      "|    value_loss           | 5.16e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-7366.82 +/- 10576.07\n",
      "Episode length: 328.25 +/- 60.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 328        |\n",
      "|    mean_reward          | -7.37e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00131999 |\n",
      "|    clip_fraction        | 4.88e-05   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | -0.00261   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.25e+05   |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | -0.000654  |\n",
      "|    std                  | 0.616      |\n",
      "|    value_loss           | 1.92e+05   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 362    |\n",
      "|    time_elapsed    | 1115   |\n",
      "|    total_timesteps | 741376 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 665           |\n",
      "|    iterations           | 363           |\n",
      "|    time_elapsed         | 1117          |\n",
      "|    total_timesteps      | 743424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020281017 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.935        |\n",
      "|    explained_variance   | -0.0162       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.24e+04      |\n",
      "|    n_updates            | 3620          |\n",
      "|    policy_gradient_loss | 2.44e-05      |\n",
      "|    std                  | 0.616         |\n",
      "|    value_loss           | 2.6e+04       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=-9110.13 +/- 8522.36\n",
      "Episode length: 315.50 +/- 50.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | -9.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 744000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004965195 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | 0.0601       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.74e+05     |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.000789    |\n",
      "|    std                  | 0.614        |\n",
      "|    value_loss           | 2.2e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 364    |\n",
      "|    time_elapsed    | 1121   |\n",
      "|    total_timesteps | 745472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 665          |\n",
      "|    iterations           | 365          |\n",
      "|    time_elapsed         | 1123         |\n",
      "|    total_timesteps      | 747520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.526239e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | 0.0345       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+05     |\n",
      "|    n_updates            | 3640         |\n",
      "|    policy_gradient_loss | -0.00014     |\n",
      "|    std                  | 0.611        |\n",
      "|    value_loss           | 2.15e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=-13529.44 +/- 11219.13\n",
      "Episode length: 325.38 +/- 54.38\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 325           |\n",
      "|    mean_reward          | -1.35e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 748000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6162473e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.927        |\n",
      "|    explained_variance   | 0.0486        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.48e+05      |\n",
      "|    n_updates            | 3650          |\n",
      "|    policy_gradient_loss | -0.000166     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 1.94e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 366    |\n",
      "|    time_elapsed    | 1126   |\n",
      "|    total_timesteps | 749568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 665          |\n",
      "|    iterations           | 367          |\n",
      "|    time_elapsed         | 1129         |\n",
      "|    total_timesteps      | 751616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032826755 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.0843       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.000932    |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 2.69e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=-2437.01 +/- 1795.12\n",
      "Episode length: 302.50 +/- 34.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 302          |\n",
      "|    mean_reward          | -2.44e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 752000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001927518 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.93        |\n",
      "|    explained_variance   | 0.0904       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.48e+05     |\n",
      "|    n_updates            | 3670         |\n",
      "|    policy_gradient_loss | -0.000592    |\n",
      "|    std                  | 0.614        |\n",
      "|    value_loss           | 7.6e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 368    |\n",
      "|    time_elapsed    | 1132   |\n",
      "|    total_timesteps | 753664 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 369           |\n",
      "|    time_elapsed         | 1134          |\n",
      "|    total_timesteps      | 755712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023184823 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.931        |\n",
      "|    explained_variance   | 0.0751        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7e+05         |\n",
      "|    n_updates            | 3680          |\n",
      "|    policy_gradient_loss | -0.000219     |\n",
      "|    std                  | 0.614         |\n",
      "|    value_loss           | 1.35e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-5834.98 +/- 5462.78\n",
      "Episode length: 320.50 +/- 37.50\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 320           |\n",
      "|    mean_reward          | -5.83e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 756000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5065849e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.93         |\n",
      "|    explained_variance   | 0.0443        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.58e+05      |\n",
      "|    n_updates            | 3690          |\n",
      "|    policy_gradient_loss | -7.55e-05     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 1.73e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 370    |\n",
      "|    time_elapsed    | 1138   |\n",
      "|    total_timesteps | 757760 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 1140        |\n",
      "|    total_timesteps      | 759808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005339535 |\n",
      "|    clip_fraction        | 0.01        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.0378      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.84e+04    |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 5.65e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-11887.60 +/- 8612.98\n",
      "Episode length: 306.75 +/- 44.27\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 307           |\n",
      "|    mean_reward          | -1.19e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 760000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066271884 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.929        |\n",
      "|    explained_variance   | 0.0312        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+04      |\n",
      "|    n_updates            | 3710          |\n",
      "|    policy_gradient_loss | -0.00036      |\n",
      "|    std                  | 0.613         |\n",
      "|    value_loss           | 5.17e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 372    |\n",
      "|    time_elapsed    | 1143   |\n",
      "|    total_timesteps | 761856 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 373           |\n",
      "|    time_elapsed         | 1146          |\n",
      "|    total_timesteps      | 763904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044695704 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.931        |\n",
      "|    explained_variance   | 0.0492        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.15e+04      |\n",
      "|    n_updates            | 3720          |\n",
      "|    policy_gradient_loss | -4.09e-05     |\n",
      "|    std                  | 0.614         |\n",
      "|    value_loss           | 7.2e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=-11951.21 +/- 11525.33\n",
      "Episode length: 332.38 +/- 47.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | -1.2e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 764000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000488832 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.0485      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.48e+03    |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.000294   |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 8.36e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 374    |\n",
      "|    time_elapsed    | 1149   |\n",
      "|    total_timesteps | 765952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-4346.11 +/- 5606.42\n",
      "Episode length: 294.88 +/- 34.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | -4.35e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 768000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010336225 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.923       |\n",
      "|    explained_variance   | 0.048        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.58e+04     |\n",
      "|    n_updates            | 3740         |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    std                  | 0.607        |\n",
      "|    value_loss           | 1.63e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 375    |\n",
      "|    time_elapsed    | 1152   |\n",
      "|    total_timesteps | 768000 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 376           |\n",
      "|    time_elapsed         | 1155          |\n",
      "|    total_timesteps      | 770048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011841064 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.92         |\n",
      "|    explained_variance   | 0.0594        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.01e+05      |\n",
      "|    n_updates            | 3750          |\n",
      "|    policy_gradient_loss | -0.000369     |\n",
      "|    std                  | 0.607         |\n",
      "|    value_loss           | 1.07e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=-11646.70 +/- 11022.79\n",
      "Episode length: 313.75 +/- 39.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 314           |\n",
      "|    mean_reward          | -1.16e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 772000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8558421e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.921        |\n",
      "|    explained_variance   | 0.089         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.17e+05      |\n",
      "|    n_updates            | 3760          |\n",
      "|    policy_gradient_loss | -0.000214     |\n",
      "|    std                  | 0.609         |\n",
      "|    value_loss           | 1.24e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 377    |\n",
      "|    time_elapsed    | 1158   |\n",
      "|    total_timesteps | 772096 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 1161        |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004055001 |\n",
      "|    clip_fraction        | 0.00552     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | -0.0121     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.76e+04    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 7.1e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=-6229.65 +/- 8128.46\n",
      "Episode length: 318.62 +/- 64.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -6.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 776000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008413332 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | 0.0532       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+06     |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.000925    |\n",
      "|    std                  | 0.606        |\n",
      "|    value_loss           | 3.42e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 379    |\n",
      "|    time_elapsed    | 1164   |\n",
      "|    total_timesteps | 776192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 380          |\n",
      "|    time_elapsed         | 1167         |\n",
      "|    total_timesteps      | 778240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022699977 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.0518       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.96e+05     |\n",
      "|    n_updates            | 3790         |\n",
      "|    policy_gradient_loss | -0.000746    |\n",
      "|    std                  | 0.608        |\n",
      "|    value_loss           | 8.59e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-6886.89 +/- 7744.18\n",
      "Episode length: 312.50 +/- 47.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 312          |\n",
      "|    mean_reward          | -6.89e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 780000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041607018 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.921       |\n",
      "|    explained_variance   | -0.189       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.19e+04     |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.607        |\n",
      "|    value_loss           | 4.76e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 381    |\n",
      "|    time_elapsed    | 1170   |\n",
      "|    total_timesteps | 780288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 1172        |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005365661 |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.0966      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97e+04    |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 1.27e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=-10173.49 +/- 11441.13\n",
      "Episode length: 313.88 +/- 30.75\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 314           |\n",
      "|    mean_reward          | -1.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 784000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023582758 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.924        |\n",
      "|    explained_variance   | 0.0731        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.37e+05      |\n",
      "|    n_updates            | 3820          |\n",
      "|    policy_gradient_loss | -0.000177     |\n",
      "|    std                  | 0.608         |\n",
      "|    value_loss           | 3.64e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 383    |\n",
      "|    time_elapsed    | 1175   |\n",
      "|    total_timesteps | 784384 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 384          |\n",
      "|    time_elapsed         | 1178         |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016898849 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.0856       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47e+05     |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -0.000897    |\n",
      "|    std                  | 0.607        |\n",
      "|    value_loss           | 9.49e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=-7244.10 +/- 9967.73\n",
      "Episode length: 307.50 +/- 37.90\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 308           |\n",
      "|    mean_reward          | -7.24e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 788000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032482797 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.921        |\n",
      "|    explained_variance   | 0.0539        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.18e+04      |\n",
      "|    n_updates            | 3840          |\n",
      "|    policy_gradient_loss | -0.000397     |\n",
      "|    std                  | 0.609         |\n",
      "|    value_loss           | 7.01e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 385    |\n",
      "|    time_elapsed    | 1181   |\n",
      "|    total_timesteps | 788480 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 667           |\n",
      "|    iterations           | 386           |\n",
      "|    time_elapsed         | 1183          |\n",
      "|    total_timesteps      | 790528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074512826 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.922        |\n",
      "|    explained_variance   | 0.0702        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.25e+06      |\n",
      "|    n_updates            | 3850          |\n",
      "|    policy_gradient_loss | -0.000405     |\n",
      "|    std                  | 0.609         |\n",
      "|    value_loss           | 2.34e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=-7336.50 +/- 8672.79\n",
      "Episode length: 363.25 +/- 41.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 363          |\n",
      "|    mean_reward          | -7.34e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 792000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014969034 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.924       |\n",
      "|    explained_variance   | 0.0566       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.59e+05     |\n",
      "|    n_updates            | 3860         |\n",
      "|    policy_gradient_loss | -0.00077     |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 4.94e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 387    |\n",
      "|    time_elapsed    | 1187   |\n",
      "|    total_timesteps | 792576 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 667           |\n",
      "|    iterations           | 388           |\n",
      "|    time_elapsed         | 1189          |\n",
      "|    total_timesteps      | 794624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012797232 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.926        |\n",
      "|    explained_variance   | 0.0578        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.72e+05      |\n",
      "|    n_updates            | 3870          |\n",
      "|    policy_gradient_loss | -0.000405     |\n",
      "|    std                  | 0.612         |\n",
      "|    value_loss           | 7.46e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=-7382.65 +/- 8404.15\n",
      "Episode length: 324.00 +/- 52.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | -7.38e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 796000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003685697 |\n",
      "|    clip_fraction        | 0.00303     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.0598      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.84e+05    |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 7.84e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 667    |\n",
      "|    iterations      | 389    |\n",
      "|    time_elapsed    | 1192   |\n",
      "|    total_timesteps | 796672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 390          |\n",
      "|    time_elapsed         | 1195         |\n",
      "|    total_timesteps      | 798720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025102235 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.0494       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.27e+04     |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    std                  | 0.611        |\n",
      "|    value_loss           | 7.92e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-5262.91 +/- 6744.25\n",
      "Episode length: 364.50 +/- 45.22\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 364           |\n",
      "|    mean_reward          | -5.26e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 800000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014301453 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.928        |\n",
      "|    explained_variance   | 0.0604        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+05      |\n",
      "|    n_updates            | 3900          |\n",
      "|    policy_gradient_loss | -0.000172     |\n",
      "|    std                  | 0.614         |\n",
      "|    value_loss           | 4.74e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 668    |\n",
      "|    iterations      | 391    |\n",
      "|    time_elapsed    | 1198   |\n",
      "|    total_timesteps | 800768 |\n",
      "-------------------------------\n",
      "Before training: mean_reward:-7803.66 +/- 8093.65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#vec_env = VecNormalize(env, norm_obs=True, norm_reward=True,clip_obs=1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=800000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Before training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b31937afa3a071",
   "metadata": {},
   "source": [
    "### 再训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdc253940e03362",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard/ppo_run_1713689561.763529_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 352       |\n",
      "|    ep_rew_mean     | -2.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 858       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-1569.59 +/- 155.36\n",
      "Episode length: 316.62 +/- 26.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | -1.57e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058822203 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.78        |\n",
      "|    explained_variance   | 0.0603       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1e+05        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00406     |\n",
      "|    std                  | 0.529        |\n",
      "|    value_loss           | 6.77e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 361       |\n",
      "|    ep_rew_mean     | -2.16e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 524       |\n",
      "|    iterations      | 2         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 4096      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 368          |\n",
      "|    ep_rew_mean          | -2.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063983416 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.783       |\n",
      "|    explained_variance   | -0.0104      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.39e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    std                  | 0.526        |\n",
      "|    value_loss           | 2.1e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-2452.67 +/- 2379.23\n",
      "Episode length: 327.25 +/- 45.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 327          |\n",
      "|    mean_reward          | -2.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063312966 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.773       |\n",
      "|    explained_variance   | -0.0402      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.64e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000994    |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | -1.6e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 492      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 397          |\n",
      "|    ep_rew_mean          | -2.25e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 508          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028919028 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.762       |\n",
      "|    explained_variance   | 0.0419       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.65e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00055     |\n",
      "|    std                  | 0.517        |\n",
      "|    value_loss           | 1.36e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-3188.37 +/- 2634.17\n",
      "Episode length: 326.12 +/- 42.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | -3.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026175128 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.76        |\n",
      "|    explained_variance   | 0.0451       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.17e+04     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.518        |\n",
      "|    value_loss           | 2.35e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 346       |\n",
      "|    ep_rew_mean     | -1.96e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 482       |\n",
      "|    iterations      | 6         |\n",
      "|    time_elapsed    | 25        |\n",
      "|    total_timesteps | 12288     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 411        |\n",
      "|    ep_rew_mean          | -2.43e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00311386 |\n",
      "|    clip_fraction        | 0.0195     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.758     |\n",
      "|    explained_variance   | 0.0323     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.55e+03   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00161   |\n",
      "|    std                  | 0.515      |\n",
      "|    value_loss           | 1.37e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-4534.24 +/- 4297.53\n",
      "Episode length: 328.62 +/- 47.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | -4.53e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005017961 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.0837      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.91e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 9.62e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 524       |\n",
      "|    ep_rew_mean     | -2.44e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 477       |\n",
      "|    iterations      | 8         |\n",
      "|    time_elapsed    | 34        |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 406           |\n",
      "|    ep_rew_mean          | -2.69e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 486           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012775813 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.753        |\n",
      "|    explained_variance   | 0.0955        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.4e+06       |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -8.15e-05     |\n",
      "|    std                  | 0.514         |\n",
      "|    value_loss           | 2.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-4994.20 +/- 9029.20\n",
      "Episode length: 307.38 +/- 29.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -4.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008600367 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.753       |\n",
      "|    explained_variance   | 0.0535       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.69e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    std                  | 0.513        |\n",
      "|    value_loss           | 1.32e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 420       |\n",
      "|    ep_rew_mean     | -4.22e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 474       |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 20480     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 403          |\n",
      "|    ep_rew_mean          | -1.25e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026726848 |\n",
      "|    clip_fraction        | 0.00166      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.75        |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 2.14e-05     |\n",
      "|    std                  | 0.512        |\n",
      "|    value_loss           | 2.44e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-6483.63 +/- 5428.66\n",
      "Episode length: 311.00 +/- 33.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | -6.48e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013369606 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.0831       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.72e+04     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    std                  | 0.512        |\n",
      "|    value_loss           | 8.83e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 479       |\n",
      "|    ep_rew_mean     | -2.77e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 12        |\n",
      "|    time_elapsed    | 51        |\n",
      "|    total_timesteps | 24576     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 392          |\n",
      "|    ep_rew_mean          | -2.22e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014479053 |\n",
      "|    clip_fraction        | 0.00337      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.752       |\n",
      "|    explained_variance   | 0.0649       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    std                  | 0.515        |\n",
      "|    value_loss           | 8.88e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-4733.43 +/- 5620.84\n",
      "Episode length: 304.00 +/- 40.87\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 304            |\n",
      "|    mean_reward          | -4.73e+03      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 28000          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000119711054 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.755         |\n",
      "|    explained_variance   | 0.0845         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.19e+06       |\n",
      "|    n_updates            | 130            |\n",
      "|    policy_gradient_loss | 4.11e-05       |\n",
      "|    std                  | 0.514          |\n",
      "|    value_loss           | 1.76e+06       |\n",
      "--------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 322       |\n",
      "|    ep_rew_mean     | -1.31e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 14        |\n",
      "|    time_elapsed    | 60        |\n",
      "|    total_timesteps | 28672     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 292         |\n",
      "|    ep_rew_mean          | -1.68e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002850104 |\n",
      "|    clip_fraction        | 0.00977     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.0565      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.92e+06    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000516   |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 2.25e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-6638.54 +/- 9644.13\n",
      "Episode length: 339.62 +/- 49.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | -6.64e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003285362 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.75       |\n",
      "|    explained_variance   | 0.0552      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.77e+05    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    std                  | 0.513       |\n",
      "|    value_loss           | 5.24e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 635       |\n",
      "|    ep_rew_mean     | -3.72e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 473       |\n",
      "|    iterations      | 16        |\n",
      "|    time_elapsed    | 69        |\n",
      "|    total_timesteps | 32768     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 353          |\n",
      "|    ep_rew_mean          | -9.34e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031896797 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.57e+03     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000455    |\n",
      "|    std                  | 0.51         |\n",
      "|    value_loss           | 7.13e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-4783.72 +/- 8316.22\n",
      "Episode length: 324.25 +/- 52.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | -4.78e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004111281 |\n",
      "|    clip_fraction        | 0.0173      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.748      |\n",
      "|    explained_variance   | 0.0789      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.6e+04     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.000195   |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 5.49e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 724       |\n",
      "|    ep_rew_mean     | -4.68e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 471       |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 78        |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 364          |\n",
      "|    ep_rew_mean          | -2.12e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021906798 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+04     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 3.01e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-6056.64 +/- 4535.46\n",
      "Episode length: 346.75 +/- 29.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | -6.06e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002600375 |\n",
      "|    clip_fraction        | 0.00444     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.68e+05    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.506       |\n",
      "|    value_loss           | 8.03e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 233       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 469       |\n",
      "|    iterations      | 20        |\n",
      "|    time_elapsed    | 87        |\n",
      "|    total_timesteps | 40960     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 382        |\n",
      "|    ep_rew_mean          | -2.58e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00662597 |\n",
      "|    clip_fraction        | 0.0614     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.74      |\n",
      "|    explained_variance   | 0.0919     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.93e+05   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00276   |\n",
      "|    std                  | 0.508      |\n",
      "|    value_loss           | 4.56e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-5072.15 +/- 6037.74\n",
      "Episode length: 318.62 +/- 52.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | -5.07e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014548573 |\n",
      "|    clip_fraction        | 0.00542      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.741       |\n",
      "|    explained_variance   | 0.0816       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52e+06     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 1.38e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 302       |\n",
      "|    ep_rew_mean     | -7.62e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 468       |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 96        |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 342          |\n",
      "|    ep_rew_mean          | -1.96e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023457012 |\n",
      "|    clip_fraction        | 0.00903      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.741       |\n",
      "|    explained_variance   | 0.0752       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 0.000231     |\n",
      "|    std                  | 0.507        |\n",
      "|    value_loss           | 6.98e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-4011.85 +/- 4455.01\n",
      "Episode length: 323.75 +/- 36.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 324           |\n",
      "|    mean_reward          | -4.01e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 48000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015237654 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.741        |\n",
      "|    explained_variance   | 0.0937        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.73e+05      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000385     |\n",
      "|    std                  | 0.509         |\n",
      "|    value_loss           | 1.72e+06      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 466       |\n",
      "|    ep_rew_mean     | -2.95e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 466       |\n",
      "|    iterations      | 24        |\n",
      "|    time_elapsed    | 105       |\n",
      "|    total_timesteps | 49152     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 452          |\n",
      "|    ep_rew_mean          | -2.73e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015266044 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.747       |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 4.15e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-5991.44 +/- 8010.79\n",
      "Episode length: 335.38 +/- 50.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -5.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021688156 |\n",
      "|    clip_fraction        | 0.00693      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.42e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000893    |\n",
      "|    std                  | 0.511        |\n",
      "|    value_loss           | 5.36e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 343       |\n",
      "|    ep_rew_mean     | -1.74e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 465       |\n",
      "|    iterations      | 26        |\n",
      "|    time_elapsed    | 114       |\n",
      "|    total_timesteps | 53248     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 331          |\n",
      "|    ep_rew_mean          | -1.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021966058 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.743       |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81e+05     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    std                  | 0.506        |\n",
      "|    value_loss           | 6.86e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-5078.20 +/- 8298.60\n",
      "Episode length: 329.75 +/- 45.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 330          |\n",
      "|    mean_reward          | -5.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015028131 |\n",
      "|    clip_fraction        | 0.0041       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.737       |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+05     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.505        |\n",
      "|    value_loss           | 3.04e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 223       |\n",
      "|    ep_rew_mean     | -1.27e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 464       |\n",
      "|    iterations      | 28        |\n",
      "|    time_elapsed    | 123       |\n",
      "|    total_timesteps | 57344     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 401          |\n",
      "|    ep_rew_mean          | -1.1e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 467          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018775349 |\n",
      "|    clip_fraction        | 0.00972      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.722       |\n",
      "|    explained_variance   | -0.0303      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57e+04     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    std                  | 0.489        |\n",
      "|    value_loss           | 2.05e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-1811.23 +/- 445.14\n",
      "Episode length: 325.25 +/- 33.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 325           |\n",
      "|    mean_reward          | -1.81e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020371772 |\n",
      "|    clip_fraction        | 0.00239       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.701        |\n",
      "|    explained_variance   | 0.0989        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.56e+05      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000872     |\n",
      "|    std                  | 0.487         |\n",
      "|    value_loss           | 1.13e+06      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 385       |\n",
      "|    ep_rew_mean     | -2.25e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 464       |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 132       |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 312           |\n",
      "|    ep_rew_mean          | -1.81e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 467           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 135           |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094628107 |\n",
      "|    clip_fraction        | 0.00488       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.703        |\n",
      "|    explained_variance   | 0.1           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.68e+05      |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000677     |\n",
      "|    std                  | 0.49          |\n",
      "|    value_loss           | 9.36e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-3390.58 +/- 3454.42\n",
      "Episode length: 345.38 +/- 43.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | -3.39e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037301802 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | -0.0557      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.16e+03     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 0.000359     |\n",
      "|    std                  | 0.489        |\n",
      "|    value_loss           | 2.03e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 370       |\n",
      "|    ep_rew_mean     | -2.14e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 141       |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 387          |\n",
      "|    ep_rew_mean          | -3.01e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007198333 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | 0.111        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+06     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000788    |\n",
      "|    std                  | 0.49         |\n",
      "|    value_loss           | 1.86e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-2206.49 +/- 1420.76\n",
      "Episode length: 320.00 +/- 31.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 320         |\n",
      "|    mean_reward          | -2.21e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002094997 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.69e+04    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 5.15e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 387       |\n",
      "|    ep_rew_mean     | -2.35e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 34        |\n",
      "|    time_elapsed    | 150       |\n",
      "|    total_timesteps | 69632     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 260         |\n",
      "|    ep_rew_mean          | -1.47e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004254721 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.58e+05    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 8.91e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-3539.73 +/- 3615.82\n",
      "Episode length: 335.62 +/- 54.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 336         |\n",
      "|    mean_reward          | -3.54e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000949232 |\n",
      "|    clip_fraction        | 0.00454     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.712      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.54e+05    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 8.1e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 373       |\n",
      "|    ep_rew_mean     | -6.48e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 36        |\n",
      "|    time_elapsed    | 159       |\n",
      "|    total_timesteps | 73728     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 339          |\n",
      "|    ep_rew_mean          | -2.05e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009412368 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.713       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.36e+05     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000307    |\n",
      "|    std                  | 0.494        |\n",
      "|    value_loss           | 2.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-4980.19 +/- 5270.31\n",
      "Episode length: 350.62 +/- 39.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | -4.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057352306 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.716       |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+06     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 1.31e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 357       |\n",
      "|    ep_rew_mean     | -2.11e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 38        |\n",
      "|    time_elapsed    | 168       |\n",
      "|    total_timesteps | 77824     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 415           |\n",
      "|    ep_rew_mean          | -2.62e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 465           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 171           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00082522136 |\n",
      "|    clip_fraction        | 0.0229        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.723        |\n",
      "|    explained_variance   | -0.104        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+04       |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | 0.000619      |\n",
      "|    std                  | 0.504         |\n",
      "|    value_loss           | 1.47e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-4732.66 +/- 4393.47\n",
      "Episode length: 307.38 +/- 45.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -4.73e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017169648 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.733       |\n",
      "|    explained_variance   | -0.0345      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+04     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | 0.000135     |\n",
      "|    std                  | 0.503        |\n",
      "|    value_loss           | 1.76e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 323       |\n",
      "|    ep_rew_mean     | -1.91e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 462       |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 176       |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 384          |\n",
      "|    ep_rew_mean          | -2.34e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021565063 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.737       |\n",
      "|    explained_variance   | -0.000626    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.76e+03     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.506        |\n",
      "|    value_loss           | 1.17e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-10185.04 +/- 8363.27\n",
      "Episode length: 328.12 +/- 24.97\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 328           |\n",
      "|    mean_reward          | -1.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 84000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033728796 |\n",
      "|    clip_fraction        | 0.00269       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.739        |\n",
      "|    explained_variance   | 0.115         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.84e+04      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000907     |\n",
      "|    std                  | 0.507         |\n",
      "|    value_loss           | 3.54e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 331       |\n",
      "|    ep_rew_mean     | -2.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 462       |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 185       |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-3078.67 +/- 3788.96\n",
      "Episode length: 321.88 +/- 40.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -3.08e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013052234 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.74        |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+06     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 3.06e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 303       |\n",
      "|    ep_rew_mean     | -1.89e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 43        |\n",
      "|    time_elapsed    | 191       |\n",
      "|    total_timesteps | 88064     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 432          |\n",
      "|    ep_rew_mean          | -6.6e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019128479 |\n",
      "|    clip_fraction        | 0.0085       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.74        |\n",
      "|    explained_variance   | -0.269       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.24e+03     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.000115     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 1.28e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-2644.02 +/- 2602.99\n",
      "Episode length: 334.38 +/- 38.07\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 334           |\n",
      "|    mean_reward          | -2.64e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 92000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016598165 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.741        |\n",
      "|    explained_variance   | 0.0759        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.81e+05      |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | 0.000104      |\n",
      "|    std                  | 0.508         |\n",
      "|    value_loss           | 1.1e+06       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 310       |\n",
      "|    ep_rew_mean     | -1.84e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 45        |\n",
      "|    time_elapsed    | 200       |\n",
      "|    total_timesteps | 92160     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 494        |\n",
      "|    ep_rew_mean          | -2.63e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 203        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00532081 |\n",
      "|    clip_fraction        | 0.0347     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.741     |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.29e+04   |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00418   |\n",
      "|    std                  | 0.508      |\n",
      "|    value_loss           | 1.32e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-2748.72 +/- 2913.96\n",
      "Episode length: 317.00 +/- 44.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 317         |\n",
      "|    mean_reward          | -2.75e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000838241 |\n",
      "|    clip_fraction        | 0.00151     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.742      |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.97e+05    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.000539   |\n",
      "|    std                  | 0.508       |\n",
      "|    value_loss           | 1.79e+06    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 259       |\n",
      "|    ep_rew_mean     | -1.55e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 209       |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 348          |\n",
      "|    ep_rew_mean          | -1.39e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014608537 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.739       |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 0.505        |\n",
      "|    value_loss           | 1.21e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-1529.32 +/- 243.81\n",
      "Episode length: 296.75 +/- 42.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 297          |\n",
      "|    mean_reward          | -1.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026470548 |\n",
      "|    clip_fraction        | 0.00522      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.732       |\n",
      "|    explained_variance   | 0.18         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.23e+04     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000927    |\n",
      "|    std                  | 0.501        |\n",
      "|    value_loss           | 3.42e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 367       |\n",
      "|    ep_rew_mean     | -5.79e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 218       |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 417          |\n",
      "|    ep_rew_mean          | -2.58e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 221          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012000814 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.161        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+04     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    std                  | 0.499        |\n",
      "|    value_loss           | 4.06e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-3982.99 +/- 4602.49\n",
      "Episode length: 331.00 +/- 42.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | -3.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032454222 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.723       |\n",
      "|    explained_variance   | -0.0159      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.03e+03     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000908    |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 2.19e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 519       |\n",
      "|    ep_rew_mean     | -3.11e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 51        |\n",
      "|    time_elapsed    | 226       |\n",
      "|    total_timesteps | 104448    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 378          |\n",
      "|    ep_rew_mean          | -3.92e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 230          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003424179 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.718       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+05     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -6.14e-06    |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 1.06e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-4345.40 +/- 5266.47\n",
      "Episode length: 321.50 +/- 48.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | -4.35e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010046931 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.09e+05     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 9.42e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 555       |\n",
      "|    ep_rew_mean     | -2.13e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 53        |\n",
      "|    time_elapsed    | 236       |\n",
      "|    total_timesteps | 108544    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 302          |\n",
      "|    ep_rew_mean          | -1.8e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 239          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036690203 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+04     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 2.05e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-2110.87 +/- 824.01\n",
      "Episode length: 343.25 +/- 54.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | -2.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016387075 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.27e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00065     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 4.28e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 429       |\n",
      "|    ep_rew_mean     | -2.03e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 55        |\n",
      "|    time_elapsed    | 244       |\n",
      "|    total_timesteps | 112640    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 279          |\n",
      "|    ep_rew_mean          | -1.7e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004078106 |\n",
      "|    clip_fraction        | 0.00269      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.723       |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000656    |\n",
      "|    std                  | 0.499        |\n",
      "|    value_loss           | 6.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-3452.41 +/- 2400.15\n",
      "Episode length: 339.50 +/- 34.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 340          |\n",
      "|    mean_reward          | -3.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065763537 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.33e+05     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 7.35e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 452       |\n",
      "|    ep_rew_mean     | -2.94e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 57        |\n",
      "|    time_elapsed    | 253       |\n",
      "|    total_timesteps | 116736    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 371          |\n",
      "|    ep_rew_mean          | -3.98e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058004893 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.704       |\n",
      "|    explained_variance   | -0.0608      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+03     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000732    |\n",
      "|    std                  | 0.482        |\n",
      "|    value_loss           | 1.58e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-3384.25 +/- 4677.14\n",
      "Episode length: 298.88 +/- 44.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | -3.38e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062474683 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.37e+03     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    std                  | 0.478        |\n",
      "|    value_loss           | 2.52e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 453       |\n",
      "|    ep_rew_mean     | -2.92e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 59        |\n",
      "|    time_elapsed    | 262       |\n",
      "|    total_timesteps | 120832    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 391          |\n",
      "|    ep_rew_mean          | -2.44e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 266          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019836836 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.677       |\n",
      "|    explained_variance   | -0.033       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+04     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000193    |\n",
      "|    std                  | 0.474        |\n",
      "|    value_loss           | 1.29e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-3181.15 +/- 3541.85\n",
      "Episode length: 349.25 +/- 42.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | -3.18e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068063186 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.671       |\n",
      "|    explained_variance   | -0.00436     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.8e+03      |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000485    |\n",
      "|    std                  | 0.473        |\n",
      "|    value_loss           | 1.3e+04      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 471       |\n",
      "|    ep_rew_mean     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 61        |\n",
      "|    time_elapsed    | 271       |\n",
      "|    total_timesteps | 124928    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 417           |\n",
      "|    ep_rew_mean          | -2.55e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 461           |\n",
      "|    iterations           | 62            |\n",
      "|    time_elapsed         | 275           |\n",
      "|    total_timesteps      | 126976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0648727e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.67         |\n",
      "|    explained_variance   | 0.0882        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.65e+05      |\n",
      "|    n_updates            | 610           |\n",
      "|    policy_gradient_loss | 0.000562      |\n",
      "|    std                  | 0.473         |\n",
      "|    value_loss           | 1.67e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-3213.53 +/- 3929.27\n",
      "Episode length: 319.50 +/- 39.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -3.21e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025742967 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.49e+05     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.000959    |\n",
      "|    std                  | 0.471        |\n",
      "|    value_loss           | 6.54e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 414       |\n",
      "|    ep_rew_mean     | -2.59e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 460       |\n",
      "|    iterations      | 63        |\n",
      "|    time_elapsed    | 280       |\n",
      "|    total_timesteps | 129024    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 280          |\n",
      "|    ep_rew_mean          | -1.63e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 283          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003508969 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.665       |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.28e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | 0.000251     |\n",
      "|    std                  | 0.47         |\n",
      "|    value_loss           | 4.43e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-1659.15 +/- 377.03\n",
      "Episode length: 306.62 +/- 62.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | -1.66e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031317542 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.662       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+04     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.468        |\n",
      "|    value_loss           | 1.42e+06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 409      |\n",
      "|    ep_rew_mean     | -4e+03   |\n",
      "| time/              |          |\n",
      "|    fps             | 460      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 239          |\n",
      "|    ep_rew_mean          | -1.38e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 292          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065083606 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.31e+05     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | 0.000413     |\n",
      "|    std                  | 0.466        |\n",
      "|    value_loss           | 5.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-4984.59 +/- 5326.30\n",
      "Episode length: 367.88 +/- 26.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 368          |\n",
      "|    mean_reward          | -4.98e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013248659 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.98e+05     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    std                  | 0.469        |\n",
      "|    value_loss           | 1.11e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 346       |\n",
      "|    ep_rew_mean     | -2.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 67        |\n",
      "|    time_elapsed    | 298       |\n",
      "|    total_timesteps | 137216    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 327          |\n",
      "|    ep_rew_mean          | -2.03e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 302          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020134463 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.657       |\n",
      "|    explained_variance   | -0.0203      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.86e+04     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.000152     |\n",
      "|    std                  | 0.462        |\n",
      "|    value_loss           | 1.82e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-2305.89 +/- 2014.16\n",
      "Episode length: 302.75 +/- 51.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 303         |\n",
      "|    mean_reward          | -2.31e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006426132 |\n",
      "|    clip_fraction        | 0.0581      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.22e+03    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    std                  | 0.462       |\n",
      "|    value_loss           | 8.69e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 440       |\n",
      "|    ep_rew_mean     | -2.73e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 69        |\n",
      "|    time_elapsed    | 307       |\n",
      "|    total_timesteps | 141312    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 406          |\n",
      "|    ep_rew_mean          | -2.17e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 311          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018653625 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.31e+05     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.0007      |\n",
      "|    std                  | 0.461        |\n",
      "|    value_loss           | 1.5e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-2635.58 +/- 1510.05\n",
      "Episode length: 345.50 +/- 57.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | -2.64e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006501942 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.28e+06    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.46        |\n",
      "|    value_loss           | 1.67e+06    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 369       |\n",
      "|    ep_rew_mean     | -2.48e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 71        |\n",
      "|    time_elapsed    | 316       |\n",
      "|    total_timesteps | 145408    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 343          |\n",
      "|    ep_rew_mean          | -2.06e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 320          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030993344 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.644       |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+04     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    std                  | 0.46         |\n",
      "|    value_loss           | 5.77e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-2654.33 +/- 1653.66\n",
      "Episode length: 351.75 +/- 36.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | -2.65e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 148000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003922368 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | -0.0563     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.73e+03    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.000808   |\n",
      "|    std                  | 0.452       |\n",
      "|    value_loss           | 2.05e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 377       |\n",
      "|    ep_rew_mean     | -2.49e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 73        |\n",
      "|    time_elapsed    | 325       |\n",
      "|    total_timesteps | 149504    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 388         |\n",
      "|    ep_rew_mean          | -2.39e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007848597 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.42e+05    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    std                  | 0.451       |\n",
      "|    value_loss           | 2.81e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-3270.78 +/- 3478.32\n",
      "Episode length: 349.75 +/- 24.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 350         |\n",
      "|    mean_reward          | -3.27e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008639485 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.01e+04    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 0.452       |\n",
      "|    value_loss           | 1.06e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 398       |\n",
      "|    ep_rew_mean     | -6.89e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 459       |\n",
      "|    iterations      | 75        |\n",
      "|    time_elapsed    | 334       |\n",
      "|    total_timesteps | 153600    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | -5.22e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007765404 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+06     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 2.42e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-2616.08 +/- 2162.93\n",
      "Episode length: 319.50 +/- 41.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | -2.62e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037877806 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03e+06     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    std                  | 0.451        |\n",
      "|    value_loss           | 1.13e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 372       |\n",
      "|    ep_rew_mean     | -2.34e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 463       |\n",
      "|    iterations      | 77        |\n",
      "|    time_elapsed    | 339       |\n",
      "|    total_timesteps | 157696    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 465          |\n",
      "|    ep_rew_mean          | -2.94e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 342          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029011655 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.45         |\n",
      "|    value_loss           | 7.37e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-1900.30 +/- 298.08\n",
      "Episode length: 341.75 +/- 50.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | -1.9e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018599881 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | 0.000144     |\n",
      "|    std                  | 0.453        |\n",
      "|    value_loss           | 4.27e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 398       |\n",
      "|    ep_rew_mean     | -2.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 467       |\n",
      "|    iterations      | 79        |\n",
      "|    time_elapsed    | 345       |\n",
      "|    total_timesteps | 161792    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 385          |\n",
      "|    ep_rew_mean          | -2.35e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055852616 |\n",
      "|    clip_fraction        | 0.0505       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.621       |\n",
      "|    explained_variance   | 0.24         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.31e+04     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    std                  | 0.447        |\n",
      "|    value_loss           | 1.22e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-5514.28 +/- 7146.12\n",
      "Episode length: 369.38 +/- 24.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 369          |\n",
      "|    mean_reward          | -5.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030756707 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.619       |\n",
      "|    explained_variance   | 0.0508       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.87e+04     |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.451        |\n",
      "|    value_loss           | 2.77e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 375       |\n",
      "|    ep_rew_mean     | -2.34e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 471       |\n",
      "|    iterations      | 81        |\n",
      "|    time_elapsed    | 351       |\n",
      "|    total_timesteps | 165888    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 438          |\n",
      "|    ep_rew_mean          | -2.73e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019392079 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | 0.0475       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+04     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.000763    |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 7.7e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-2453.65 +/- 1944.10\n",
      "Episode length: 320.75 +/- 45.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | -2.45e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036279024 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.453        |\n",
      "|    value_loss           | 9.56e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 271       |\n",
      "|    ep_rew_mean     | -1.61e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 83        |\n",
      "|    time_elapsed    | 357       |\n",
      "|    total_timesteps | 169984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-3602.20 +/- 2545.44\n",
      "Episode length: 339.25 +/- 25.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | -3.6e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015975062 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | -0.134       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.21e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 1.43e-05     |\n",
      "|    std                  | 0.449        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 452      |\n",
      "|    ep_rew_mean     | -3e+03   |\n",
      "| time/              |          |\n",
      "|    fps             | 476      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 379          |\n",
      "|    ep_rew_mean          | -8.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 363          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017928347 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | -0.0665      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.32e+03     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000568    |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-4650.15 +/- 7330.35\n",
      "Episode length: 347.25 +/- 42.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | -4.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020712775 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.104        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.3e+03      |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 1.15e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 377       |\n",
      "|    ep_rew_mean     | -7.85e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 479       |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 367       |\n",
      "|    total_timesteps | 176128    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 397          |\n",
      "|    ep_rew_mean          | -2.38e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 369          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039405394 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.79e+05     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.453        |\n",
      "|    value_loss           | 8.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7757.26 +/- 11281.43\n",
      "Episode length: 358.38 +/- 56.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 358         |\n",
      "|    mean_reward          | -7.76e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004128854 |\n",
      "|    clip_fraction        | 0.013       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.27e+04    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 8.04e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 357       |\n",
      "|    ep_rew_mean     | -2.18e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 482       |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 373       |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | -2.44e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 375          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067914985 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | 0.361        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.51e+04     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.447        |\n",
      "|    value_loss           | 1.39e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-2931.95 +/- 1938.21\n",
      "Episode length: 335.38 +/- 56.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | -2.93e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011658419 |\n",
      "|    clip_fraction        | 0.00361      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.614       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.98e+04     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000791    |\n",
      "|    std                  | 0.448        |\n",
      "|    value_loss           | 2.34e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 329       |\n",
      "|    ep_rew_mean     | -1.77e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 485       |\n",
      "|    iterations      | 90        |\n",
      "|    time_elapsed    | 379       |\n",
      "|    total_timesteps | 184320    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 296          |\n",
      "|    ep_rew_mean          | -1.78e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 381          |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025584698 |\n",
      "|    clip_fraction        | 0.00249      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | 0.175        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.74e+04     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.444        |\n",
      "|    value_loss           | 5.54e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-4456.28 +/- 7293.13\n",
      "Episode length: 318.00 +/- 42.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | -4.46e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020203653 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.11e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00073     |\n",
      "|    std                  | 0.442        |\n",
      "|    value_loss           | 1.61e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 426       |\n",
      "|    ep_rew_mean     | -2.58e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 489       |\n",
      "|    iterations      | 92        |\n",
      "|    time_elapsed    | 385       |\n",
      "|    total_timesteps | 188416    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 376          |\n",
      "|    ep_rew_mean          | -2.29e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 387          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044838246 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.6         |\n",
      "|    explained_variance   | -0.176       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+03     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.439        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-7507.91 +/- 14105.16\n",
      "Episode length: 343.75 +/- 49.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | -7.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030315695 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.06e+05     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.438        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 536       |\n",
      "|    ep_rew_mean     | -2.48e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 492       |\n",
      "|    iterations      | 94        |\n",
      "|    time_elapsed    | 391       |\n",
      "|    total_timesteps | 192512    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 375          |\n",
      "|    ep_rew_mean          | -2.34e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 393          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018106019 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+06     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    std                  | 0.439        |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-4204.83 +/- 6520.28\n",
      "Episode length: 317.62 +/- 35.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 318         |\n",
      "|    mean_reward          | -4.2e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004321959 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | -0.0274     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63e+04    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 3.49e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 857       |\n",
      "|    ep_rew_mean     | -9.72e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 495       |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 397       |\n",
      "|    total_timesteps | 196608    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 346          |\n",
      "|    ep_rew_mean          | -2.15e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 399          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038970914 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.0855       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.48e+04     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.000572    |\n",
      "|    std                  | 0.428        |\n",
      "|    value_loss           | 6.48e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-3776.74 +/- 4861.09\n",
      "Episode length: 340.88 +/- 32.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | -3.78e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069252644 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+03     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    std                  | 0.422        |\n",
      "|    value_loss           | 1.68e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 613       |\n",
      "|    ep_rew_mean     | -3.83e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 498       |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 402       |\n",
      "|    total_timesteps | 200704    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f2c70296790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "model.set_parameters(\"last_model\")\n",
    "model.learn(\n",
    "    total_timesteps=200000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de305199a42f8d",
   "metadata": {},
   "source": [
    "### 绘图检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c11d5f894713807",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-14T06:59:39.646152100Z",
     "start_time": "2024-05-14T06:59:34.637632500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 total reward: -15552.772293404003\n",
      "Episode 2 total reward: -1386.998581718737\n",
      "Episode 3 total reward: -20034.521111268263\n",
      "Episode 4 total reward: -1811.7595348251762\n",
      "Episode 5 total reward: -2177.125398369052\n",
      "Episode 6 total reward: -2046.6170371615028\n",
      "Episode 7 total reward: -7051.5856925804355\n",
      "Episode 8 total reward: -3536.8153138119064\n",
      "Episode 9 total reward: -992.2671000013494\n",
      "Episode 10 total reward: -1335.691960302166\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2000x800 with 10 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8YAAAMWCAYAAACDduxsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hUZfrG8XtSJj2TAkkooYkiIIKKiygIKlJ0Weu6KgIqduydXQuiKwgudl3dn4oFd9EVsaOgAisiKooFEek9CaTMpE87vz9GopEAk2RmzpTv57pyKTNnzvtk0HMn85z3fS2GYRgCAAAAAAAAAAAAACBKxZldAAAAAAAAAAAAAAAAwURjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHGiByZMny2KxhHTMTZs2yWKxaNasWSEdFwDQOmQGAMBfZAYAwF9kBgDAX2QG8Csa44h6s2bNksVi2efX559/bnaJppkzZ44uuOACHXzwwbJYLBo6dKjZJQGAqciMppWWlmrGjBk6/vjj1bZtW2VlZemYY47RnDlzzC4NAExDZuzbDTfcoCOPPFI5OTlKTU1Vz549NXnyZFVVVZldGgCYgszwz/r165WcnCyLxaKvvvrK7HIAwBRkxr516dKlyffkiiuuMLs0RJAEswsAQmXKlCnq2rXrXo9379692ee64447dPvttweiLFM99dRTWrFihY4++miVlpaaXQ4AhA0yo7Fly5bpb3/7m0455RTdcccdSkhI0Ouvv65zzz1XP/74o+655x6zSwQA05AZe/vyyy81ePBgXXTRRUpOTtY333yjadOmaeHChVqyZIni4rhHH0BsIjP274YbblBCQoLq6+vNLgUATEdmNK1fv3666aabGj12yCGHmFQNIhGNccSMUaNGqX///gE5V0JCghISIv9/n5deekkdOnRQXFycDjvsMLPLAYCwQWY01rt3b61du1adO3dueOyqq67SsGHD9MADD+jWW29VWlqaiRUCgHnIjL19+umnez120EEH6eabb9YXX3yhY445xoSqAMB8ZMa+ffDBB/rggw9066236r777jO7HAAwHZnRtA4dOuiCCy4wuwxEMG7TBn6xZ8+LBx98UA899JA6d+6slJQUDRkyRD/88EOjY5vak2PBggUaNGiQsrKylJ6erh49euivf/1ro2NKSko0YcIE5efnKzk5WX379tULL7ywVy0VFRW68MILZbPZlJWVpfHjx6uioqLJun/66SedffbZysnJUXJysvr376+33nrLr++5sLCQ2RoA0AKxlhldu3Zt1BSXJIvFotNPP1319fXasGHDAc8BALEq1jJjX7p06dJQAwCgabGaGS6XS9ddd52uu+46HXTQQX6/DgBiWaxmhiQ5nU5VV1c36zXAHtFxiwjgB7vdrt27dzd6zGKxKDc3t9FjL774oiorKzVx4kTV1dXpkUce0Yknnqjvv/9e+fn5TZ571apV+uMf/6jDDz9cU6ZMUVJSktatW6elS5c2HFNbW6uhQ4dq3bp1uvrqq9W1a1e99tpruvDCC1VRUaHrrrtOkmQYhk477TR9+umnuuKKK9SzZ0+98cYbGj9+fJPjHnfccerQoYNuv/12paWl6dVXX9Xpp5+u119/XWeccUZr3zYAiElkhn+KiookSW3atGn2awEgWpAZTXO73aqoqJDT6dQPP/ygO+64QxkZGfrDH/5wwNcCQLQiM5r28MMPq7y8XHfccYfmzp17wOMBIBaQGU37+OOPlZqaKo/Ho86dO+uGG25oqAXwiwFEueeff96Q1ORXUlJSw3EbN240JBkpKSnGtm3bGh5fvny5Icm44YYbGh67++67jd/+7/PQQw8Zkoxdu3bts46HH37YkGS8/PLLDY85nU5j4MCBRnp6uuFwOAzDMIx58+YZkozp06c3HOd2u43Bgwcbkoznn3++4fGTTjrJ6NOnj1FXV9fwmNfrNY499ljj4IMPbtb71Lt3b2PIkCHNeg0ARBsyw3+lpaVGXl6eMXjw4Ga/FgCiAZmxf8uWLWv0nvTo0cP45JNP/HotAEQbMmPfdu7caWRkZBhPP/10o/fqyy+/POBrASAakRn7Nnr0aOOBBx4w5s2bZzz77LMNY9x6660HfC2wB2soI2Y88cQTWrBgQaOv999/f6/jTj/9dHXo0KHhz3/4wx80YMAAvffee/s8d1ZWliTpzTfflNfrbfKY9957TwUFBTrvvPMaHktMTNS1116rqqoqLV68uOG4hIQEXXnllQ3HxcfH65prrml0vrKyMn388cc655xzVFlZqd27d2v37t0qLS3ViBEjtHbtWm3fvv3AbwwAYC9kxv55vV6NGTNGFRUVeuyxx/x+HQBEIzKjab169dKCBQs0b9483XrrrUpLS1NVVdUBXwcA0YzM2Nttt92mbt266ZJLLtnvcQAQa8iMvb311lu69dZbddppp+niiy/W4sWLNWLECM2cOVPbtm3b72uBPVhKHTHjD3/4g/r373/A4w4++OC9HjvkkEP06quv7vM1f/nLX/R///d/uuSSS3T77bfrpJNO0plnnqmzzz67YQ/vzZs36+CDD95rT++ePXs2PL/nn+3atVN6enqj43r06NHoz+vWrZNhGLrzzjt15513NllXSUlJo1AEAPiHzNi/a665RvPnz9eLL76ovn37+vUaAIhWZEbTMjMzNWzYMEnSaaedpldeeUWnnXaavv76a7IDQMwiMxr7/PPP9dJLL+mjjz7aqyYAiHVkxoFZLBbdcMMN+uCDD7Ro0SJdcMEFfr8WsYvGOBAAKSkpWrJkiT755BO9++67mj9/vubMmaMTTzxRH374oeLj4wM+5p47uW6++WaNGDGiyWO6d+8e8HEBAK0T6Zlxzz336Mknn9S0adM0duzYgNUIANhbpGfGb5155pkaO3as/vOf/9AYB4AgiMTMuPXWWzV48GB17dpVmzZtkqSG/XR37typLVu2qFOnToEtGgAQkZmxL4WFhZJ8M9IBf9AYB35n7dq1ez32888/q0uXLvt9XVxcnE466SSddNJJmjlzpu6//3797W9/0yeffKJhw4apc+fO+u677+T1ehvdZfXTTz9Jkjp37tzwz48++khVVVWN7rJas2ZNo/G6desmybd8yZ6ZGACA0Iq1zHjiiSc0efJkXX/99brttttafB4AiEWxlhm/V19fL6/XK7vdHrBzAkC0ipXM2LJlizZv3qyuXbvu9dyf/vQn2Ww2VVRUNPu8ABBLYiUz9mXDhg2SpLZt2wbsnIhurFED/M68efMa7WXxxRdfaPny5Ro1atQ+X9PU3Uj9+vWT5PsASJJOOeUUFRUVac6cOQ3HuN1uPfbYY0pPT9eQIUMajnO73XrqqacajvN4PHvt4ZqXl6ehQ4fq6aef1s6dO/caf9euXX58twCA1oilzJgzZ46uvfZajRkzRjNnzjzg8QCAxmIlMyoqKuRyufZ6/P/+7/8kya/lIAEg1sVKZjzzzDN64403Gn3t2ZP2wQcf1OzZs/f7egBA7GRGWVmZPB5Po8dcLpemTZsmq9WqE044Yb+vB/Zgxjhixvvvv99wN9NvHXvssQ13K0m+5ToGDRqkK6+8UvX19Xr44YeVm5urW2+9dZ/nnjJlipYsWaJTTz1VnTt3VklJiZ588kl17NhRgwYNkiRddtllevrpp3XhhRdqxYoV6tKli/773/9q6dKlevjhh5WRkSFJGj16tI477jjdfvvt2rRpk3r16qW5c+c2ObPiiSee0KBBg9SnTx9deuml6tatm4qLi7Vs2TJt27ZN33777X7fkyVLlmjJkiWSfMFTXV2t++67T5J0/PHH6/jjjz/AuwoA0YnMaOyLL77QuHHjlJubq5NOOmmvD6h+/74AQCwhMxpbtGiRrr32Wp199tk6+OCD5XQ69b///U9z585V//792fcPQEwjMxobPnz4Xo/tmSE+ZMgQbqYCENPIjMbeeust3XfffTr77LPVtWtXlZWV6ZVXXtEPP/yg+++/XwUFBX6/t4hxBhDlnn/+eUPSPr+ef/55wzAMY+PGjYYkY8aMGcY//vEPo7Cw0EhKSjIGDx5sfPvtt43Oeffddxu//d/no48+Mk477TSjffv2htVqNdq3b2+cd955xs8//9zodcXFxcZFF11ktGnTxrBarUafPn0axv+t0tJSY+zYsUZmZqZhs9mMsWPHGt98802jevdYv369MW7cOKOgoMBITEw0OnToYPzxj380/vvf/x7wvdnzfTT1dffdd/v1/gJANCEzWve+AEAsITOatm7dOmPcuHFGt27djJSUFCM5Odno3bu3cffddxtVVVX+v8EAEEXIjOa/V19++WWzXwsA0YDMaNpXX31ljB492ujQoYNhtVqN9PR0Y9CgQcarr77q/5sLGIZhMQzDaH17HYh8mzZtUteuXTVjxgzdfPPNZpcDAAhjZAYAwF9kBgDAX2QGAMBfZAbQMuwxDgAAAAAAAAAAAACIajTGAQAAAAAAAAAAAABRjcY4AAAAAAAAAAAAACCqmdoYnzp1qo4++mhlZGQoLy9Pp59+utasWdPomLq6Ok2cOFG5ublKT0/XWWedpeLi4kbHbNmyRaeeeqpSU1OVl5enW265RW63u9ExixYt0pFHHqmkpCR1795ds2bNCva3hwjTpUsXGYbBfhxAmCIzEE7IDCC8kRkIJ2QGEN7IDIQTMgMIb2QGwgmZAbSMqY3xxYsXa+LEifr888+1YMECuVwuDR8+XNXV1Q3H3HDDDXr77bf12muvafHixdqxY4fOPPPMhuc9Ho9OPfVUOZ1OffbZZ3rhhRc0a9Ys3XXXXQ3HbNy4UaeeeqpOOOEErVy5Utdff70uueQSffDBByH9fgEALUdmAAD8RWYAAPxFZgAA/EVmAEAUMMJISUmJIclYvHixYRiGUVFRYSQmJhqvvfZawzGrV682JBnLli0zDMMw3nvvPSMuLs4oKipqOOapp54yMjMzjfr6esMwDOPWW281evfu3Wisv/zlL8aIESOC/S0BAIKEzAAA+IvMAAD4i8wAAPiLzACAyJNgVkO+KXa7XZKUk5MjSVqxYoVcLpeGDRvWcMyhhx6qTp06admyZTrmmGO0bNky9enTR/n5+Q3HjBgxQldeeaVWrVqlI444QsuWLWt0jj3HXH/99U3WUV9fr/r6+oY/e71elZWVKTc3VxaLJVDfLgAEnGEYqqysVPv27RUXZ+qiIEFHZgBA68VKbpAZANB6ZAaZAQD+IjPIDADwV6gzI2wa416vV9dff72OO+44HXbYYZKkoqIiWa1WZWVlNTo2Pz9fRUVFDcf8NkT2PL/nuf0d43A4VFtbq5SUlEbPTZ06Vffcc0/AvjcACLWtW7eqY8eOZpcRNGQGAARWNOcGmQEAgUVm+JAZAHBgZIYPmQEABxaqzAibxvjEiRP1ww8/6NNPPzW7FE2aNEk33nhjw5/tdrs6deqkrVu3KjMz08TKAGD/HA6HCgsLlZGRYXYpQUVmAEBgxEJukBkAEBhkRmiRGQAiGZkRWmQGgEgW6swIi8b41VdfrXfeeUdLlixpdDdAQUGBnE6nKioqGt1lVVxcrIKCgoZjvvjii0bnKy4ubnhuzz/3PPbbYzIzM/e6u0qSkpKSlJSUtNfjmZmZBAmAiBDNyySRGQAQeNGaG2QGAAQemeFDZgDAgZEZPmQGABxYqDLD1A0+DMPQ1VdfrTfeeEMff/yxunbt2uj5o446SomJifroo48aHluzZo22bNmigQMHSpIGDhyo77//XiUlJQ3HLFiwQJmZmerVq1fDMb89x55j9pwDABD+yAwAgL/IDACAv8gMAIC/yAwAiAKGia688krDZrMZixYtMnbu3NnwVVNT03DMFVdcYXTq1Mn4+OOPja+++soYOHCgMXDgwIbn3W63cdhhhxnDhw83Vq5cacyfP99o27atMWnSpIZjNmzYYKSmphq33HKLsXr1auOJJ54w4uPjjfnz5/tVp91uNyQZdrs9cN88AARBNF+vyAwACLxovWaRGQAQeNF6zSIzACDwovWaRWYAQOCF+pplamNcUpNfzz//fMMxtbW1xlVXXWVkZ2cbqampxhlnnGHs3Lmz0Xk2bdpkjBo1ykhJSTHatGlj3HTTTYbL5Wp0zCeffGL069fPsFqtRrdu3RqNcSAECYBIEc3XKzIDAAIvWq9ZZAYABF60XrPIDAAIvGi9ZpEZABB4ob5mWQzDMAI9Cz3aOBwO2Ww22e129uQAAsjj8cjlcpldRsRJTExUfHx8k89xvTIffwdAcJAZLbO/zJC4ZpmN9x8IDjKjZciM8Mb7DwQHmdEyZEZ44/0HgoPMaJlwy4yEoI8AAL9jGIaKiopUUVFhdikRKysrSwUFBbJYLGaXAgBBRWa0HpkBIFaQGa1HZgCIFWRG65EZAGIFmdF64ZQZNMYBhNyeEMnLy1NqampYXAwjhWEYqqmpUUlJiSSpXbt2JlcEAMFFZrQcmQEg1pAZLUdmAIg1ZEbLkRkAYg2Z0XLhmBk0xgGElMfjaQiR3Nxcs8uJSCkpKZKkkpIS5eXl7XcZEgCIZGRG65EZAGIFmdF6ZAaAWEFmtB6ZASBWkBmtF26ZEWfq6ABizp49OFJTU02uJLLtef/Y0wRANCMzAoPMABALyIzAIDMAxAIyIzDIDACxgMwIjHDKDBrjAEzBciOtw/sHIJZwzWsd3j8AsYRrXuvw/gGIJVzzWof3D0As4ZrXOuH0/tEYBwAAAAAAAAAAAABENRrjABCGunTpoocfftjsMgAAEYDMAAD4i8wAAPiLzAAA+CuSMoPGOAA0Q1FRka677jp1795dycnJys/P13HHHaennnpKNTU1ZpcHAAgjZAYAwF9kBgDAX2QGAMBfZMbeEswuAACaw15nV6WzUh0zO+713DbHNmVYM2RLtgVl7A0bNui4445TVlaW7r//fvXp00dJSUn6/vvv9cwzz6hDhw7605/+FJSxAQDNR2YAAPxFZgAA/EVmAAD8RWaEH2aMA4gY9jq7Rs4eqSGzhmirfWuj57bat2rIrCEaOXuk7HX2oIx/1VVXKSEhQV999ZXOOecc9ezZU926ddNpp52md999V6NHj5YkbdmyRaeddprS09OVmZmpc845R8XFxQ3nWb9+vU477TTl5+crPT1dRx99tBYuXBiUmgEgVpEZAAB/kRkAAH+RGQAAf5EZ4YnGOICIUemsVEl1iTaUb9DQF4Y2hMlW+1YNfWGoNpRvUEl1iSqdlQEfu7S0VB9++KEmTpyotLS0Jo+xWCzyer067bTTVFZWpsWLF2vBggXasGGD/vKXvzQcV1VVpVNOOUUfffSRvvnmG40cOVKjR4/Wli1bAl43AMQqMgMA4C8yAwDgLzIDAOAvMiM8sZQ6gIjRMbOjFo1f1BAaQ18YqpfOeElj3xirDeUb1C27mxaNX9TksiSttW7dOhmGoR49ejR6vE2bNqqrq5MkTZw4UcOGDdP333+vjRs3qrCwUJL04osvqnfv3vryyy919NFHq2/fvurbt2/DOe6991698cYbeuutt3T11VcHvHYAiEVkBgDAX2QGAMBfZAYAwF9kRnhixjiAiFJoK9Si8YvULbubNpRv0HHPHdcoRApthSGt54svvtDKlSvVu3dv1dfXa/Xq1SosLGwIEUnq1auXsrKytHr1akm+O6xuvvlm9ezZU1lZWUpPT9fq1asj9g4rAAhXZAYAwF9kBgDAX2QGAMBfZEb4YcY4gIhTaCvUS2e8pOOeO67hsZfOeCmoIdK9e3dZLBatWbOm0ePdunWTJKWkpPh9rptvvlkLFizQgw8+qO7duyslJUVnn322nE5nQGsGAJAZAAD/kRkAAH+RGQAAf5EZ4YUZ4wAizlb7Vo19Y2yjx8a+MbZhj45gyM3N1cknn6zHH39c1dXV+zyuZ8+e2rp1q7Zu/bWWH3/8URUVFerVq5ckaenSpbrwwgt1xhlnqE+fPiooKNCmTZuCVjsAxDIyAwDgLzIDAOAvMgMA4C8yI7zQGAcQUbbatzbsydEtu5uWXry0YRmSoS8MDWqYPPnkk3K73erfv7/mzJmj1atXa82aNXr55Zf1008/KT4+XsOGDVOfPn00ZswYff311/riiy80btw4DRkyRP3795ckHXzwwZo7d65Wrlypb7/9Vueff768Xm/Q6gaAWEVmAAD8RWYAAPxFZgAA/EVmhB8a4wAixjbHtkYhsmj8Ih1beGyjPTqGvjBU2xzbgjL+QQcdpG+++UbDhg3TpEmT1LdvX/Xv31+PPfaYbr75Zt17772yWCx68803lZ2dreOPP17Dhg1Tt27dNGfOnIbzzJw5U9nZ2Tr22GM1evRojRgxQkceeWRQagaAWEVmAAD8RWYAAPxFZgAA/EVmhCeLYRiG2UWEO4fDIZvNJrvdrszMTLPLASJaXV2dNm7cqK5duyo5OblZr7XX2TVy9kiVVJdo0fhFjfbg2HPnVV5anuaPmS9bsi3AlYeXfb2PXK/Mx98BEDhkRmDs733kmmUu3n8gcMiMwCAzwhfvPxA4ZEZgkBnhi/cfCBwyIzDCKTMSgj4CAASILdmm+WPmq9JZqY6ZHRs9V2gr1OILFyvDmhH1IQIAODAyAwDgLzIDAOAvMgMA4C8yIzzRGAcQUWzJtn0Gxe/DBQAQ28gMAIC/yAwAgL/IDACAv8iM8MMe4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAKbwer1mlxDReP8AxBKuea3D+wcglnDNax3ePwCxhGte6/D+AYglXPNaJ5zevwSzCwAQW6xWq+Li4rRjxw61bdtWVqtVFovF7LIihmEYcjqd2rVrl+Li4mS1Ws0uCQCChsxoHTIDQCwhM1qHzAAQS8iM1iEzAMQSMqN1wjEzaIwDCKm4uDh17dpVO3fu1I4dO8wuJ2KlpqaqU6dOiotj4Q8A0YvMCAwyA0AsIDMCg8wAEAvIjMAgMwDEAjIjMMIpM2iMAwg5q9WqTp06ye12y+PxmF1OxImPj1dCQgJ3pgGICWRG65AZAGIJmdE6ZAaAWEJmtA6ZASCWkBmtE26ZQWMcgCksFosSExOVmJhodikAgDBHZgAA/EVmAAD8RWYAAPxFZkQPU+esL1myRKNHj1b79u1lsVg0b968Rs9bLJYmv2bMmNFwTJcuXfZ6ftq0aY3O891332nw4MFKTk5WYWGhpk+fHopvDwAQQGQGAMBfZAYAwF9kBgDAX2QGAEQ+Uxvj1dXV6tu3r5544okmn9+5c2ejr+eee04Wi0VnnXVWo+OmTJnS6Lhrrrmm4TmHw6Hhw4erc+fOWrFihWbMmKHJkyfrmWeeCer3BgAILDIDAOAvMgMA4C8yAwDgLzIDACKfqUupjxo1SqNGjdrn8wUFBY3+/Oabb+qEE05Qt27dGj2ekZGx17F7zJ49W06nU88995ysVqt69+6tlStXaubMmbrsssta/00AAEKCzAAA+IvMAAD4i8wAAPiLzACAyGfqjPHmKC4u1rvvvqsJEybs9dy0adOUm5urI444QjNmzJDb7W54btmyZTr++ONltVobHhsxYoTWrFmj8vLyJseqr6+Xw+Fo9AUAiBxkBgDAX2QGAMBfZAYAwF9kBgCEJ1NnjDfHCy+8oIyMDJ155pmNHr/22mt15JFHKicnR5999pkmTZqknTt3aubMmZKkoqIide3atdFr8vPzG57Lzs7ea6ypU6fqnnvuCdJ3AgAINjIDAOAvMgMA4C8yAwDgLzIDAMJTxDTGn3vuOY0ZM0bJycmNHr/xxhsb/v3www+X1WrV5ZdfrqlTpyopKalFY02aNKnReR0OhwoLC1tWOAAg5MgMAIC/yAwAgL/IDACAv8gMAAhPEdEY/9///qc1a9Zozpw5Bzx2wIABcrvd2rRpk3r06KGCggIVFxc3OmbPn/e1j0dSUlKLQwgAYC4yAwDgLzIDAOAvMgMA4C8yAwDCV0TsMf7ss8/qqKOOUt++fQ947MqVKxUXF6e8vDxJ0sCBA7VkyRK5XK6GYxYsWKAePXo0uewIACCykRkAAH+RGQAAf5EZAAB/kRkAEL5MbYxXVVVp5cqVWrlypSRp48aNWrlypbZs2dJwjMPh0GuvvaZLLrlkr9cvW7ZMDz/8sL799ltt2LBBs2fP1g033KALLrigISTOP/98Wa1WTZgwQatWrdKcOXP0yCOPNFpaBAAQ/sgMAIC/yAwAgL/IDACAv8gMAIgChok++eQTQ9JeX+PHj2845umnnzZSUlKMioqKvV6/YsUKY8CAAYbNZjOSk5ONnj17Gvfff79RV1fX6Lhvv/3WGDRokJGUlGR06NDBmDZtWrPqtNvthiTDbre36PsEgFCJ5usVmQEAgRet1ywyAwACL1qvWWQGAARetF6zyAwACLxQX7MshmEYQe69RzyHwyGbzSa73a7MzEyzywGAfeJ6ZT7+DgBEEq5Z5uL9BxBJuGaZi/cfQCThmmUu3n8AkSTU16yI2GMcAAAAAAAAAAAAAICWojEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAIAjsdXZtc2xr8rltjm2y19lDXFHsojEOAAAAAAAAAAAAAAFmr7Nr5OyRGjJriLbatzZ6bqt9q4bMGqKRs0fSHA8RGuMAAAAAAAAAAAAAEGCVzkqVVJdoQ/kGDX1haENzfKt9q4a+MFQbyjeopLpElc5KU+uMFTTGAQAAAAAAACAGsJQrAACh1TGzoxaNX6Ru2d0amuOfbf2soSneLbubFo1fpI6ZHc0uNSbQGAcAAAAAAACAKMdSrgAAmKPQVtioOX7cc8c1aooX2grNLjFm0BgHAAAAAAAAgCjHUq4AAJin0Faol854qdFjL53xEk3xEKMxDgAAAAAAAABRjqVcAQAwz1b7Vo19Y6wk6eht0sAt0tg3xu61iguCi8Y4AAAAAAAAAMQAlnIFACD0frs6S7fsbnrm7FkauyVrr1VcEHw0xgEAAAAAAAAgRrCUKwAAobPNsW2v1Vn6HX+Oxqcf12gVl22ObWaXGhNojAMAAAAAAABAjPjtUq57sJQrAADBkWHNUF5aXuPVWVJSlOq2NKzikpeWpwxrhtmlxgRTG+NLlizR6NGj1b59e1ksFs2bN6/R8xdeeKEsFkujr5EjRzY6pqysTGPGjFFmZqaysrI0YcIEVVVVNTrmu+++0+DBg5WcnKzCwkJNnz492N8aACDAyAwAgL/IDACAv8gMxJrfL+W69OKlunhHgcq3s5QrcCBkBoCWsCXbNH/MfC2+cHHj1Vni4lRoK9TiCxdr/pj5siXbzCsyhpjaGK+urlbfvn31xBNP7POYkSNHaufOnQ1f//73vxs9P2bMGK1atUoLFizQO++8oyVLluiyyy5reN7hcGj48OHq3LmzVqxYoRkzZmjy5Ml65plngvZ9AQACj8wAAPiLzAAA+IvMQCxpainXYwuP1Ywe1+jssnyWcgUOgMwA0FK2ZJs6ZnZs8rmOmR1piodQgpmDjxo1SqNGjdrvMUlJSSooKGjyudWrV2v+/Pn68ssv1b9/f0nSY489plNOOUUPPvig2rdvr9mzZ8vpdOq5556T1WpV7969tXLlSs2cObNR4AAAwhuZAQDwF5kBAPAXmYFYsmcpV0m/LuUqKefEU/Tgjp36KPs9lnIF9oPMABBQhmF2BTEp7PcYX7RokfLy8tSjRw9deeWVKi0tbXhu2bJlysrKaggRSRo2bJji4uK0fPnyhmOOP/54Wa3WhmNGjBihNWvWqLy8vMkx6+vr5XA4Gn0BAMIfmQEA8BeZAQDwF5mBaLHPpVwPOkiZ23ezlCsQAGQGAIS3sG6Mjxw5Ui+++KI++ugjPfDAA1q8eLFGjRolj8cjSSoqKlJeXl6j1yQkJCgnJ0dFRUUNx+Tn5zc6Zs+f9xzze1OnTpXNZmv4KiwsbPI4AED4IDMAAP4iMwAA/iIzEG2aXMo1I0OqqmIpV6CVyAwACH+mLqV+IOeee27Dv/fp00eHH364DjroIC1atEgnnXRS0MadNGmSbrzxxoY/OxwOwgQAwhyZAQDwF5kBAPAXmYGYYbGYXQEQ8cgMAH6rq5N+szIEQiesZ4z/Xrdu3dSmTRutW7dOklRQUKCSkpJGx7jdbpWVlTXs41FQUKDi4uJGx+z58772+khKSlJmZmajLwBAZCEzAAD+IjMQSPY6u7Y5tjX53DbHNtnr7CGuCEAgkRkAAH+RGQD2aeNGqWtXs6uISRHVGN+2bZtKS0vVrl07SdLAgQNVUVGhFStWNBzz8ccfy+v1asCAAQ3HLFmyRC6Xq+GYBQsWqEePHsrOzg7tNwAACBkyAwDgLzIDgWKvs2vk7JEaMmuIttq3Nnpuq32rhswaopGzR9IcByIYmQEA8BeZAWCf1q+XDjrI7CpikqmN8aqqKq1cuVIrV66UJG3cuFErV67Uli1bVFVVpVtuuUWff/65Nm3apI8++kinnXaaunfvrhEjRkiSevbsqZEjR+rSSy/VF198oaVLl+rqq6/Wueeeq/bt20uSzj//fFmtVk2YMEGrVq3SnDlz9MgjjzRaWgQAEP7IDACAv8gMmKXSWamS6hJtKN+goS8MbWiOb7Vv1dAXhmpD+QaVVJeo0llpap0AfkVmAAD8RWYACJg1a6RDDjG7ithkmOiTTz4xJO31NX78eKOmpsYYPny40bZtWyMxMdHo3LmzcemllxpFRUWNzlFaWmqcd955Rnp6upGZmWlcdNFFRmVlZaNjvv32W2PQoEFGUlKS0aFDB2PatGnNqtNutxuSDLvd3urvGQCCKZqvV2QGAARetF6zyAyYaUvFFqPbI90MTZbR7ZFuxtItSxv9eUvFFrNLBFokWq9ZZAbwiz/+0ewKEEWi9ZpFZgAImPPPN4yyMrOrCAuhvmZZDMMwgtx7j3gOh0M2m012u539OQCENa5X5uPvAEAk4ZplLt7/6PXbGeJ7dMvupkXjF6nQVmheYUArcM0yF+8/gsowpD/9SXr7bbMrQZTgmmUu3n8gApx6qvTuu2ZXERZCfc2KqD3GAQAAAAAId4W2Qr10xkuNHnvpjJdoigMAwlNZmZSba3YVAADEBrtd4qYV09AYBwAAAAAggLbat2rsG2MbPTb2jbENe44DABBWtm+XftnfGAAABNmyZdIf/mB2FTGLxjgAAAAAAAHy22XUu2V302cXLNK9K3O0oXyDhr4wlOY4ACD8bNokde5sdhUAAMSGjz6STjrJ7CpiFo1xAAAAAAACYJtjW6Om+KLxizSw2/G6qfYIdcvu1tAc3+bYZnapAAD86scfpd69za4CAIDY8MMP0mGHmV1FzKIxDgAAAABAAGRYM5SXltfQFC+0FUoWi1ISU7Ro/CJ1y+6mvLQ8ZVgzzC4VAIBf/fij1LOn2VUAABD9Skul7GwpjvasWRLMLgAAAAAAgGhgS7Zp/pj5qnRWqmNmx1+fsFhUaCvU4gsXK8OaIVuyzbwiAQD4vdJSKTfX7CoAAIh+770njRxpdhUxjcY4AAAAAAABYku27d34NgxJatwsBwAgHNTXSwl8RAwAQEi884701FNmVxHTmKsPAAAAAECwVFRINmaIAwDC1LffSkccYXYVAABEv5oaqa5Oyskxu5KYRmMcAAAAAIBg2bxZ6tzZ7CoAAGjasmXSMceYXQUAANHvgw+k4cPNriLm0RgHAAAAACBYfvhB6t3b7CoAAGja8uXSH/5gdhUAAES/uXOl0083u4qYR2McAAAAAIBg+fJLqX9/s6sAAGBvhiGVlbGkKwAAweZwSJWVUocOZlcS82iMAwAAAAAQLGvXSt27m10FAAB7+/57qU8fs6sAACD6/fe/0tlnm10FRGMcAAAAAIDgqKqSrFYpjl+9AQBh6MMPpZNPNrsKAACi39y50hlnmF0FRGMcAAAAAIDgWLRIOuEEs6sAAKBpn34qDR5sdhUAAES3jRultm2ltDSzK4FojAMAAAAAEBwffCCNHGl2FQAA7K262vfPlBRz6wAAINr985/SpZeaXQV+QWMcAAAAAIBA83qln36SDj7Y7EoAANjb++9z8xYAAMFWVyd98400cKDZleAXNMYBAAAAAAi0Tz+VBg2SLBazKwEAYG/z5kmnn252FQAARLdXX5XOOYffC8MIjXEAAAAAAALtP/+R/vIXs6sAAGBv9fVSeblUUGB2JQAARLdXXpHOP9/sKvAbNMYBAAAAAAgkp1Nav1469FCzKwEAYG8LFkgnn2x2FQAARLcvv5R69ZJSU82uBL9BYxwAAAAAgECaN0867TSzqwAAoGmvvy6deabZVQAAEN0eeki67jqzq8Dv0BgHAAAAACCQXn5ZuuACs6sAAGBvVVXS7t1Sp05mVwIAQPRau1ayWqXOnc2uBL9DYxwAAAAAgED56Sffnq2ZmWZXAgDA3ubOZbY4AADB9uCD0k03mV0FmkBjHAAAAACAQHnsMWniRLOrAACgaf/9r3T22WZXAQBA9Nq5Uyotlfr0MbsSNIHGOAAAAAAAgVBSIu3YIfXta3YlAADsbcsWKTtbysgwuxIAAKLXww9L119vdhXYBxrjAAAAAAAEwqOPStdea3YVAAA07cUXpbFjza4CAIDoVVoq/fCDNGiQ2ZVgH2iMAwAAAADQWpWV0ldfSUOHml0JAAB7c7uljz+WTjzR7EoAAIhe06dLt9xidhXYDxrjAAAAAAC01pNPSpdfLlksZlcCAMDe3n5b+uMfpTg+DgYAICiKiqTVq7lZOszxkxAAAAAAAK1ht/tm4Z1+utmVAADQtFmzpIsuMrsKAACi19Sp0l//anYVOABTG+NLlizR6NGj1b59e1ksFs2bN6/hOZfLpdtuu019+vRRWlqa2rdvr3HjxmnHjh2NztGlSxdZLJZGX9OmTWt0zHfffafBgwcrOTlZhYWFmj59eii+PQBAAJEZAAB/kRkIuX/8Q7rxRmaLAxGIzEBMWLNGysuTsrPNrgSIaGQGgH3aulXavl065hizK8EBmNoYr66uVt++ffXEE0/s9VxNTY2+/vpr3Xnnnfr66681d+5crVmzRn/605/2OnbKlCnauXNnw9c111zT8JzD4dDw4cPVuXNnrVixQjNmzNDkyZP1zDPPBPV7AwAEFpkBAPAXmYGQKimRvv5aGj7c7EoAtACZgZjw5JPSVVeZXQUQ8cgMAPv0979Lf/ub2VXADwlmDj5q1CiNGjWqyedsNpsWLFjQ6LHHH39cf/jDH7RlyxZ16tSp4fGMjAwVFBQ0eZ7Zs2fL6XTqueeek9VqVe/evbVy5UrNnDlTl112WeC+GQBAUJEZAAB/kRkIqb//Xbr9dmaLAxGKzEDUs9ultWulI44wuxIg4pEZAJq0apVUU0PWRoiI2mPcbrfLYrEoKyur0ePTpk1Tbm6ujjjiCM2YMUNut7vhuWXLlun444+X1WpteGzEiBFas2aNysvLmxynvr5eDoej0RcAILKQGQAAf5EZaLEff5R27ZIGDTK7EgAhQmYg4jzzjEQzDTAFmQHEiLvuku67z+wq4CdTZ4w3R11dnW677Tadd955yszMbHj82muv1ZFHHqmcnBx99tlnmjRpknbu3KmZM2dKkoqKitS1a9dG58rPz294LruJvXWmTp2qe+65J4jfDQAgmMgMAIC/yAy0mGH4lsp76CGzKwEQImQGIo7TKc2fL910k9mVADGHzABixAcfSD16SL9ZFQLhLSIa4y6XS+ecc44Mw9BTTz3V6Lkbb7yx4d8PP/xwWa1WXX755Zo6daqSkpJaNN6kSZMandfhcKiwsLBlxQMAQorMAAD4i8xAq7z7rtSnj9Sli9mVAAgBMgMR6ZVXpPPOk+IiatFQIOKRGUCMcLulGTOkuXPNrgTNEPaN8T0hsnnzZn388ceN7q5qyoABA+R2u7Vp0yb16NFDBQUFKi4ubnTMnj/vax+PpKSkFocQAMA8ZAYAwF9kBlrF6fTNFH/rLbMrARACZAYiktcrvfyy9M47ZlcCxBQyA4ghzz0nnXOOdID/zxFewvp2wT0hsnbtWi1cuFC5ubkHfM3KlSsVFxenvLw8SdLAgQO1ZMkSuVyuhmMWLFigHj16NLnsCAAgMpEZAAB/kRlotQcflC69VEpLM7sSAEFGZiBivfuuNGyYlJxsdiVAzCAzECvq3V6V1Tq1u8apslqnqp1uGYZhdlmh5XBIr74qXXyx2ZWgmUydMV5VVaV169Y1/Hnjxo1auXKlcnJy1K5dO5199tn6+uuv9c4778jj8aioqEiSlJOTI6vVqmXLlmn58uU64YQTlJGRoWXLlumGG27QBRdc0BAS559/vu655x5NmDBBt912m3744Qc98sgjeoh94AAgopAZAAB/kRkIqp9/lr76Spo0yexKAAQAmYGoZBjSE09Ic+aYXQkQVcgMxCqvYWhHVZ22O+pUWudUndu71zEJcRblJCeqIC1ZnWwpssaH9bzc1rv7bt/vhAlhvzA3fs8w0SeffGJI2utr/PjxxsaNG5t8TpLxySefGIZhGCtWrDAGDBhg2Gw2Izk52ejZs6dx//33G3V1dY3G+fbbb41BgwYZSUlJRocOHYxp06Y1q0673W5IMux2e6C+dQAIimi+XpEZABB40XrNIjMQNF6vYfzxj4axcaPZlQAhF63XLDIDUemddwxjyhSzq0AMi9ZrFpmBWOP2eI2fdlca76wtMl7/aYcx96cdxut+fL2xZofxTVGFUeN0m/0tBMe33xrGBReYXUXUCPU1y2IYsba+QfM5HA7ZbDbZ7fYD7gkCAGbiemU+/g4ARBKuWebi/Y9Azz0n2e3SDTeYXQkQclyzzMX7D78ZhjRypPTaa+x5CtNwzTIX7/+vDMNQtcuj8jqXalweeQ1DcRaLkhPilJ2cqAxrgiwWi9llhqWKOpe+2FGuKpenRa+3SIqPs+jIfJs6ZqYEtjgzeb3SH/8o/d//Se3bm11NVAj1NYs5/gAAAAAAHEhxsfSf/0jvvWd2JQAA7Nu770rHH09THEBMq6hzaX15tbZX1cnt9c0N/W37e89s0XiLRQVpSeqenaaclESa5L/YUF6tb0scrTqHIcntNfTFzgoVVdfpyIIsxUXD+/vCC9Lw4TTFIxiNcQAAAAAA9scwfLPEH3iAPeQAAOHLMKTHHvPNFgeAGGSvc2lliV2ltS5Z9GsDXL/79z08e/bOrqpTZlKC+uVlqk1qUoiqDU/ryqr13a7WNcV/b4ujTi5vuQa0z47s5nhZmTR7tjR/vtmVoBXizC4AAAAAAICw9sor0qGHSkccYXYlAADs29tvS0OHMlscQMwxDEM/l1bp4827VVbr8j3m72t/+aej3q0lW8v0fYlDHm9s7kC81VEb8Kb4Hjur6rWy2B6Uc4fM3/4m3XMPN0tHOP72AAAAAADYl23bpBdf9C1NCwBAuPJ4fLPF5841uxIACCmP19Cy7WUqqXEG5Hxry6tVUlOvwYW5ssbHztzSGpdHXxcFt3G9yV6rgrRktc9IDuo4QbFkiW9lluOOM7sStFLs/F8NAAAAAEBzeL3SxInSo48yKwAAEN5eflk6/XQpI8PsSgAgZLyGoc8C2BTfwzd7vFQujzeg5w1XhmHom2K7vEbwZ8p/U2yPvPe1tlaaPNm3tRYiHo1xAAAAAACa8uST0sknSz16mF0JAAD7VlvrW93kssvMrgQAQuqbIrt2BbgpLvmWV6+sd+vzHeUyQtAsNltxdb2Kq+v9Xn6+NZwer34qrQrBSAE0ebJ0442SzWZ2JQgAGuNAmDIMQzUuj3bXOFVSXa/dNfWqcrpjIogBAAAA061aJS1cKF11ldmVAACwf489Jl1xhZSYaHYlABAyRVV12uyoDdr5DUm7apzaZA/eGOFifXmNLCEay5C00V4TOfu4f/WVtHOn9Mc/ml0JAoS14IAw4vEa2lpZq+2VdSqvdcrZRDgkWCzKTk5UQXqSOttSY2qfEwAAACAkamqka6+VZs+W4vh5GwAQxkpLpY8/lt5/3+xKACBkXF5v0PfD3uO7EocK0pOUkhAfkvFCrcblUXFNfUjHdHsNba+qU6fMlJCO22xOp3T77dK//212JQggGuNAGHB5vfq5tFobKqrlOsCdUm7D0K5ap3bVOrVqd6W62FLVIzc9aoMZAAAACLnrr5cmTZIKCsyuBACA/bv/fl9mWUI11w8AzLehvEZ1Idqn2msY+qm0SkfkR+cy2tsrzZkRv9VeG/6N8QcekC65RGrb1uxKEEDc+g6YbHeNUws37tLPZVUHbIr/nteQNlbUaOHGXaYFGAAAABBVZs+W8vKkYcPMrgQAgP1bv17askUaMsTsSgAgZAzD0IaK6tCNJ2mLvVZub2ga8aFWXucK2TLqv1VW5wzvbWO/+076/nvpL38xuxIEGDPGAZMYhqE1ZVX6cXdV684jyeU1tHxHhbrY6tUv36Y47hIGAAAAmu/nn6VXXpHefNPsSgAAOLBJk6SpU82uAgBCqqTGqVp3aJvUHsPQtso6dbGlhnTcUCivc8mM9rTLa6jO4w3PlXDr66Ubb/TdNE2vJeowYxwwyY+7W98U/71N9lp9ubMivO+0AgAAAMJRXZ00caL09NNSAveQAwDC3AcfSN26Sd27m10JAITUzqo6U2Y476isM2HU4PIahqpdHtPGr6x3mzb2ft15p3TNNVJ+vtmVIAj4bR8wwfryaq0pC2xTfI/tlXX6Lt6hvlG65wkAAAAQcIYhXX21b1ZAx45mVwMAwP45ndL06dK8eWZXAgAhZ9YM5/I6lwmjBpenmVu7Bprb5PGbtHixVFYmnXaa2ZUgSGiMAyFWWe/WdyWOoI6xvqJGBenJyk9LCuo4AAAAQFR46infrLtRo8yuBACAA3v8cenCC6WMDLMrAYCQMgxD9npzGtT1Hq/q3V4lJbAQc6CEXVvc4ZDuuYcbz6Ic/wcDIWQYhlYUVYRkrK+LKuT2hnavFQAAACDifPqptGSJb59WAADCXXGxtGCBNGaM2ZUAQMjVebwyc5JxtStMl/5uofg4c/fPTjB5/L3ceKOvMZ6ZaXYlCCIa40AIba2sU9kBlnrp/N9X1GH+21Irm9q1bq9+Lqtu1TkAAACAqLZ9u3T33dK//iVZwuxDGQAAmnLHHdK990pxfKwLIPZ4TV5622OE3RznVomzWJRi4gz4DGsYLWo9d66UmysNHmx2JQiyMPqvDoh+68v33aiOq69T/1uvVV3bPHmSk3Xwc/9U8aChWj/2Yjmzclr0Qd2GihodmpuuOD7kAwAAABqrr5cuucS3jDpL0QIAIsHSpVJ8vNS/v9mVAIApzP6Y26Lo+5w9OzlRtVX1IR83Ic7cpnwjmzdLTz8tvf222ZUgBGiMAyFir3epvK6J/U+8XhUsWqieTz6kVdffppJBQyVJCZUOtf1ymY6+5Rq5U1K1+cy/qHjQUBmJiX6P6fR4VVRVr/YZyQH6LgAAAIAoYBjS1VdLV10lHXKI2dUAAHBgTqdvlZPXXjO7EgAwTYLJq2UkhtvS3wGQnWzVzqr6kO/3nZ2UKIvZdzpIksslXXGF9OSTktVqdjUIARrjQIhsc9TJIjUKmKTdu9T7oaky4uK07MlZqsvLb3jOnZGpnSeO0M4TRyhj7Rp1nfOSejz9qL54+GnVFrT3a0yLpK2VtTTGAaAVDMOQx5AkQ3EWC6twAEA0mDFD6tFDGj3a7EoAAPDPzJnSxRdL2dlmVwIAprHGxykpPk71ntZtQ9oSFkkZSdHXUuuQkaxVuytDPm7HzJSQj9mkyZOl8eOlgw4yuxKESPT9XwyEqfI6Z6OmeNvPP1Xvh6bppyuuU9GQk/a7N1TlwT303R33Kbl4p5xZ/v8CZEgqr21iljoAYJ88XkM7qupUWutUaa1Tjnp3o+t3WmK8clOsyklOVIfMFCXFh8myTwAA/7zxhrR+vfTPf5pdCQAA/tmwQfr8c+m228yuBABMl52cqKLq0C/9nWFNiMrJEunWBLVJsWp3rTNkY8ZbpMLMMJjMt3ChtGuXdO65ZleCEKIxDoRIxS/LqFvLStXziZlK2r1L/3tujjxpaX6foy6/XbPHrXF75PJ4lUjjBgD2q87t0YaKGq0vr5bLa+y1ysce1S6Paly12uKo1XclDhVmpujgnDRlJvm/1QUAwCQrVkjPPy+9/rr5GxQCAOAPw5Buukl68EGyCwAk5aQkqrg6tEt/WyTlpkTvMtsHZaeGrDFukdTJlmr6svgqLpamTmVf8RhEpwwIAafHK6fXUN7SxTrmmgkqP6yvvnjkmWY1xVuj0ukOyTgAEKm2Vdbqw427tKa0Si6v71er/f2Ctec5r6Qtjlp9tGm3Vu+ulNcI9Y5MAAC/bdsm3Xqr9MILUiI3MwEAIsSrr0r9+0vdu5tdCQCEhU6ZKSHfD9uQ1NkWJkt/B0H79GTlpiQqFLdfJcRZ1Cs3PQQj7YfXK11+ufTww1Jqqrm1IOSYMQ6EgNtrqPeD9yljwzp9Of1x1XboGNLxPTRqAKBJHq+hr4sqtLWyrsXn2HOFXV1apZ1V9TqmQ7ZSE+NbfD57nV2Vzkp1zNw7K7Y5tinDmiFbsq3F5weAaFPj8qi8zqnyOpcqnW65PYYsFt/+g1nJicpOTlS2u14JF10kPf00e7MCACJHWZn0zDPS+++bXQkAhI3UxATlpVq1q8YZsgZ5hjVe2cnRe3OtxWLRUQVZWrhxV9Df0755mUpKaPnnZgHxwAPSqFFSnz7m1gFT0BgHgq22Vkknn6zaoSO16uY7TCmBvjgA7M3jNbR0W1lAl4qy17u0aMtuDemUq7TE5v+YZa+za+TskSqpLtGi8YtUaCtseG6rfauGvjBUeWl5mj9mPs1xADHNMAztqKrXuvIqldb6tixqaguMbZV1srhc+sPNV6nm6htVUNhFGSGvFgCAFrrpJunvf5es0bt8LwC0xME56SqpKQvZeN2z02WJ8u0s0q0J6pOXqW9LHEEbo316kgozTZ55//HH0s8/S889Z24dMA1LqQPB5nTKYrNpw9gJppWQEBfdoQ0AzWUYhpbvKA/4/kmGpHq3V//bWqZ6t6fZr690VqqkukQbyjdo6AtDtdW+VdKvTfEN5RtUUl2iSmdlQOsGgEiyu8apDzbu0vId5Sr7pSku7WMLDMPQEffcrq2nnK4fDu2nBZt26Ysd5XJ6vCGrFwCAFnnnHalNG+mYY8yuBADCTn5akjqkJwd96W+LpOzkRHWJ4mXUf+ug7DT1yAnOMue5KYk6ul22uTcYbN8u3X+/9PjjUpTf6IB9ozEOBJvFIovLpUQTm9PpVhaHAIDfWlderaLq+qCc25BU6/Lo62J7s1/bMbOjFo1fpG7Z3Rqa459t/ayhKd4tu5sWjV/U5DLrABDtvIahH3Y5tGRrqWpcvpuPDrQwUq+HH1BFrz7aMeLUhmO3V9ZpwcZdKg5SDgAA0GoVFdLMmdKUKWZXAgBhq29+puJD8Jn7UQW2qJ8t/lu92qSrZ4D3AM9PTdJxHXND8ve1Ty6XdOml0pNPSmlp5tUB09EYB4ItM1MWl0tZJu1BkpIQJ2s8/6sDwB7VTrdW7Q7ujGtD0s6qem2vrG32awtthY2a48c9d1yjpvhvl1cHgFjh8Rr6bFuZfi6r9vs13V56VoqL04bzL2z0uCGp3uPV0m1l2myvCWyhAAAEws03S/feK6XExgxFAGiJ5IR4Hd0uK6hj9Mu3KTMpevcWb4rFYlHPNhkaXJij5Pi4Fs/Kt8jXgOybl6ljO2abv6rtbbdJEyZIhxxibh0wnd/dsh07dgSzDiC6WSzKMdxBX9plr2El5SSzDxVCj8xAOFtZ7JBxoCmGAfJNsUNub/MHK7QV6qUzXmr02EtnvERTHFGJzMCBeA1Dn+8oV0mN/9tfdJj/tmw//6Qfr71lv8etKLJrq6P5NzEBMAeZgZgwf76UkSEdd5zZlQARjcyIDe3Sk9W/wBaUc/duk6GuWalBOXckaJuapGFd2+qg7DTFN2PG/J4j26Un66QuvtebPuN+1iwpMVE66yxz60BY8Lsx3rt3b73yyisBHXzJkiUaPXq02rdvL4vFonnz5jV63jAM3XXXXWrXrp1SUlI0bNgwrV27ttExZWVlGjNmjDIzM5WVlaUJEyaoqqqq0THfffedBg8erOTkZBUWFmr69OkB/T6AAyooUKGj9IBLPQaaIaljZnKIRwXIDISvSqdbxTX1IbseOz3eFs0a32rfqrFvjG302Ng3xjbsOQ5EEzIDB7JqV2Wzlj1v8/lSdXj/ba28e6pf+8Z9tbNCFXWuAx4HwHxkBqKe3S5Nny79/e9mVwJEPDIjdnSypWpA+yzFWyytnpi2Z5Zzv/xM9QjwcuKRyBofp8PzMnVK9zz1zctUbkriPpvkFkmZ1gQdnJOmkd3ydEyHbGUkhcEWrwsXSh9+KE2danYlCBN+N8b//ve/6/LLL9ef//xnlZWVBWTw6upq9e3bV0888USTz0+fPl2PPvqo/vnPf2r58uVKS0vTiBEjVFdX13DMmDFjtGrVKi1YsEDvvPOOlixZossuu6zheYfDoeHDh6tz585asWKFZsyYocmTJ+uZZ54JyPcA+KVbN2Xu3K6sEC+7Yo2zqF06jXGEHpmBcLWpoibkq3dsqGjeMr1b7Vsb7Sm+qe5Kja7q0LDnOM1xRBsyA/tTXufU2nL/l0/PXrlChzz/T62Y9rCMBP8/hFlRVCFvqJYTAdBiZAai3i23SHffLaXG7gxFIFDIjNjSISNFJ3dtq9wU3+qpLf3sJzMpQSd2aaNuWexB/VuJcXE6KDtNQzq10Z8OztewLm00sEO2BrTP0sAO2RraKVd/OrhAw7q21WFtM5WaGG92yT5ffSXNnCk969tmC5Aki2H4/9v/xo0bNWHCBP3444/617/+pdGjRweuEItFb7zxhk4//XRJvrur2rdvr5tuukk333yzJMlutys/P1+zZs3Sueeeq9WrV6tXr1768ssv1b9/f0nS/Pnzdcopp2jbtm1q3769nnrqKf3tb39TUVGRrFbfRfH222/XvHnz9NNPP/lVm8PhkM1mk91uV2ZmZsC+Z8SQxYuljz/W5htv04oie8iG7ZGTpt5t+W82loTT9YrMMP/vAI0ZhqF31hXL1YKlzVtreNe2SrceuEGzzbFNQ2YNabyn+Lv/U2lNqf5Q83DD44svXKyOmR1DUDmiXbhcs8gMMqMpXsPQx5t2q9Lp9mulj6xV36n3zKla/sgzcqdnNHu8Pm0zdHAOs0KAfQmXaxaZQWZErTfekJYt880YByJcuFyzyIzYywzDMLS9sk5ry6tVXueSRdrv7xJ7ns+wJujg7DR1sqUozuxlvxEYy5f7bjabM0eyBWe5fQRGqK9ZzbpFomvXrvr44491xx136Mwzz9Thhx+uI488stFXoGzcuFFFRUUaNmxYw2M2m00DBgzQsmXLJEnLli1TVlZWQ4hI0rBhwxQXF6fly5c3HHP88cc3hIgkjRgxQmvWrFF5eXmTY9fX18vhcDT6AlrlsMOkzz5Tp8wUZSUlhmS2YlJ8nA5huReYiMxAuKlxeUxpiktSmZ9L9GZYM5SXlvdrU9xWKBUUKNfh0qLxi9Qtu5vy0vKUYW1+wwcIZ2QGmrKzql4OP5viGWvX6LAH79MX/3iyRU1xSfqptEoek3ICgP/IDESlHTukJ5+U7r3X7EqAqEJmxB6LxaKOmSk6oXMbndC5jQ7JTVdeqlUJcY0/kY+3WNQmJVEH56RpcGGOhnVpoy5ZqTTFo8WiRdLkydKrr9IUx16avcD/5s2bNXfuXGVnZ+u0005TQjOWp2uOoqIiSVJ+fn6jx/Pz8xueKyoqUl5eXqPnExISlJOT0+iYrl277nWOPc9lZ2fvNfbUqVN1zz33BOYbASQpJ0dyuWRxOHRUO5s+3rQ76EMeWWBTIsuDwGRkBsJJRb05+8daJFXUudQpM+WAx9qSbZo/Zr4qnZW/zghPTpacThXaCrX4wsXKsGbIlswP9Yg+ZAZ+b2NF9QFneEhS2qYN6vv3O/XlP56UK2vv991fLq+hnVV16ujH9RqAucgMRBWvV7rySunRR6WkJLOrAaIOmRG7spMTlZ3s29rUMAx5DEMeQ4qzSAkWiyw0waPTU09JS5ZIr70mpTNxEHtrVgr861//0k033aRhw4Zp1apVatu2bbDqMtWkSZN04403NvzZ4XCosLDQxIoQ8SwWqVMnqahIth49dFjbDH2/qzJow3WxpbC3OExHZiDc2OvdfjVYAs2QZPdzxrjka47vq/HN8umIVmQGfq/G5VZJjfOAx6Vs36Yj7r5NX01/VPW5bVo97oaKGhrjQJgjMxB1HnlEGjlS6tnT7EqAqENmYA+LxaIEi6X5M0UROZxO6frrpYICafZs9hTHPvl9HRg5cqS++OILPf744xo3blwwa5IkFRQUSJKKi4vVrl27hseLi4vVr1+/hmNKSkoavc7tdqusrKzh9QUFBSouLm50zJ4/7znm95KSkpTEHZoItFNOkT75ROrRQ92z01Tr9mpdeXXAh2mXnqR++cwkhLnIDIQjl9dr2tjO1oxdVSWlpQWuGCDMkBloyi4/muLJxUXqP+l6rbj/IdXltzvg8f4orXXKaxgsoQiEKTIDUefbb6WlS32z2vxgGIZq3B7Z69wNv98kxFmUmZSo9MR4Zj8Cv0FmADFkzRrpuuukyy+XzjjD7GoQ5vy+ZcLj8ei7774LSYhIvv0/CgoK9NFHHzU85nA4tHz5cg0cOFCSNHDgQFVUVGjFihUNx3z88cfyer0aMGBAwzFLliyRy/XrTK0FCxaoR48eTS47AgTNMcdIH34oyXeHWp+2GeqRE9ilPDpmJGtA+2w+yIPpyAyEI8PEbWNbtWXtzp1Su8A0fIBwRGagKRV1Lu3vJ9qk0t3qf+s1+uaeB1TTMXCzYQxJjnp3wM4HILDIDESV2lrpxht9e4vv53McwzBUXF2vZdvL9M66Yn2wYZc+31GuFUV2rSiya/mOCi3YuEtvrS3W/7aWaqujVl4zf/kBwgSZAcSILVt8W5L86180xeEXvxvjCxYsUMeOgV2+s6qqSitXrtTKlSslSRs3btTKlSu1ZcsWWSwWXX/99brvvvv01ltv6fvvv9e4cePUvn17nX766ZKknj17auTIkbr00kv1xRdfaOnSpbr66qt17rnnqn379pKk888/X1arVRMmTNCqVas0Z84cPfLII42WFgFCIj9f2r1b+uWHGovFot5tMzSoY46S4uP2+8Hf/lgkxVss6l9g09HtsmiKIyyQGQhH8SZeHxPiWjH2zp2+ZaCAKEVmoCnlda79bn1x1KTr9d3f7lVV14MCPra93v/tLwCEFpmBqHLbbdJNN0m/2294D8MwtMVeow837tLSbWUqqqqXaz933HoMQ7tqnPpyZ4XeX1+itWVVNMgR08gMIAaUl0sXXSQ9+6zE9gHwk6lbKnz11Vc64YQTGv685+I+fvx4zZo1S7feequqq6t12WWXqaKiQoMGDdL8+fOVnPzr3smzZ8/W1VdfrZNOOklxcXE666yz9OijjzY8b7PZ9OGHH2rixIk66qij1KZNG91111267LLLQveNApKUkuJbCtfhkHJzGx7OS0vSyV3b6qfSKm2sqJGnGb+0WCQVZqaoV5sMpSbGB6FoIHyQGWittMT4kO8vLvmu1enWVvzItWmTNGZMoMoBYgKZEflq3Z59PmctK1WcyynHIYcGfFyLpFq3eVtvAAg9MgOmeO8935JWp5zS5NN1bo++LrKrqLq+4bHm/C5T7/Hq+12V2lZZp6PbZbXu9xEADcgMIIzU10tjx0rTp0tdu5pdDSKIxTC4dfBAHA6HbDab7Ha7MjMzzS4Hkeyhh3wX6V/uEvw9t9erzfZaba+sU3mdq8kmeZxFsiUlql16srraUpSUQEMcv+J6ZT7+DsJXWa1Ti7aUmjJ2n7YZOril22f88Y/S22/vd3lFoKW4ZpmL93/f3l1XrHpP0w3qgk8WKHPtT/r5smsCPq5FUo/cdPVqkxHwcwORjmuWuXj/o0hJiXTeedI77/gmUfxOeZ1LS7eWyuU1Wn1jr0W+XyMGtM9Wu/TkAx4PBArXLHPx/iPqeb3SuHG+iSSjRpldDVop1NcsbhcEQum446RZs/bZGE+Ii9NB2Wk6KDtNhmGoyuVRjcsjr2EozmJRcnycMpISWC4dAFogMynRtLGzkls4tveXphDXfQAxZn87UHR8/y39dOX1powNAECrGIZ01VXSzJlNNsUr6lxasqVUXqP1TXHJN8vcMKTPt5frmA40xwEAUeK226QTTqApjhbxe49xAAHQubO0fr1fh1osFmVYE5SflqR26cnKT0uSLTmRpjgAtFBCnEVtU60K9VU0Mc6inGRry168Zo108MGBLQgAIkDyPlZFinPWK23rZtW0ax+UcQ1JSfGsyAQACJJ//lM69lipb9+9nqpxefTp1sA1xX/LkLR8R7nK61wBPjMAACH2yCNSaqo0YYLZlSBC0RgHQik/X0pM/HUGIAAgpLplpYZ0n3GLpM62VMW3ZPqh3S5NmSKddFLA6wKAcJeVnNjkjUwZ69equrCzvMl7z7IL3NgsrAYACIKffpLmz5euv36vpwzD0DfF9oAsn74vhiF9tbNcXnbVBABEqtmzfXk6ebLZlSCC0RgHQq2+XiovN7sKAIhJ7dKTZQ3hGrmGpC62FjRvvv9eOvVU6YwzfP8EgBiTlZTYZGMg77P/acew4C6Xl2k1b+sNAECUcjp9M9uefFKK2/vj2O2VdSqurg/qTbyGpEqnRz+XVQVxFAAAguTdd6X33pMef5wtB9EqNMaBUBsxQlq50uwqACAmxVksOiwvMyRjWSR1zkxp3t7mVVXStGnSTTdJr74qnXMOP+wDiEltUpvegiL/f5+oaMiJQRs3OzmxZat8AACwPx98IB19tNShw15PeQ1D35Y4QlbK6t1VqnN7QjYeAACt9umn0r/+JT33nMTWV2glGuNAqP30k29JdQCAKTpnpqhNSvD3Gk+Ms6hPc5vwF1/s+wF//nypfXD2zwWASJBhTVBOcuMbi1K3bZEnJUXepOSgjds1KzVo5wYAxCjDkJ59Vrr22iafLqqqV70ndFvuGZK2OGpDNh4AAK3y3XfSffdJL78sJSWZXQ2iAI1xINROO01avNjsKgAgZlksFh1ZYFNckGdiH1mQJWt8M37UWrRI6thRuuWWJpdXBIBY0+13Teqcb75Sab/+QRsv3mJRx4zgNd0BADFqxw7fz/fdujX59IaK6qDftLv3mDUy2GscABDu1q+XbrzRt7d4errZ1SBK8KkrEGrffCMddJDZVQBATEu3JujYjtlB+0Ho8LaZat+c5orXK91/v3TnnUGqCAAiT4eMFKUm/LpMXt6y/6lo6LCgjdc9J00J3JgEAAi0GTOkq65q8imX16uSGmdQ9xZvSo3LI3u9O8SjAgDQDDt3SpdeKs2aJeXmml0Nogi/9QOhdvTR0urVZlcBADGvbWqSju2YoziLAjpDo0/bDHXPSWvei158URo9WsrODmAlABDZ4uN8K3xIkgxD6Vs2yXHIoQEfxyIpLTFeh+YwAwEAEGAVFdLXX0sDBzb5tL3OvOZ0Rb3LtLEBANiv8nJp7Fjpn//0ra4IBBCNcSDUduyQunQxuwoAgKS8tCQN69JWWUmJBz54PyySkuLjNKhjjg5ubmOlvNy3T9IVV7SqBgCIRnlpSeqcmaK8z5ao/LB+UhC2wTAkHVWQpfi4UC9kCwCIeu+8I51wgpTW9I2zZjWnLZLsdTTGAQBhqLJSOu88afp06ZBDzK4GUYjGOBBq69ZJ3bubXQUA4Bfp1gQN6Zyr3m0ylNCCpohFUmdbik7u2lZ5aUnNL+D226UpU6TE1jXnASBa9c23ydK7t3K++1ppmzYE/vx5mWqTag34eQEAMc7jkZ5/Xrr++n0eUuV0h3x/ccl3U1ilk6XUAQBhprpaOvdc6e67pSOPNLsaRCka40CorV3LHuMAEGbiLBb1yE3XKQfl64h8mzKtCY2et/zma4/khDj1zE3XqIPydGRBlqzxLfixaskSKSFBOvbY1pQPAFEtIc6io/v31oY779VRd9ys+JqagJ27d5sMHZTdzO0vAADwx+rVvhUD97NdkscI9e7iv3J7zRsbAIC91NVJ558v3XrrPrcgAQIh4cCHAAgYw5BqaqTUVLMrAQA0ISHOoq5ZqeqalSqX16uKOpfsdW65vF4ZkuItFmUmJSg7OVHJCfGtG6y+XrrnHmnu3IDUDgDRzBofp35/OllbirbpqEnX64tHnmnxuSzy3RB1RH6mOtn4uRwAECTTp0vXXGN2FQAAhD+nU7rgAmniRGnIELOrQZSjMQ6E0oYNUrduZlcBAPBDYlyc2qYmqW1qC5ZH98edd/o+KLPZgnN+AIgyiXFxOuiKCXJs36I+D03V9zdMatbrLfItHZuTkqij22UrNbGVNzgBALAv27dL27ZJ/frt97DEOPMW80xsyYpXAAAEmtstjR/v+xo+3OxqEAP4CQgIpUWLpMGDza4CAGC2xYul8nLp9NPNrgQAIk7mvffooDqHBn3whtJ+09xuao/W3z5WkJ6kwYU5Or4wl6Y4ACC45s2TRo+WEhP3e1iGNUFmLGhukWRLYr4UAMBkHo80YYJ01lm+3ARCgJ+AgFB6/33p2WfNrgIAYCa7XZo8WXrrLbMrAYCIFff888o78UQN791D5ccOVnmdS+V1LtnrXfJ4DVksvuXXs5Otyk5OVG6KlWY4ACA0XC7p5ZelTz894KFZyftvnAeLISkryZyxAQCQJHm90pVX+maJn3222dUghtAYB0KlosJ3sWfJXACIbdddJ913n5SRYXYlABC5LBbp1VdlGT1aOS+8oJwePcyuCAAAn+XLpWOPleIPfENWpjWhYauPUDOrKQ8AgLxe6eqrpQEDpDFjzK4GMYal1IFQeeklLvIAEOuee07q2lU67jizKwGAyNe2rfT449JVV/m2pwAAIBzMnCldfLFfh8bHWdQxI7nJ7UCCKSspUelW5ksBAEzg9fp+h+vf37eMOhBiNMaBUPB6pTfflP70J7MrAQCY5dtvfcun33GH2ZUAQPTo31+66CLfahwej9nVAABi3Zo1Um2t1Lu33y/pmpUW8hnj3bJTQzwiAADy9UmuuMI3U9zPm8iAQKMxDoTCf/8rjRolJbJMFQDEJLtduuEG6V//8mtJRQBAM1xwgdSunTR9utmVAABi3euvS3/+c7NekpuSqIxfllQPhcRfZqkDABBSXq90+eW+7UYuusjsahDDaIwDweZySU8/LV15pdmVAADMYBi+DLjvPt+yvwCAwJs61TdLb/16sysBAMSq+nrp3Xeb/WG/xWLRkQW2kM0a75uXqYQ4PhIGAISQxyNdeqk0eLB04YVmV4MYx09BQLA99JA0fryUyjJVABCTpk2TjjnGd0csACA44uKkc86Rbr7ZNxMBAIBQ+/BDacQIydL8ud+5KVZ1ywru50YWSXmpVhVmpgR1HAAAGtnTFB86VBo3zuxqABrjQFCtXSt9+qk0dqzZlQAAzDBvnrR5s3TNNWZXAgDR75RTpB49pG+/NbsSAEAseuop6fzzW/zy3m0zlBmkJdUtkqzxcTqyIEuWFjTuAQBoEY9HmjBBOvFEeiQIGzTGgWCpqfEtnfvEEy26W9gfHq+hslqnSqrrVVxdr9Jap5weZsgAQFj49lvp//5PevTRoOUAAOB3hg+XFi40u4pGvIahaqdblfVuVTndcvHzOgBEn6+/9q0U2L17i0+RGBen4wpzlJoYH9DmuEVSQpxFg345NwAAIeF2Sxdf7Psd7YILzK4GaJBgdgFAVKqt9d0BdfvtUmFhQE9tr3dpU0WNSmqcqnS6mzwmNSFebVKt6mxLUZsUK3cDA0CoFRdLN94ovfqqZLWaXQ0AxI4//EH65z9NLcEwDJXWurStslZltS7Z61177RubmhCv3JRE5aUlqWNGiuLj+HkdACLaq6+2arb4HikJ8RrSKVefbStTRX3Tn/m05JzHdsxWZlJiQM4HAMAB1df7tpc980zflldAGKExDgTDW29JfftKw4YF7JS7auq1eneVdtc6ZZH2+nDtt2rcHm111GqLo1YZ1nj1yElXYWYKDXIACIXaWumii6THH5dyc82uBgBiS3q6VF1tytCGYWiLo1Zry6rlcLr3+zN7jduj2kqPtlbW6bsSh7plpap7drqSEljUDQAiTm2ttGyZNHVqQE6XnBCvoZ3baE1plVaXVh3wM6Cm7HlNF1uKDs/LVEIc+QIACJHaWmnMGN8S6qeeanY1wF5ojAOB5nb7ls19553AnM5raNUuh9ZX1DQspeXPL0R7jql0evRVkV1bK+t0VIFNyQksmwUAQeNy+VYMuekmqWdPs6sBgNiUkSFVVfma5CFS4/Loq53l2l3ranjsQD+z73ne5TX0c1m1NlbUqH+7LBWkJwetTgBAEPz739Jf/hLQ7ZPiLBb1bJOhdunJWlNWpe2VdX41yPcc0zbVqkNy0pWXlhSwmgAAOKDKSum883yrKJ54otnVAE2iMQ4E2j33SJdcImVnt/pU1S63Pt1apmqXR1Lz7xD+rZLqei3YuEsDO+SoTSrL+gJAwHk8vrthL7hAOukks6sBgNjVvbu0bp3Ur19IhttRWacvd1bIa7T8p3VDktNr6LPt5eqWlaq+eZms9gQAkWLWLOk//wnKqbOSEzWgfbZqXB5tttdod61T5XUuub2NMyfO4ju2TYpVnTJTWDYdABB65eW+pvhdd0nHHmt2NcA+hf06Ol26dJHFYtnra+LEiZKkoUOH7vXcFVdc0egcW7Zs0amnnqrU1FTl5eXplltukdsdmH16gEbWrZOWLpXOPbfVp6pxubV4c6lqfmmKt5Yh32yUT7eVaneNMyDnBMINmQHTGIZ07bW+LTROPz1ApzTk8nrl9Hjl8bbm1igATSEzolhBgVRcHJKhtjlq9fmOcnkMo1U3sf7WhooafbmzQkYrGu0AAovMwD698YZ09NFS+/ZBHSY1MV4922RocGGuRnfP18hubXVSlzY6qXMbDe/aVqcdXKChndrosLaZNMUBk5EZiEklJb69xO+/n6Y4wl7Yzxj/8ssv5fH82hj84YcfdPLJJ+vPf/5zw2OXXnqppkyZ0vDn1NTUhn/3eDw69dRTVVBQoM8++0w7d+7UuHHjlJiYqPvvvz803wRix9//Lt1wg5SS0qrTOD1e/W9rmeo93oB9wLaH15CWbivTCZ1z+WUJUYfMgGn++lfp0EOlceNafArDMFRcXa+SGqdKa52y17v02364NT5OOcmJyklJVIeMFGVYw/7HOCCskRlRLCtLqqgI+jAl1fX6cmdwxtlWWSdrvEP98m1BOT+A5iEzsE8ffSTdcktIh7RYLEpN5HcBIFyRGYg527f7Pg977DGpVy+zqwEOKOx/imrbtm2jP0+bNk0HHXSQhgwZ0vBYamqqCgoKmnz9hx9+qB9//FELFy5Ufn6++vXrp3vvvVe33XabJk+eLKuVJaURIJ99Jjkc0ujRrT7V9yUO1bg8AW+K7+E1DH25s0Indm7DEo2IKmQGTPHAA1JqqnTNNS16udvr1SZ7rdaVVanG7d3n3oFOj1dF1fUqrq7Xj7urlJ+apO45acpn30CgRciMKJaaKtntQR3C6fHqi53lQft5XfLNHM9PS1I79hwHTEdmYJ/KyqTcXLOrABBGyAzElI0bfdsKPvOMb0srIAKE/VLqv+V0OvXyyy/r4osvbtTMmz17ttq0aaPDDjtMkyZNUk1NTcNzy5YtU58+fZSfn9/w2IgRI+RwOLRq1aomx6mvr5fD4Wj0BeyX0ylNmSI99FCrT7Wrpl6bHbVB/ZDNkGSvd2tdeXUQRwHMRWYgJJ5+2rdc1B13tOjlZbVOLdy4W9+VOFTj9kpquin+W3ueL6mp19JtZfpiR7mcHm+LxgfgQ2ZEGbdbSgzuykjflzjk9AR/qfOvi+xycY0HwgqZgUYqK6W0NLOrABCmyAxEtR9+kC65RJo1i6Y4IkrYzxj/rXnz5qmiokIXXnhhw2Pnn3++OnfurPbt2+u7777TbbfdpjVr1mju3LmSpKKiokYhIqnhz0VFRU2OM3XqVN1zzz3B+SYQnd5/X+rbV+rUqVWnMQxDXxcFd3bLb63aXalOtlQlxUfUPTKAX8gMBN2//y2tWOFrjjdz9Q3DMPRTaZVWl1appet27GnHbK+s064apwa0z1abVO4cB1qCzIgydXVScvBmWZfVOrXZURu08/9Wvcern0qr1CcvMyTjATgwMgONeL3N/l0AQOwgMxC1li6V7r1XmjNHatPG7GqAZomoxvizzz6rUaNGqX379g2PXXbZZQ3/3qdPH7Vr104nnXSS1q9fr4MOOqhF40yaNEk33nhjw58dDocKCwtbXjiim2FIM2ZIr7/e6lPtrnWq2uU58IEB4jWkrfZadc/h7mZEHzIDQfXOO76vF19sUVP82xKHNlT47ghv7XxDQ77GyafbSnVshxzlsbQ60GxkRpTZtk0aOjRop19fXr3PbS+CYWNFjXq1yVB8HI0XIByQGWhQXe3bvgMA9oHMQFR6913f0umvvSZlZJhdDdBsETNNdPPmzVq4cKEuueSS/R43YMAASdK6deskSQUFBSouLm50zJ4/72sfj6SkJGVmZjb6Avbpvvuk886TfncnX0tsrKhp8czBllpfUS3DCNXHekBokBkIqkWLpGeflZ5/XoqPb/bLV5dWNTTFA8lrSMu2l6m8zhXwcwPRjMyIQps3S507B+XUTo9X2yvrQtYUlyS3YWh7ZWhmqAPYPzIDjXz1ldSvn9lVAAhTZAai0ksv+VZQfPVVmuKIWBHTGH/++eeVl5enU089db/HrVy5UpLUrl07SdLAgQP1/fffq6SkpOGYBQsWKDMzU7169QpavYgRGzdKH30kXXRRq0/lNYyQf8gmSdUuj+z17hCPCgQXmYGg+fZbafp06eWXJWvzly0vqa7XT6VVQSjMx2tIy7eXy+PlhifAX2RGFNq5U/rl7ynQtlfWyYwdv0O1dDuA/SMz0MgHH0gjRphdBYAwRWYg6jz0kLR8ufTCC1ISqxUickVEY9zr9er555/X+PHjlZDw6+rv69ev17333qsVK1Zo06ZNeuuttzRu3Dgdf/zxOvzwwyVJw4cPV69evTR27Fh9++23+uCDD3THHXdo4sSJSuJ/XrTWAw9I11wTkKWzHPXukDfF96ioZ3YhogeZgaB6/33piiuktOZvQeHxGvq6yB6Eon5lSKpxe7S6tDKo4wDRgsyIQoYhuVxSXHB+1S2vc4V8hSdJKq91scoTYDIyA40YhvTll9KRR5pdCYAwRGYgqhiGNGmSZLdLjz3WotUTgXASEY3xhQsXasuWLbr44osbPW61WrVw4UINHz5chx56qG666SadddZZevvttxuOiY+P1zvvvKP4+HgNHDhQF1xwgcaNG6cpU6aE+ttAtFm5UtqxQzrrrICcrsKkpW8tJo4NBAOZgaD64ANp+PAWvXRNWZVq3J4AF9S0n8uq5eCmJ+CAyIwoVFkZ1CX9yuucptzM6jaMkGUIgKaRGWjk88+lo44K2o1YACIbmYGo4XZLl18udewoTZ4sWcy4TRgILIvBbecH5HA4ZLPZZLfb2Z8DPm63dOaZvuVDDjooIKf8vsShdeXVpnzQ1ibFquM75ZowMgKN65X5+DuIYh9/LL32mvTUU81+qcdr6L31xXKFaIlzi6SuWanql28LyXiIXFyzzMX7HwT33Sf16uX7WT0I5v28U2btVjGwQ7bapSebMzggrllm4/0PM2PGSFOmBOwzISDacM0yF+8/AqK2VrrwQumMM6RzzzW7GkSxUF+zuK0RaImPPpK6dg3oL0BuE+9RcXvN2CkRACLMiy+2+BeBnVV1IWuKS74l1Tfba7m+A4g9y5b5PrgJAq9hmNYUlyS3mYMDAH71wQdSQQFNcQBA9Nq923ez8aWX0hRH1Ek48CEA9nLvvdIrrwT2nHzOBQDha906qahIGjKkRS/fZK8JcEEH5jEM7ayqV2FmSsjHBgBTrFolZWdH7fJ+/LoAAGGgokJ64AHpN8seAwAQVTZskC65xLdabt++ZlcDBBwzxoHmmj5dOu00qVOngJ42Ic68D/AS2BMLAPbNMKQHH5TuuKOFLzdUVhf6/b4tkspqnSEfFwBM89hj0t13B+30Zrfb480uAABinWFIV1whTZ0qpaWZXQ0AAIH35Ze+WeIvvEBTHFGLGeNAc2zeLL3zjvTeewE/dYY1wZRZIBZJmUlcCgBgn9av980WHziwRS+vdXtMWf7WkFRuQkMeAExRW+tb3aN796ANYbFYlJYYr2qXJ2hj7E+6lZ/ZAcBUjz4qHXOMNGCA2ZUAABB477wjPf209PrrUlaW2dUAQcNv1kBzPPigdPPNUnp6wE9tS04M+Dn9YUjKMmlsAIgIt9wi3XWXFB/fopfb690BLigyxgaAkProI2nYsKAvo56TnKgalyfkN7TGyXcjLQDAJMuXS8uWSf/+t9mVAAAQeE8/LX32mfTf/0pJSWZXAwQVv1kD/vrmG8njkf70p6CcPtOaIIvM2TswK4nGOAA06b33fHfJHnlki0/h8ngDV08zeQxDhmHIEqX77QJAg2eekWbPDvowWcmJ2lpZF/Rxfi8jKUFxXMsBwBxlZdKkSdLcuUG/AQsAgJAyDN/WgV6v9PzzEluuIgbwXzngr0cfla67Lminj4+zqH1Gcsj3LkxLjJeNpdQBYG8ul/Tww74Z461gxg1PABBTPv1U6tJFysgI+lDt0pODPkZTOmakmDIuAMQ8w5Auv1yaMYNlZQEA0cXplC6+WOrYUZo6laY4Ygb/pQP+WLdOcrulHj2COky3rNSQN1C6ZaUykxAAmvLOO1LPnlKvXq06TbyJ11iLxDUeQPR77DHptttCMlS6NUFtUqwhvZnVIqmzjcY4AJjiH/+Qhg6VjjrK7EoAAAic8nLp7LOlM86QrrzS7GqAkGKaKOCPyZOlv/416MO0SbEqLTFe1S5P0MeSpDiL1CkzNSRjAUBEqamRpk+XFi9u9anSrC3bmzwQUhPNGxsAQmL1aik9XerQIWRDHpSdqt21zpCMZZHUPj1ZyQlczwEg5JYu9W2r9/LLZlcCAEDgrFvnWw1l+nRu/EJMYsY4cCDz50vt2rV6xqA/LBaLjsi3BX2cPXq3yVBSApcBANjLyy9Lf/mLZLW2+lSZ1sQAFNR8FknZyeaMDQAh8/e/t3rLi+Zqn56snOTEkMwat1ik3m2Dv0Q8AOB3du+W7rxTeuop9hUHAESPJUt8M8RfeIGmOGIWM8aB/dm923fn1Ntvh2zIvLQkdcpM0VZHbdCWVbdIykxK0EHZaUEaAQAimNcr/etf0gcfBOR08XEWZVgTVOl0B+R8/jIkZdEYBxDNFi6UCgqkQw8N6bAWi0VHtcvSwo27gj5WrzYZSrfyazsAhJTHI11yiTRzppSZaXY1AAAExosv+rYNfOMN36pbQIziN2xgX9xu6eKLpYcektJC20A+PC9Tu2udqnV5gtIcj7NYdHS7LMVx1zMA7G3SJGniRCknJ2CnbJ+RrJ9Lq4J2w9O+tEtLDvGIABAi9fXS1KnSvHmmDJ9hTdBhbTP0/a7KoJx/z6of3bmRFQBC7847pbPOkvr1M7sSAABaz+uV7rrL9zvUv/8txbNNE2IbaygDTXG7pQkTpHHjpL59Qz68NT5OgzvmyBofF/AlGuMs0rEds5WZxCxCANjLzz9Ly5dLF1wQ0NN2saWEtClukZSbkqiMJO6BBBCl7r1XuvpqKcO8Zca7Z6cFpXFtkZRmjdexHXO4kRUAQm3ePKmyUho71uxKAABovdpaX6Z17izNmEFTHBCNcWBve2aKjxolnX22aWWkWRM0tFOuUhMDE1YWSQlxFh3XMUdtU5MCck4AiDpPPSVdd52UENiGclpigvJSrSHZj1byLaPeLYtZhgCi1IoV0ubN0hlnmFqGxWJRn7YZOjjAzfHMpAQNKcyVNZ5f1wEgpNaskZ5+WvrHP8yuBACA1isq8v3OdPHF0qWXml0NEDb4TRv4Lbdbuugi6dRTpXPPNbsapVkTdFKXtupmS231udqmWnVy17Y0xQFgXzZvlnbvDlqj5bC2mSGZNW6RZEtKUIcMllEHEIWcTum223zbHYUBi8WiPnmZOqZDthLjLC2+AWrP6w7JSdPQTm2UlMBMDgAIqaoq6aqrpGeflaxWs6sBAKB1vv9eGjNGevhh6aSTzK4GCCusrwns4XZLF14o/elP0jnnmF1Ng4Q4i/oV2NQ+I1mrSytVWuuSRTpgc2XPMemJ8eqRm65OmSmysBQjAOzbf/7j+6UhSLKSE3VITpp+LqsO2hh7HFWQxfK7AKLTffdJV14ptWljdiWNtE9PVm7XtvphV6W2OGr9vhFqz8/s2cmJOqxtptqk0owBgJAzDOnyy6W775batze7GgAAWuett6RnnpHmzAm735uAcEBjHJAkl8vXFD/9dOnPfza7miblpSUpLy1JFXUubbTXaFdNvaqcniaPTUmIU26KVV1sqWqbaqUhDgAH4vVKCxZIN98c1GF65mZoR1Wdqp2eoM0ePzQ3XVnJiUE6OwCY6OuvpfXrpSlTzK6kSUkJ8TqqXZYOa5uhjfYabXXUqnIfP69LUlJ8nPLTknRQdpqyuW4DgHlmzpSOPlo6/nizKwEAoOUMQ7r/fmnbNmnuXFZAAfaBxjjgcknjx0tnneX7CnNZyYk6ItkmSXJ7vXLUu+XyGjJkKMESp4ykBCWxHyEANM8HH0gnnCDFB3fp2vg4iwZ1zNWizbtV7/EGvDneKTNZh+amB/isABAG9iyh/sorZldyQEkJ8To0N0OH5mbI7fWqos6tapdbHsNQnCxKSohTVnKiUlguHQDMt2iRtHKl9OKLZlcCAEDL1dRIl10mHXus9Ne/SkyUA/aJxjhiW12dNHasdP75QdtTNpgS4uKUk8KdXwDQKobh23NpzpyQDJeaGK8hnXO1ZEup6tyBa453ykzRUQU2VgkBEJ3uv9+3zG3btmZX0iwJcXFqk2pVG/EzOwCEnW3bfFt0vPkmDQQAQOTaskW65BLp9tulE080uxog7DGtFLGrqsq3l/gll0RkUxwAECCvveabLZ6VFbIh0xITNKxLW3XMTG7VeSyS4i3SkQU2muIAotfKldLatdLZZ5tdCQAgWtTWShMmSE89JaWlmV0NAAAt8+mn0sUXS//8J01xwE/MGEdsKi+XzjtPuuMOadAgs6sBAJilpER65hnp3XdDPnRifJyObpet9um1+r6kUjVujyySXzPI9xyXn5akvnmZSrPyIx2AKOV0SrfeKr38stmVAACihWFIV14p3XSTdPDBZlcDAEDL/N//SQsWSG+8IWVkmF0NEDH4FBWxp7hYGjNGmj5dOvJIs6sBAJjF7fYty/vgg1JSkmlldMhIUfv0ZBVX12tdebV21zjl/eW5387/3tMwt8ZZVJiZooOy05ROQxxAtJsyxbdXXl6e2ZUAAKLFgw9Khx8uDR9udiUAADSfyyXdfLNks0n//rcUx8LQQHPwaSpiy5Yt0oUXSk88IfXsaXY1AACzGIZ0xRXS+edL/fqZXY0sFosK0pNVkJ4sr2HIUe9WeZ1L9R6vDMNQvMWiNGu8spOtSkmIY8l0ALFh2TJp+3bf/q8AAATC++9LP/3km2UHAECkKS2VLrpIGjtW+vOfza4GiEg0xhE71q71zQx89lmpa1ezqwEAmMUwpNtuk446Kix/iYizWJSVnKis5ESzSwEA81RVSX/7mzR3rtmVAACixZo10iOPSPPmSRF2o6nT41V5nUvldU7Z691ye7wyJCXGxSkzKUHZKYnKSbbKGs+sQQCIWl9/7ZspPnNmWEzyACIVjXHEhq+//nVvwvbtza4GAGAWw5D++lepTRvfvoIAgPB0yy3SHXdIWVlmVwIAiAYVFdJVV/k+F0pONrsav5XVOrWuvFrbK+tkyLfVkvG7Y3ZU/fpYh/Rkdc9OU05KIqtMAUA0eeEF317ir70m5eaaXQ0Q0WiMI/otWCA99JAvNLKzza4GAGAWw5Buv11q29Z3hy0AIDy9+66UkiKdeKLZlQAAooHH41t2dto0qV07s6vxS7XLrRU77dpd62zUDP99U/z3j+2oqtP2qjrlJCeqf7sspVv56BcAIprTKd10k5SZKb3+uhQfb3ZFQMTjpyNEt9mzpbff9oVGSorZ1QAAzLJn+fT8fN8vFACA8LR7t++m1rffNrsSAEC0uP126ayzpKOPNruSAzIMQ1sctVpZ7JDX8LW8m2qG7/P1v/yzvM6lhZt26fC8THW1pTJ7HAAi0Y4d0oQJ0mWXSWecYXY1QNSgMY7o9Y9/SBs2+Jrj3EkFALHLMHzbabRvL91wg9nVAAD2xTCka66RZszgplYAQGC89JJvP/ELLjC7kgMyDEOrdlfq57Lq1p9LvlhdWexQpdOtw9tm0hwHgEjy6afSXXdJTz4pHXqo2dUAUSXO7AL2Z/LkybJYLI2+Dv3NRaCurk4TJ05Ubm6u0tPTddZZZ6m4uLjRObZs2aJTTz1VqampysvL0y233CK32x3qbwWh5PX6ZgPW1UmPP05THIgRZAaaZBi+ZdM7dKApDqABmRGmXnpJ6tNHOuIIsysBgAZkRgRbvty3H+vUqWZX4pefSqsC0hT/vfXlNfphV2XAzwtgb2QGWs0wfD2NJ56Q3nyTpjgQBGE/Y7x3795auHBhw58TEn4t+YYbbtC7776r1157TTabTVdffbXOPPNMLV26VJLk8Xh06qmnqqCgQJ999pl27typcePGKTExUffff3/IvxeEQH29dMkl0tChvmVGAMQUMgONGIZ0441Sly7SddeZXQ2AMENmhJnNm6V//5sl1AGEJTIjAm3eLP3tbxGzH+v2ylqtLq0K2vnXllfLlpSgTrbUoI0BwIfMQIvV1EgTJ/qa4a+84lvxBEDAhX1jPCEhQQUFBXs9brfb9eyzz+qVV17RiSeeKEl6/vnn1bNnT33++ec65phj9OGHH+rHH3/UwoULlZ+fr379+unee+/VbbfdpsmTJ8tqtYb620EwlZdL48ZJl18u/fGPZlcDwARkBhp4PNJVV/lmHV5xhdnVAAhDZEYY8Xh8HwA99piUEPa/ogKIQWRGhHE4fJMl/vUvyWYzu5oDcnq8+qbIHvRxVpY4lJeWpOSE8L9RAIhkZAZaZONG317it94qnXyy2dUAUS2sl1KXpLVr16p9+/bq1q2bxowZoy1btkiSVqxYIZfLpWHDhjUce+ihh6pTp05atmyZJGnZsmXq06eP8vPzG44ZMWKEHA6HVq1atc8x6+vr5XA4Gn0hzG3YIJ11lnT33TTFgRhGZkCS5HJJF14oDR5MUxzAPpEZYWTqVOnss6Xu3c2uBACaRGZEELfb97vAvfdKXbuaXY1fvi9xyOU1gj6Ox2vouxL+OwKCjcxAs739tm+y37/+RVMcCIGwbowPGDBAs2bN0vz58/XUU09p48aNGjx4sCorK1VUVCSr1aqsrKxGr8nPz1dRUZEkqaioqFGI7Hl+z3P7MnXqVNlstoavwsLCwH5jCKzPPvPdTTVrltS/v9nVADAJmQFJUm2tdN55vpulLrjA7GoAhCkyI4wsWyatWSONH292JQDQJDIjwtx0k/TnP0sDB5pdiV9qXB5tdtQq+G1xyZC0rbJOlU72KgaChcxAs7jd0m23SR984GuOd+lidkVATAjrdepGjRrV8O+HH364BgwYoM6dO+vVV19VSkpK0MadNGmSbrzxxoY/OxwOwiRc/ec/0quvSnPnSpmZZlcDwERkBlRZKZ1/vnTttdxhC2C/yIwwYbf79n994w32zwMQtsiMCPL441Juru9G2QixyV4jixSSxrgkWSRtqqhRnzw+QwOCgcyA33bskC69VBo7Vjr3XLOrAWJKWM8Y/72srCwdcsghWrdunQoKCuR0OlVRUdHomOLi4oY9PAoKClRcXLzX83ue25ekpCRlZmY2+kKYMQzpvvukpUt9jXH+jgD8DpkRY8rLfTNDbr+dpjiAZiMzTGAY0tVX+36mj4D9XwFgDzIjTL3/vvTll9Kdd5pdid8Mw9DGipqQNcUlXwN+k71GXiOUowKxi8xAkz76yNcQ/8c/aIoDJoioxnhVVZXWr1+vdu3a6aijjlJiYqI++uijhufXrFmjLVu2aOAvyyUNHDhQ33//vUpKShqOWbBggTIzM9WrV6+Q148AcTqlSy6RMjKkRx+VEsJ64QMAJiEzYkhxsXTOOdL990vHHWd2NQAiEJlhghdflA45RDr2WLMrAYBmITPC0A8/SI89Jj39dEStQFLpdKve4w35uC6vIXs9y6kDoUBmoBGvV7r3Xunll6W33pIOPdTsioCYFNYdxZtvvlmjR49W586dtWPHDt19992Kj4/XeeedJ5vNpgkTJujGG29UTk6OMjMzdc0112jgwIE65phjJEnDhw9Xr169NHbsWE2fPl1FRUW64447NHHiRCUlJZn83aFFysp8+w9edpk0erTZ1QAII2RGjFq3zrf01JNPSj17ml0NgAhBZphs7VrptdekN980uxIAOCAyI8wVFfm2UpozR0pONruaZjGzOV1R51J2cqJp4wPRiszAPu3e7fv86pRTpDvuiKgbuYBoE9aN8W3btum8885TaWmp2rZtq0GDBunzzz9X27ZtJUkPPfSQ4uLidNZZZ6m+vl4jRozQk08+2fD6+Ph4vfPOO7ryyis1cOBApaWlafz48ZoyZYpZ3xJaY9Uq6ZprpJkzpX79zK4GQJghM2LQihXSrbdKL70kdexodjUAIgiZYSKn07eE+v/9nxQfb3Y1AHBAZEYYq631TZ54/HHpl7+PSFJR5wrp/uJ7WCTZ610hHhWIDWQGmrRsmfTXv/r6GkccYXY1QMyzGAabyhyIw+GQzWaT3W5nfw6zvPmm9Mwz0vPPS3l5ZlcDhC2uV+bj7yBEFiyQHnpImj1bys42uxogYnHNMldMvv+33iodc4x05plmVwKgmWLymhVGeP9/x+ORxozxbbU3bJjZ1bTIFzvKtb2yLuSNcUlql56kgR1yTBgZsYJrlrl4/8OEYUgPPywtX+7b7sNmM7siICyF+poV1jPGARmGdN990o4d0htvSFar2RUBAMz2739L8+ZJr78upaSYXQ0AwF8LFkgOB01xAEDr3XyzbznaCG2KS5LXCP1s8T08XuZJAUBQlZZKl18uDR7s+xyLpdOBsEFjHOGrutq378bgwdKdd5pdDQAgHDz0kG9v2ldeYQleAIgkO3ZI06ZJb79tdiUAgEj30ENSbq40bpzZlbRKnIk9kngzBweAaLdkiTR5sjRjhnTUUWZXA+B3aIwjPG3e7FsO629/k4YONbsaAIDZvF7p9tultDTpiSe40xYAIonbLV12mW8P2NRUs6sBAESy116TfvpJ+uc/za6k1ZIS4kzbYzwpPi7EowJADPB4pL//XVq/3rc1bEaG2RUBaAI/BSH8LFkiTZjg21OcpjgAoK5OGj9e6t5duvtumuIAEGmmTJHOPVfq2dPsSgAAkezTT6X//CdqbpTNSko0ZSl1Q5ItKdGEkQEgiu3YIZ1+ulRYKM2aRVMcCGPMGEf4MAzp0Uel5ct9e8emp5tdEQDAbLt3+5ZIvPZaaeRIs6sBADTXggVSUZGvOQ4AQEv99JMvS+bOlRKi4+NMM5vTWck0xgEgYN5/X/rHP6THHuNmYCACRMdPkoh8VVXSlVdK/fpJs2dHxZ2/AIBW+vln6YorpIcflg4/3OxqAADNtWOH9MAD0ltvmV0JACCSFRVJV10lvfJKVE2iyExKUGKcRS5vaOeNx1sszBgHgEBwOn1bwdbUSG+/LaWkmF0RAD+wlDrM9/PP0mmn+ZZPv+kmmuIAAOl//5Ouvlp6+WWa4gAQidhXHAAQCFVVvhWknnxSKigwu5qAirNY1NmWqlB+CmaR1CkzRQlxfPYGAK2yYYM0erQ0YIBviw+a4kDEYMY4zPXGG769xF98UerQwexqAADhYPZs35Yac+dG1YwQAIgp99wjnXeedOihZlcCAIhUbrc0frx0111RmyddbalaV14dsvEMSV2zuGENAFrltdekZ5+V/vlPqWtXs6sB0Ew0xmEOt1u64w6ptlZ6803JajW7IgCA2QxDuvdeqaxM+s9/pPh4sysCALTEhx9KJSW+azoAAC1hGL4VpM49Vxo0yOxqgiYjKUHt0pJUVF2vYC+obpHUJtXK/uIA0FKVldINN0ht2/qWTk/kegpEIpZSR+iVlEhnnin16SM98ghNcQCAVFfn21IjK8u3pzhNcQCITDt2SNOn+67lAAC01N13S716SX/+s9mVBF3ffJviQrCtoMUiHZlvC/o4ABCVvvjCtx3sBRdIU6fSFAciGDPGEVqffy799a++D8rYMxYAIEk7d0oXXyxde600apTZ1QAAWsrtli691LevOHvsAQBa6rHHJI/H9/tBDEhNjFefvAytLHYEdZzebTKUZuWjYABoFo9HmjZN+vFH6fXXpexssysC0Er8NITQ8HqlmTOlr7/27Stu4w5VAICkFSukW26RnnwyavcNBICYMWmSby9YrucAgJaaM0f64Qffvq0xpKstVeW1Lm121Abl/IUZyeqenRaUcwNA1Nq8WbrqKt/qJX/9q2/pDQARj8Y4gq+0VLr8cunEE6XZswkQAIDPnDnSv//NHbcAEA3++1/fbIpzzjG7EgBApFqwQJo7NyY/O7JYLDqiwCavYWhrZV1Az90+PUlHtcuSJcbeUwBolf/8R5o1y7caVvfuZlcDIIBojCO4li6V7rxTevBB6cgjza4GABAOvF7prruk6mpfIyWBH0cAIKKtXi09/7w0b57ZlQAAItWXX0qPPBLTvx/EWSzq3y5L6dYqrS6tkkWS0cJz7XntITlp6tUmIyR7mANAVHA4pOuukwoLpbffZi9xIArF5k+aCD6vV5o+3bf81bx5Umam2RUBAMJBZaV0ySXSiBG+fcUBAJGtslK6+mrp5Zf50AgA0DJr1viWqH3tNSk52exqTGWxWNSzTYYK0pP0xY4KVbs8LWqQpyTE6+j2WcpNsQajTACITsuWSX/7mzRlijRokNnVAAgSGuMIvF27fEunDx8uvfRSzC1/BQDYh40bpUsvlSZP5hcMAIgGhiFdcYXvut6undnVAAAi0fbt0pVX+pZPz8oyu5qwkZ1s1cld22pHZZ3WllervM4lSU02yX/7WFZSgg7OSVeHjGRmiQOAv9xu6e9/l9av923pQR4BUY3GOALrf/+T7r5bmjlT6tfP7GoAAOFi4ULfSiLPPSd16mR2NQCAQHj4Yeno/2fvzqOjqu//j78m+0JWIAmBgCCIgIg74gYKZXGp+0oRFdEq2rqgiNa6tcVK61JF0f4ErOIuoHXhW0QCKgiKsoiKsgchCVtmsi8zn98fU6aMBLiQmbmzPB/nzJHce3Pve67kvsi87/18TpROP93uSgAAkWjXLmnkSOn557nBqhlxDoc6ZKaqQ2aqXPWN2lHbqF11jaqoa1SjxyNJSoxzKCslUbkpScpNTVRWMqO3AMBB+ekn6Xe/k4YP9/Y1AEQ9GuMIjKYm6ZFHpI0bpXfflTIy7K4IABAOjJH++lfpxx+9U2ukpdldEQAgEBYskJYu9Y4QBQDAwaqp8TYhHntM6tbN7mrCXmZyojKTE9XZ7kIAIFoYI02eLH30kfcGLR7iAGJGnN0FIAqsXy+dd550xBHStGk0xQEAXi6XdOWV3lx48UWa4gAQLbZulR56yPtBEsO0AgAOVmOjdPXV0tix0nHH2V0NACDW/PyzdNFF3jyaNYumOBBjeGIcLTN9uvTKK9KkSVKXLnZXAwAIF99/L91yi3c0kVNOsbsaAECgNDZKo0Z5//3fqpXd1QAAIo3bLV13nfSb30hnnWV3NQCAWPPaa96H+558UurRw+5qANiAxjgOjcvlnXujUyfp3/+WEvirBAD4r3fe8c4lPn26VFBgdzUAgEAaO9bbGD/ySLsrAQBEGmOkm2+WhgyRLrjA7moAALFkxw7p97/3Tt/x/vtSYqLdFQGwCd1MHLxFi6T77pMeflg67TS7qwEAhIumJm8+NDR4h6LilwwAiC5Tp3qfEr/4YrsrAQBEGmOku+6S+vTxPi0OAECofPSR9Le/SX/9q3TCCXZXA8BmNMZhndst/eUv0o8/SjNmSNnZdlcEAAgX27ZJ118vXX65dNVVdlcDAAi0L76QPvhAevNNuysBAESiP/1JatPG+8Q4AAChUFXlvSkrJcX7lHhqqt0VAQgDcXYXgAixYYP06197h07/179oigMA/ufTT70N8UceoSkOANFoyxbp3nulF1+U4vgVEgBwkJ56Sqqtle65x+5KAACx4vPPvf2MSy+VnniCpjgAH54Yx/4Z450ndsYM6R//kA4/3O6KAADhwuPxDkP13XfSu+9KGRl2VwQACLT6eum666TnnpOysuyuBgAQaaZMkdas8X6mBABAsNXVSQ884B3ZcOZMfocBsBdu98e+lZZ676jauVN67z2a4gCA/9m+XbrkEik31zuSCE1xAIg+xki33up9de9udzUAgEjz1lve0aWeekpyOOyuBgAQ7RYvls45Rzr1VO+NWTTFATSDJ8bRvHfekSZPlh5/XOrd2+5qAADh5PPPpfvvl/7+d+nYY+2uBgAQLM8+651K6Zxz7K4EABBpZs+W3n5bmj6daTgAAMFVVyf98Y/ep8Tfesv7EAcA7AONcfirqJBuu01q31764AMpKcnuigAA4cLjkf72N2n5cmnWLCkz0+6KAADBMn++90ao6dPtrgQAEGkWLPA+bPHGG1ICHz0CAILoiy+ke++Vbr9dOu88u6sBEAHC+pbNCRMm6MQTT1RGRoby8vJ0wQUXaPXq1X7bDBgwQA6Hw+/129/+1m+bTZs26ZxzzlFaWpry8vJ01113qampKZRvJTLMmSNddJF0443Sn/9MUxxARCEzgmzHDumyy6RWraRXXqEpDiCikRkHsHGj9Mgj0gsvMPQtgJhHZhykJUukRx/13liVnGx3NQAQUmRGCNXVSXff7f2d5e23aYoDsCysb9ucP3++xowZoxNPPFFNTU269957NXjwYH333XdKT0/3bTd69Gg9/PDDvq/T0tJ8f3a73TrnnHNUUFCghQsXauvWrbr66quVmJiov/zlLyF9P2Grpka65x7vk4D//re0x7kFgEhBZgTRokXSffdJEydKxx9vdzUA0GJkxn7U1EjXX+/9gKlVK7urAQDbkRkH4euvvVMuvfkmny0BiElkRoh88YX3c6rbbqMhDuCgOYwxxu4irNq2bZvy8vI0f/58nXHGGZK8d1gdc8wxevLJJ5v9no8++kjnnnuutmzZovz8fEnS5MmTNW7cOG3btk1JFp6KdrlcysrKktPpVGa0PSG3YIH04IPSuHHSkCF2VwOghaL6enWQyIwAcLulCROk1aulp5+WsrPtrghAgEXVNasFyIz/Mka6+mpp5Ehp0CC7qwEQZsLummUTMmMfVq6U7rjD2xTPybG7GgA2C/trVoiQGQFWW+udS3zHDunvfydvgCgR6mtWWA+l/ktOp1OSlJub67d8+vTpatOmjY466iiNHz9eNTU1vnWLFi1S7969fSEiSUOGDJHL5dKqVauaPU59fb1cLpffK+pUVUm/+5306qvSzJk0xQFEHTKjhUpKpPPPlzp0kP71L5riAKIamfFff/qTdPLJNMUBYD/IjGZ8/713btfXXqNJAQB7IDMCaNEi6dxzpf79pSlTyBsAhyysh1Lfk8fj0W233aZTTz1VRx11lG/5VVddpU6dOqmwsFArVqzQuHHjtHr1as2YMUOSVFpa6hciknxfl5aWNnusCRMm6KGHHgrSOwkDn3zi/dDr3nv50AtAVCIzWmjGDGnyZOmZZ6QjjrC7GgAIKjLjv956Syor8w6BCwBoFpnRjJ9+km65xfvgRZs2dlcDAGGDzAiQ3U+J79zpnUuchjiAFoqYxviYMWP07bff6rPPPvNbfsMNN/j+3Lt3b7Vr104DBw7U2rVrdfjhhx/SscaPH6877rjD97XL5VJRUdGhFR5OXC7vXOLx8dK770oZGXZXBABBQWYcopoa6c47vXPKvv++ZGF4LgCIdGSGpK++kl55xftBEwBgn8iMX1i3TrrxRm+G/KKJAwCxjswIgHnzpEce8X5Wdc45dlcDIEpExFDqt9xyi95//33NmzdPHTp02O+2ffv2lSStWbNGklRQUKCysjK/bXZ/XVBQ0Ow+kpOTlZmZ6feKeP/3f9KFF0qXX+6dJ5amOIAoRWYcouXLpfPO82bFxIk0xQHEBDJD0s8/S3ffLU2bJiUm2l0NAIQtMuMXNm6Urr9eeuklqbDQ7moAIKyQGS1UUeG98WrGDGnWLJriAAIqrBvjxhjdcsstmjlzpj755BN17tz5gN+zbNkySVK7du0kSf369dPKlStVXl7u22bOnDnKzMxUz549g1J3WKmokEaP9jbG33vPOwcHAEQhMuMQGSP94x/Sww975wQcPNjuigAg6MiM/6qpka69Vnr+eYYkBIB9IDOa8fPP3vyYMkWKhicSASBAyIwAmDVLuvhi6eqrvQ/4RXqTH0DYCeuh1MeMGaNXX31V7777rjIyMnxzaGRlZSk1NVVr167Vq6++qrPPPlutW7fWihUrdPvtt+uMM87Q0UcfLUkaPHiwevbsqREjRuixxx5TaWmp/vCHP2jMmDFKTk628+0F3/vvS0884W12nHqq3dUAQFCRGYegtFQaM8Z709Tbb0sOh90VAUBIkBmSPB7vk3733CN162Z3NQAQtsiMXygt9TYrXnhBOuwwu6sBgLBCZrRAaal0xx1Sly7SBx9IKSl2VwQgWpkwJqnZ19SpU40xxmzatMmcccYZJjc31yQnJ5uuXbuau+66yzidTr/9bNiwwQwbNsykpqaaNm3amDvvvNM0NjZarsPpdBpJe+03bG3dasxVVxlzzz3G1NTYXQ2AEIq461UAkRkH6e23jRk0yJiVK+2uBICNIuaaFWBkhjHmj380ZvLk0B8XQMQiM2I4M3YrLzdm4EBjfvjBvhoARISwuGbZgMw4BB6PMS++aMzgwcYsX253NQBsEOprlsMYY4LXdo8OLpdLWVlZcjqd4T0/h8cjvfii9M470mOPSf+9ywxA7IiY61UUC/v/B06ndPvtUkGB9OCDzCUOxLiwv2ZFOdvO/2uvSV98IT31VOiOCSDikRn2sv38b9smXXmlNzt69Qr98QFEFNuvWTEuYs7/unXez6jOOEP6/e+lhLAe4BhAkIT6msWVJlr88IN0553S0KHeoUbi4+2uCAAQboqLvdNrPPIIU2wAQKxavFh6803prbfsrgQAECl2N8WffJKmOACg5dxu741WxcXeqWAPP9zuigDEEBrjka6+Xnr0Uenbb6XJk6WiIrsrAgCEm7o66b77pKoq6d13pYwMuysCANhh40bp3nulGTN4GgMAYE15uXTVVd6m+FFH2V0NACDSrVgh3X23dPnl3s+oHA67KwIQY+LsLgAt8Nln0tlne+/WffNNmuIAgL0tWyade640YID0/PM0xQEgVlVUSNddJ/2//ydlZdldDQAgEuxuij/1FE1xAEDL1NRI99zjfchv2jTp2mtpigOwBY8JRKKKCmn8eO+f33lHys62sxoAQDhyu6WJE6VvvvHOJdu2rd0VAQDs0tAgXX21NGGC1Lmz3dUAACJBWZk0fLj0j39IPXvaXQ0AIJJ99JH3M6qxY70P+gGAjWiMRxJjpDfekP75T+nBB6XTT7e7IgBAOPrxR+m226RLL5Vef507cAEglhkj3XST92nxk06yuxoAQCQoK/M+Kf7MM1KPHnZXAwCIVFu3SnfdJbVvL/3731J6ut0VAQCN8YixerV37o1+/bx3WCUl2V0RACDcuN3eYQ4XLJAmT5Y6drS7IgCA3f70J+noo6ULLrC7EgBAJCgt9T4pTlMcAHCoPB7vdH7vvis99pj39xEACBM0xsNdTY30l794n/576inpsMPsrggAEI5Wr/Y+JX7xxdLMmTwlDgCQXnlF2r5duv9+uysBAESC3U3xSZOkI4+0uxoAQCRavlwaN857Y+6HH0pxcXZXBAB+aIyHs/fflx5/XLr9du+THgAA/JLbLT35pPTZZ967cXlKHAAgScXF0nvvSa+9ZnclAIBIsHWrtyn+7LM0xQEAB6+6WnroIWnLFmnqVKldO7srAoBmcbtOONq4UbrsMumLL7zN8fPOs7siAEA4+uEH6dxzpexsacYMmuIAAK/vv/eOOjV1qhQfb3c1AIBwt7sp/txzNMUBAAfvgw+8PYwzz/SOWkVTHEAY44nxcNLQ4H1CfOFC79wb/DICAGiO2y098YT0+efSCy9IRUV2VwQACBdlZdKYMdKrr0rp6XZXAwAIdz//LI0Y4W2Kd+9udzUAgEiyZYs0dqz3QY3335fS0uyuCAAOiCfGw8W8edKwYVKnTtK779IUBwA0b/dT4rm53qfEaYoDAHarqZFGjpSeeUYqKLC7GgBAuNu40dsUf+EFmuIAAOuamqSnn5ZGjZLGj5cefZSmOICIwRPjdisp8YZHmzbeBkdWlt0VAQDCUWOjNHGitHQpT4kDAPbmdkvXXSfdfbfUs6fd1QAAwt3atdLo0dKUKdJhh9ldDQAgUnzxhXT//dLll3uHUI/j2UsAkYXGuF3q6qS//90bJBMmSEcdZXdFAIBw9dVX3puorr3W+1+Hw+6KAADh5q67pLPPls46y+5KAADhbvVq6aabpJde4oZbAIA1O3ZI997r/fNrr3kf9AOACERjPNSM8c638cQT3l9C7r2XBgcAoHk1NdIf/yht3+6dK7ZtW7srAgCEo6ef9o48dfXVdlcCAAh3q1ZJv/udNH261K6d3dUAAMKdxyNNnSq9/rr08MNSv352VwQALcI4F6H044/SxRdLS5ZI//63dOmlNMUBAM375BPvXOIDB0rTptEUBwA07513pGXLvDdSAQCwP8uWSbfd5n3Sj6Y4AOBAli+XzjlHqq6WPvqIpjiAqMAT46FQUSE98oh3PvGJE6XDD7e7IgBAuNq2zTs/bGam9O67UkaG3RUBAMJVcbG3ufHaa9xwCwDYv88/93429frrUuvWdlcDAAhnu3Z55xGvrJSmTOFmKgBRhcZ4sHk80ogR0h13SGeeaXc1AIBw5vFI110nPfigdPzxdlcDAAhnHo/0t79Jb74pJSbaXQ0AIJx5PNKECd5RRtLT7a4GABDOPB5p5Ehp3Djp1FPtrgYAAo7GeLDFxUkzZ0oJnGoAwAGQGQAAq+LipFmzyAwAwIGRGQAAq+LipBkzyAwAUYs5xkOBEAEAWEVmAACsIjMAAFaRGQAAq8gMAFGMxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrFVGN80qRJOuyww5SSkqK+fftqyZIldpcEAAhTZAYAwCoyAwBgFZkBALCKzACAwIuZxvgbb7yhO+64Qw888IC+/vpr9enTR0OGDFF5ebndpQEAwgyZAQCwiswAAFhFZgAArCIzACA4YqYx/vjjj2v06NG69tpr1bNnT02ePFlpaWmaMmWK3aUBAMIMmQEAsIrMAABYRWYAAKwiMwAgOBLsLiAUGhoatHTpUo0fP963LC4uToMGDdKiRYv22r6+vl719fW+r51OpyTJ5XIFv1gAaIHd1yljjM2VRC4yA0AsITdahswAEEvIjJYhMwDEEjKjZcgMALEk1JkRE43x7du3y+12Kz8/3295fn6+fvjhh722nzBhgh566KG9lhcVFQWtRgAIpB07digrK8vuMiISmQEgFpEbh4bMABCLyIxDQ2YAiEVkxqEhMwDEolBlRkw0xg/W+PHjdccdd/i+rqioUKdOnbRp0yaC/ABcLpeKiopUUlKizMxMu8sJa5wr6zhX1jmdTnXs2FG5ubl2lxIzyIxDx8+2dZwr6zhXB4fcCC0y49Dxs20d58o6ztXBITNCi8w4dPxsW8e5so5zdXDIjNAiMw4dP9vWca6s41wdnFBnRkw0xtu0aaP4+HiVlZX5LS8rK1NBQcFe2ycnJys5OXmv5VlZWfwltigzM5NzZRHnyjrOlXVxcXF2lxCxyIzQ42fbOs6VdZyrg0NuHBoyI/T42baOc2Ud5+rgkBmHhswIPX62reNcWce5OjhkxqEhM0KPn23rOFfWca4OTqgyIyaSKSkpSccff7zmzp3rW+bxeDR37lz169fPxsoAAOGGzAAAWEVmAACsIjMAAFaRGQAQPDHxxLgk3XHHHRo5cqROOOEEnXTSSXryySdVXV2ta6+91u7SAABhhswAAFhFZgAArCIzAABWkRkAEBwx0xi//PLLtW3bNv3xj39UaWmpjjnmGM2ePVv5+fkH/N7k5GQ98MADzQ5HAn+cK+s4V9ZxrqzjXAUGmREanCvrOFfWca4ODuer5ciM0OBcWce5so5zdXA4Xy1HZoQG58o6zpV1nKuDw/lqOTIjNDhX1nGurONcHZxQny+HMcaE5EgAAAAAAAAAAAAAANggJuYYBwAAAAAAAAAAAADELhrjAAAAAAAAAAAAAICoRmMcAAAAAAAAAAAAABDVaIwDAAAAAAAAAAAAAKIajXELJk2apMMOO0wpKSnq27evlixZYndJIfXggw/K4XD4vY488kjf+rq6Oo0ZM0atW7dWq1atdPHFF6usrMxvH5s2bdI555yjtLQ05eXl6a677lJTU1Oo30rALViwQOedd54KCwvlcDg0a9Ysv/XGGP3xj39Uu3btlJqaqkGDBumnn37y22bnzp0aPny4MjMzlZ2drVGjRqmqqspvmxUrVuj0009XSkqKioqK9NhjjwX7rQXcgc7VNddcs9ffs6FDh/ptEyvnasKECTrxxBOVkZGhvLw8XXDBBVq9erXfNoH6uSsuLtZxxx2n5ORkde3aVdOmTQv224t6ZAaZsS9khnVkhnVkRmQjM8iMfSEzrCMzrCMzIhuZQWbsC5lhHZlhHZkR2cgMMmNfyAzryAzrIi4zDPbr9ddfN0lJSWbKlClm1apVZvTo0SY7O9uUlZXZXVrIPPDAA6ZXr15m69atvte2bdt863/729+aoqIiM3fuXPPVV1+Zk08+2Zxyyim+9U1NTeaoo44ygwYNMt9884358MMPTZs2bcz48ePteDsB9eGHH5r77rvPzJgxw0gyM2fO9Fv/6KOPmqysLDNr1iyzfPly8+tf/9p07tzZ1NbW+rYZOnSo6dOnj/niiy/Mp59+arp27WquvPJK33qn02ny8/PN8OHDzbfffmtee+01k5qaap5//vlQvc2AONC5GjlypBk6dKjf37OdO3f6bRMr52rIkCFm6tSp5ttvvzXLli0zZ599tunYsaOpqqrybROIn7t169aZtLQ0c8cdd5jvvvvOPP300yY+Pt7Mnj07pO83mpAZZMb+kBnWkRnWkRmRi8wgM/aHzLCOzLCOzIhcZAaZsT9khnVkhnVkRuQiM8iM/SEzrCMzrIu0zKAxfgAnnXSSGTNmjO9rt9ttCgsLzYQJE2ysKrQeeOAB06dPn2bXVVRUmMTERPPWW2/5ln3//fdGklm0aJExxnsBiYuLM6Wlpb5tnnvuOZOZmWnq6+uDWnso/fLi6PF4TEFBgZk4caJvWUVFhUlOTjavvfaaMcaY7777zkgyX375pW+bjz76yDgcDvPzzz8bY4x59tlnTU5Ojt+5GjdunOnevXuQ31Hw7CtIzj///H1+T6yeK2OMKS8vN5LM/PnzjTGB+7m7++67Ta9evfyOdfnll5shQ4YE+y1FLTKDzLCKzLCOzDg4ZEbkIDPIDKvIDOvIjINDZkQOMoPMsIrMsI7MODhkRuQgM8gMq8gM68iMgxPumcFQ6vvR0NCgpUuXatCgQb5lcXFxGjRokBYtWmRjZaH3008/qbCwUF26dNHw4cO1adMmSdLSpUvV2Njod46OPPJIdezY0XeOFi1apN69eys/P9+3zZAhQ+RyubRq1arQvpEQWr9+vUpLS/3OTVZWlvr27et3brKzs3XCCSf4thk0aJDi4uK0ePFi3zZnnHGGkpKSfNsMGTJEq1ev1q5du0L0bkKjuLhYeXl56t69u2666Sbt2LHDty6Wz5XT6ZQk5ebmSgrcz92iRYv89rF7m1i7vgUKmfE/ZMbBIzMOHpnRPDIjMpAZ/0NmHDwy4+CRGc0jMyIDmfE/ZMbBIzMOHpnRPDIjMpAZ/0NmHDwy4+CRGc0L98ygMb4f27dvl9vt9vsfIUn5+fkqLS21qarQ69u3r6ZNm6bZs2frueee0/r163X66aersrJSpaWlSkpKUnZ2tt/37HmOSktLmz2Hu9dFq93vbX9/f0pLS5WXl+e3PiEhQbm5uTF3/oYOHap//etfmjt3rv76179q/vz5GjZsmNxut6TYPVcej0e33XabTj31VB111FGSFLCfu31t43K5VFtbG4y3E9XIDC8y49CQGQeHzGgemRE5yAwvMuPQkBkHh8xoHpkROcgMLzLj0JAZB4fMaB6ZETnIDC8y49CQGQeHzGheJGRGwkG9I8SkYcOG+f589NFHq2/fvurUqZPefPNNpaam2lgZoskVV1zh+3Pv3r119NFH6/DDD1dxcbEGDhxoY2X2GjNmjL799lt99tlndpcCWEJmIBTIjOaRGYg0ZAZCgcxoHpmBSENmIBTIjOaRGYg0ZAZCgcxoXiRkBk+M70ebNm0UHx+vsrIyv+VlZWUqKCiwqSr7ZWdn64gjjtCaNWtUUFCghoYGVVRU+G2z5zkqKCho9hzuXhetdr+3/f39KSgoUHl5ud/6pqYm7dy5M+bPX5cuXdSmTRutWbNGUmyeq1tuuUXvv/++5s2bpw4dOviWB+rnbl/bZGZm8o/EQ0BmNI/MsIbMaBkyg8yINGRG88gMa8iMliEzyIxIQ2Y0j8ywhsxoGTKDzIg0ZEbzyAxryIyWITMiJzNojO9HUlKSjj/+eM2dO9e3zOPxaO7cuerXr5+NldmrqqpKa9euVbt27XT88ccrMTHR7xytXr1amzZt8p2jfv36aeXKlX4XgTlz5igzM1M9e/YMef2h0rlzZxUUFPidG5fLpcWLF/udm4qKCi1dutS3zSeffCKPx6O+ffv6tlmwYIEaGxt928yZM0fdu3dXTk5OiN5N6G3evFk7duxQu3btJMXWuTLG6JZbbtHMmTP1ySefqHPnzn7rA/Vz169fP7997N4mlq9vLUFmNI/MsIbMaBkyg8yINGRG88gMa8iMliEzyIxIQ2Y0j8ywhsxoGTKDzIg0ZEbzyAxryIyWITMiKDMM9uv11183ycnJZtq0aea7774zN9xwg8nOzjalpaV2lxYyd955pykuLjbr1683n3/+uRk0aJBp06aNKS8vN8YY89vf/tZ07NjRfPLJJ+arr74y/fr1M/369fN9f1NTkznqqKPM4MGDzbJly8zs2bNN27Ztzfjx4+16SwFTWVlpvvnmG/PNN98YSebxxx8333zzjdm4caMxxphHH33UZGdnm3fffdesWLHCnH/++aZz586mtrbWt4+hQ4eaY4891ixevNh89tlnplu3bubKK6/0ra+oqDD5+flmxIgR5ttvvzWvv/66SUtLM88//3zI329L7O9cVVZWmrFjx5pFixaZ9evXm48//tgcd9xxplu3bqaurs63j1g5VzfddJPJysoyxcXFZuvWrb5XTU2Nb5tA/NytW7fOpKWlmbvuust8//33ZtKkSSY+Pt7Mnj07pO83mpAZZMb+kBnWkRnWkRmRi8wgM/aHzLCOzLCOzIhcZAaZsT9khnVkhnVkRuQiM8iM/SEzrCMzrIu0zKAxbsHTTz9tOnbsaJKSksxJJ51kvvjiC7tLCqnLL7/ctGvXziQlJZn27dubyy+/3KxZs8a3vra21tx8880mJyfHpKWlmQsvvNBs3brVbx8bNmwww4YNM6mpqaZNmzbmzjvvNI2NjaF+KwE3b948I2mv18iRI40xxng8HnP//feb/Px8k5ycbAYOHGhWr17tt48dO3aYK6+80rRq1cpkZmaaa6+91lRWVvpts3z5cnPaaaeZ5ORk0759e/Poo4+G6i0GzP7OVU1NjRk8eLBp27atSUxMNJ06dTKjR4/e6x9ssXKumjtPkszUqVN92wTq527evHnmmGOOMUlJSaZLly5+x8ChITPIjH0hM6wjM6wjMyIbmUFm7AuZYR2ZYR2ZEdnIDDJjX8gM68gM68iMyEZmkBn7QmZYR2ZYF2mZ4fhv0QAAAAAAAAAAAAAARCXmGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAdCyO1265RTTtFFF13kt9zpdKqoqEj33XefTZUBAMINmQEAsIrMAABYRWYAAKwiMxCNHMYYY3cRQCz58ccfdcwxx+if//ynhg8fLkm6+uqrtXz5cn355ZdKSkqyuUIAQLggMwAAVpEZAACryAwAgFVkBqINjXHABv/4xz/04IMPatWqVVqyZIkuvfRSffnll+rTp4/dpQEAwgyZAQCwiswAAFhFZgAArCIzEE1ojAM2MMborLPOUnx8vFauXKlbb71Vf/jDH+wuCwAQhsgMAIBVZAYAwCoyAwBgFZmBaEJjHLDJDz/8oB49eqh37976+uuvlZCQYHdJAIAwRWYAAKwiMwAAVpEZAACryAxEizi7CwBi1ZQpU5SWlqb169dr8+bNdpcDAAhjZAYAwCoyAwBgFZkBALCKzEC04IlxwAYLFy5U//799Z///Ed/+tOfJEkff/yxHA6HzZUBAMINmQEAsIrMAABYRWYAAKwiMxBNeGIcCLGamhpdc801uummm3TmmWfqxRdf1JIlSzR58mS7SwMAhBkyAwBgFZkBALCKzAAAWEVmINrwxDgQYr///e/14Ycfavny5UpLS5MkPf/88xo7dqxWrlypww47zN4CAQBhg8wAAFhFZgAArCIzAABWkRmINjTGgRCaP3++Bg4cqOLiYp122ml+64YMGaKmpiaGIAEASCIzAADWkRkAAKvIDACAVWQGohGNcQAAAAAAAAAAAABAVGOOcQAAAAAAAAAAAABAVKMxDgAAAAAAAAAAAACIajTGAQAAAAAAAAAAAABRjcY4AAAAAAAAAAAAACCq0RgHAAAAAAAAAAAAAEQ1GuMAAAAAAAAAAAAAgKhGYxwAAAAAAAAAAAAAENVojAMAAAAAAAAAAAAAohqNcQAAAAAAAAAAAABAVKMxDgAAAAAAAAAAAACIajTGAQAAAAAAAAAAAABRjcY4AAAAAAAAAAAAACCq0RgHAAAAAAAAAAAAAEQ1GuMAAAAAAAAAAAAAgKhGYxwAAAAAAAAAAAAAENVojAMAAAAAAAAAAAAAohqNcQAAAAAAAAAAAABAVKMxDhyCBx98UA6HI6TH3LBhgxwOh6ZNmxbS4wIAWobMAABYRWYAAKwiMwAAzSEfgP2jMY6oN23aNDkcjn2+vvjiC7tLtFVlZaXuvvtude7cWcnJyWrfvr0uueQS1dTU2F0aAIQcmdG84uLi/Z6XP//5z3aXCAAhR2bsW11dnSZMmKCePXsqLS1N7du316WXXqpVq1bZXRoA2ILM2Leqqirddttt6tChg5KTk9WjRw8999xzdpcFACFBPuzbG2+8od/85jfq1q2bHA6HBgwYsM9t6+vrNW7cOBUWFio1NVV9+/bVnDlzQlcsIkqC3QUAofLwww+rc+fOey3v2rXrQe/rD3/4g+65555AlGUrp9Op/v37a/PmzbrhhhvUtWtXbdu2TZ9++qnq6+uVlpZmd4kAYAsyw1+PHj308ssv77X85Zdf1n/+8x8NHjzYhqoAIDyQGXsbPny43nvvPY0ePVrHHXectmzZokmTJqlfv35auXKlOnXqZHeJAGALMsOf2+3WkCFD9NVXX2nMmDHq1q2b/u///k8333yzdu3apXvvvdfuEgEgJMiHvT333HNaunSpTjzxRO3YsWO/215zzTV6++23ddttt6lbt26aNm2azj77bM2bN0+nnXZaiCpGpKAxjpgxbNgwnXDCCQHZV0JCghISIv/HZ/z48dq4caO+/vprv+AdN26cjVUBgP3IDH/5+fn6zW9+s9fyhx56SN26ddOJJ55oQ1UAEB7IDH8///yzZsyYobFjx2rixIm+5aeffrrOOusszZgxQ7fffruNFQKAfcgMfzNmzNDChQv14osv6rrrrpMk3XTTTbrkkkv0yCOP6Prrr1deXp7NVQJA8JEPe3v55ZfVvn17xcXF6aijjtrndkuWLNHrr7+uiRMnauzYsZKkq6++WkcddZTuvvtuLVy4MFQlI0IwlDrwX7vnwfjb3/6mJ554Qp06dVJqaqr69++vb7/91m/b5ubpmDNnjk477TRlZ2erVatW6t69+153tpaXl2vUqFHKz89XSkqK+vTpo5deemmvWioqKnTNNdcoKytL2dnZGjlypCoqKpqt+4cfftAll1yi3NxcpaSk6IQTTtB77713wPdbUVGhqVOn6oYbblDnzp3V0NCg+vr6A34fACD2MqM5S5Ys0Zo1azR8+PBD+n4AiBWxlhmVlZWSvDdV7aldu3aSpNTU1APuAwBiVaxlxqeffipJuuKKK/yWX3HFFaqrq9O77757wH0AQCyItXyQpKKiIsXFHbiF+fbbbys+Pl433HCDb1lKSopGjRqlRYsWqaSkxNLxEDsi/7YRwCKn06nt27f7LXM4HGrdurXfsn/961+qrKzUmDFjVFdXp6eeekpnnXWWVq5cudeHO7utWrVK5557ro4++mg9/PDDSk5O1po1a/T555/7tqmtrdWAAQO0Zs0a3XLLLercubPeeustXXPNNaqoqNDvf/97SZIxRueff74+++wz/fa3v1WPHj00c+ZMjRw5stnjnnrqqWrfvr3uuecepaen680339QFF1ygd955RxdeeOE+z8dnn32muro6de3aVZdccolmzZolj8ejfv36adKkSTrmmGOsnloAiDpkxoFNnz5dkmiMA4h5ZIa/ww8/XB06dNDf//53de/eXccee6y2bNmiu+++W507d96r+QEAsYTM8FdfX6/4+HglJSX5Ld89td/SpUs1evTo/ZxRAIgO5MOh++abb3TEEUcoMzPTb/lJJ50kSVq2bJmKiooCcixECQNEualTpxpJzb6Sk5N9261fv95IMqmpqWbz5s2+5YsXLzaSzO233+5b9sADD5g9f3yeeOIJI8ls27Ztn3U8+eSTRpJ55ZVXfMsaGhpMv379TKtWrYzL5TLGGDNr1iwjyTz22GO+7Zqamszpp59uJJmpU6f6lg8cOND07t3b1NXV+ZZ5PB5zyimnmG7duu33vDz++ONGkmndurU56aSTzPTp082zzz5r8vPzTU5OjtmyZct+vx8AohGZYU1TU5PJz883J5100kF9HwBEEzJj3xYvXmwOP/xwv3Ny/PHHm61btx7wewEgGpEZzfv73/9uJJlPP/3Ub/k999xjJJlzzz13v98PAJGOfLCmV69epn///vtcd9ZZZ+21fNWqVUaSmTx58kEdC9GPodQRMyZNmqQ5c+b4vT766KO9trvgggvUvn1739cnnXSS+vbtqw8//HCf+87OzpYkvfvuu/J4PM1u8+GHH6qgoEBXXnmlb1liYqJ+97vfqaqqSvPnz/dtl5CQoJtuusm3XXx8vG699Va//e3cuVOffPKJLrvsMlVWVmr79u3avn27duzYoSFDhuinn37Szz//vM+aq6qqJHnvPJs7d66uuuoq3XTTTZo1a5Z27dqlSZMm7fN7ASDakRn7N3fuXJWVlfG0OACIzGhOTk6OjjnmGN1zzz2aNWuW/va3v2nDhg269NJLVVdXt9/vBYBoRmb4u+qqq5SVlaXrrrtOc+bM0YYNG/TCCy/o2WefleR9ghEAYgH5cOhqa2uVnJy81/KUlBTfemBPDKWOmHHSSSfphBNOOOB23bp122vZEUccoTfffHOf33P55Zfr//2//6frr79e99xzjwYOHKiLLrpIl1xyiW8ejI0bN6pbt257zYvRo0cP3/rd/23Xrp1atWrlt1337t39vl6zZo2MMbr//vt1//33N1tXeXm5X1Duaffcfuedd57fsU4++WR17txZCxcu3Of7BYBoR2bs3/Tp0xUfH6/LL7/c0vYAEM3IDH9Op1Onn3667rrrLt15552+5SeccIIGDBigqVOn+n2QBgCxhMzwV1BQoPfee08jRozQ4MGDJUmZmZl6+umnNXLkyL2ODwDRinw4dKmpqaqvr99r+e4bcnf3QYDdaIwDAZCamqoFCxZo3rx5+uCDDzR79my98cYbOuuss/Sf//xH8fHxAT/m7ru7xo4dqyFDhjS7TdeuXff5/YWFhZLU7NwjeXl52rVrVwCqBAD8UiRmxp5qa2s1c+ZMDRo0aJ/zVwEAAiMSM+Odd95RWVmZfv3rX/st79+/vzIzM/X555/TGAeAIIjEzJCkM844Q+vWrdPKlStVXV2tPn36aMuWLZK8zR4AQMtEaj5Y1a5du2afPt+6dauk//VBgN1ojAO/8NNPP+217Mcff9Rhhx223++Li4vTwIEDNXDgQD3++OP6y1/+ovvuu0/z5s3ToEGD1KlTJ61YsUIej8fvzqsffvhBktSpUyfff+fOnauqqiq/O69Wr17td7wuXbpI8g5pMmjQoIN+n8cff7wkNRsaW7Zs0ZFHHnnQ+wSAWBMrmbGn9957T5WVlQyjDgAHKVYyo6ysTJLkdrv9lhtj5Ha71dTUdND7BIBYEyuZsVt8fLyOOeYY39cff/yxJLX4dxcAiDaxlg9WHHPMMZo3b55cLpcyMzN9yxcvXuxbD+yJOcaBX5g1a5Zfs3jJkiVavHixhg0bts/v2blz517Ldl9wdw/jcfbZZ6u0tFRvvPGGb5umpiY9/fTTatWqlfr37+/brqmpSc8995xvO7fbraefftpv/3l5eRowYICef/55391Pe9q2bdt+32f37t3Vp08fvfvuu9q+fbtv+X/+8x+VlJToV7/61X6/HwAQO5mxp1dffVVpaWm68MILLX8PACB2MmP3032vv/663/L33ntP1dXVOvbYY/f7/QCA2MmM5mzbtk1//etfdfTRR9MYB4BfiOV82JdLLrlEbrdbL7zwgm9ZfX29pk6dqr59+6qoqChgx0J04IlxxIyPPvrId4fTnk455RTfHUySdwiP0047TTfddJPq6+v15JNPqnXr1rr77rv3ue+HH35YCxYs0DnnnKNOnTqpvLxczz77rDp06KDTTjtNknTDDTfo+eef1zXXXKOlS5fqsMMO09tvv63PP/9cTz75pDIyMiR55/w+9dRTdc8992jDhg3q2bOnZsyYIafTuddxJ02apNNOO029e/fW6NGj1aVLF5WVlWnRokXavHmzli9fvt9z8sQTT+hXv/qVTjvtNN14441yOp16/PHHdcQRRzC8IYCYRmY0b+fOnfroo4908cUXM98fAPwXmeHvvPPOU69evfTwww9r48aNOvnkk7VmzRo988wzateunUaNGmX53AJAtCEz9ta/f3/169dPXbt2VWlpqV544QVVVVXp/fff32uuWwCIVuTD3hYsWKAFCxZI8jbSq6ur9ac//UmSdxqOM844Q5LUt29fXXrppRo/frzKy8vVtWtXvfTSS9qwYYNefPHF/R4DMcoAUW7q1KlG0j5fU6dONcYYs379eiPJTJw40fz97383RUVFJjk52Zx++ulm+fLlfvt84IEHzJ4/PnPnzjXnn3++KSwsNElJSaawsNBceeWV5scff/T7vrKyMnPttdeaNm3amKSkJNO7d2/f8fe0Y8cOM2LECJOZmWmysrLMiBEjzDfffONX725r1641V199tSkoKDCJiYmmffv25txzzzVvv/22pfMzZ84cc/LJJ5uUlBSTm5trRowYYbZu3WrpewEg2pAZ+zd58mQjybz33nuWtgeAaEZm7NvOnTvN7bffbo444giTnJxs2rRpY6644gqzbt06aycXAKIMmbFvt99+u+nSpYtJTk42bdu2NVdddZVZu3attRMLABGOfNi33e+judcDDzzgt21tba0ZO3asKSgoMMnJyebEE080s2fPPuAxEJscxhgTsC47EME2bNigzp07a+LEiRo7dqzd5QAAwhiZAQCwiswAAFhFZgAAmkM+AIHDeDQAAAAAAAAAAAAAgKhGYxwAAAAAAAAAAAAAENVojAMAAAAAAAAAAAAAopqtjfEJEyboxBNPVEZGhvLy8nTBBRdo9erVftvU1dVpzJgxat26tVq1aqWLL75YZWVlftts2rRJ55xzjtLS0pSXl6e77rpLTU1NftsUFxfruOOOU3Jysrp27app06YF++0hwhx22GEyxjBHBxCmyAyEEzIDCG9kBsIJmQGENzID4YTMAMIbmQG7kA9A4NjaGJ8/f77GjBmjL774QnPmzFFjY6MGDx6s6upq3za33367/v3vf+utt97S/PnztWXLFl100UW+9W63W+ecc44aGhq0cOFCvfTSS5o2bZr++Mc/+rZZv369zjnnHJ155platmyZbrvtNl1//fX6v//7v5C+XwDAoSMzAABWkRkAAKvIDACAVWQGAEQBE0bKy8uNJDN//nxjjDEVFRUmMTHRvPXWW75tvv/+eyPJLFq0yBhjzIcffmji4uJMaWmpb5vnnnvOZGZmmvr6emOMMXfffbfp1auX37Euv/xyM2TIkGC/JQBAkJAZAACryAwAgFVkBgDAKjIDACJPWM0x7nQ6JUm5ubmSpKVLl6qxsVGDBg3ybXPkkUeqY8eOWrRokSRp0aJF6t27t/Lz833bDBkyRC6XS6tWrfJts+c+dm+zex8AgMhDZgAArCIzAABWkRkAAKvIDACIPAl2F7Cbx+PRbbfdplNPPVVHHXWUJKm0tFRJSUnKzs722zY/P1+lpaW+bfYMkd3rd6/b3zYul0u1tbVKTU31W1dfX6/6+nq/2nbu3KnWrVvL4XC0/M0CQJAYY1RZWanCwkLFxYXVvU8BRWYAQGDEQm6QGQAQGGRGtt+2ZAYA7BuZke23LZkBAPsW6swIm8b4mDFj9O233+qzzz6zuxRNmDBBDz30kN1lAMAhKykpUYcOHewuI2jIDAAIrGjODTIDAAKLzAgNMgNANCAzQoPMABANQpUZYdEYv+WWW/T+++9rwYIFfm+6oKBADQ0Nqqio8LvLqqysTAUFBb5tlixZ4re/srIy37rd/929bM9tMjMz97q7SpLGjx+vO+64w/e10+lUx44dVVJSoszMzJa9WQAIIpfLpaKiImVkZNhdStCQGQAQONGeG2QGAAQOmUFmAIBVZAaZAQBWhTozbG2MG2N06623aubMmSouLlbnzp391h9//PFKTEzU3LlzdfHFF0uSVq9erU2bNqlfv36SpH79+unPf/6zysvLlZeXJ0maM2eOMjMz1bNnT982H374od++58yZ49vHLyUnJys5OXmv5ZmZmQQJgIgQjcMkkRkAEDzRlhtkBgAED5lBZgCAVWQGmQEAVoUsM4yNbrrpJpOVlWWKi4vN1q1bfa+amhrfNr/97W9Nx44dzSeffGK++uor069fP9OvXz/f+qamJnPUUUeZwYMHm2XLlpnZs2ebtm3bmvHjx/u2WbdunUlLSzN33XWX+f77782kSZNMfHy8mT17tqU6nU6nkWScTmfg3jwABEE0X6/IDAAIvGi9ZpEZABB40XrNIjMAIPCi9ZpFZgBA4IX6mmVrY1xSs6+pU6f6tqmtrTU333yzycnJMWlpaebCCy80W7du9dvPhg0bzLBhw0xqaqpp06aNufPOO01jY6PfNvPmzTPHHHOMSUpKMl26dPE7xoEQJAAiRTRfr8gMAAi8aL1mkRkAEHjRes0iMwAg8KL1mkVmAEDghfqa5TDGmEA/hR5tXC6XsrKy5HQ6GXoEQFjjemU//h8AiCRcs+zF+QcQSbhm2YvzDyCScM2yF+cfQCQJ9TXL1jnGAcQ2t9utxsZGu8uIOImJiYqPj7e7DAAIKTLj0JAZAGIRmXFoyAwAsYjMODRkBoBYRGYcmnDLDBrjAELOGKPS0lJVVFTYXUrEys7OVkFBgRwOh92lAEBQkRktR2YAiBVkRsuRGQBiBZnRcmQGgFhBZrRcOGUGjXEAIbc7RPLy8pSWlhYWF8NIYYxRTU2NysvLJUnt2rWzuSIACC4y49CRGQBiDZlx6MgMALGGzDh0ZAaAWENmHLpwzAwa4wBCyu12+0KkdevWdpcTkVJTUyVJ5eXlysvLC6thSAAgkMiMliMzAMQKMqPlyAwAsYLMaDkyA0CsIDNaLtwyI87WowOIObvn4EhLS7O5ksi2+/wxpwmAaEZmBAaZASAWkBmBQWYAiAVkRmCQGQBiAZkRGOGUGTTGAdiC4UZahvMHIJZwzWsZzh+AWMI1r2U4fwBiCde8luH8AYglXPNaJpzOH41xAAhDhx12mJ588km7ywAARAAyAwBgFZkBALCKzAAAWBVJmUFjHAAOQmlpqX7/+9+ra9euSklJUX5+vk499VQ999xzqqmpsbs8AEAYITMAAFaRGQAAq8gMAIBVZMbeEuwuAAAixbp163TqqacqOztbf/nLX9S7d28lJydr5cqVeuGFF9S+fXv9+te/trtMAEAYIDMAAFaRGQAAq8gMAIBVZEbzeGIcQERx1jm12bW52XWbXZvlrHMG7dg333yzEhIS9NVXX+myyy5Tjx491KVLF51//vn64IMPdN5550mSNm3apPPPP1+tWrVSZmamLrvsMpWVlfn2s3btWp1//vnKz89Xq1atdOKJJ+rjjz8OWt0AEKvIDACAVWQGAMAqMgMAYBWZEX5ojAOIGM46p4ZOH6r+0/qrxFnit67EWaL+0/pr6PShQQmTHTt26D//+Y/GjBmj9PT0ZrdxOBzyeDw6//zztXPnTs2fP19z5szRunXrdPnll/u2q6qq0tlnn625c+fqm2++0dChQ3Xeeedp06ZNAa8bAGIVmQEAsIrMAABYRWYAAKwiM8ITQ6kDiBiVDZUqry7Xul3rNOClASoeWayirCKVOEs04KUBWrdrnW+7rJSsgB57zZo1Msaoe/fufsvbtGmjuro6SdKYMWM0aNAgrVy5UuvXr1dRUZEk6V//+pd69eqlL7/8UieeeKL69OmjPn36+PbxyCOPaObMmXrvvfd0yy23BLRuAIhVZAYAwCoyAwBgFZkBALCKzAhPPDEOIGJ0yOyg4pHF6pLTxRcmC0sW+kKkS04XFY8sVofMDiGracmSJVq2bJl69eql+vp6ff/99yoqKvKFiCT17NlT2dnZ+v777yV577AaO3asevTooezsbLVq1Urff/99xN5hBQDhiMwAAFhFZgAArCIzAABWkRnhiSfGAUSUoqwiFY8s9oXHqVNOlSRfiBRlFe1/B4eoa9eucjgcWr16td/yLl26SJJSU1Mt72vs2LGaM2eO/va3v6lr165KTU3VJZdcooaGhoDWDACxjswAAFhFZgAArCIzAABWkRnhhyfGAUScoqwivXzhy37LXr7w5aCFiCS1bt1av/rVr/TMM8+ourp6n9v16NFDJSUlKin535wh3333nSoqKtSzZ09J0ueff65rrrlGF154oXr37q2CggJt2LAhaLUDQCwjMwAAVpEZAACryAwAgFVkRnihMQ4g4pQ4SzRi5gi/ZSNmjlCJs2Qf3xEYzz77rJqamnTCCSfojTfe0Pfff6/Vq1frlVde0Q8//KD4+HgNGjRIvXv31vDhw/X1119ryZIluvrqq9W/f3+dcMIJkqRu3bppxowZWrZsmZYvX66rrrpKHo8nqLUDQKwiMwAAVpEZAACryAwAgFVkRnihMQ4gopQ4S/zm4Pj8us/95ugIZpgcfvjh+uabbzRo0CCNHz9effr00QknnKCnn35aY8eO1SOPPCKHw6F3331XOTk5OuOMMzRo0CB16dJFb7zxhm8/jz/+uHJycnTKKafovPPO05AhQ3TccccFrW4AiFVkBgDAKjIDAGAVmQEAsIrMCD8OY4yxu4hw53K5lJWVJafTqczMTLvLASJaXV2d1q9fr86dOyslJeWgvneza7P6T+vvC5Hdc3D8MlzmXzNfHTI7BOcNhIl9nUeuV/bj/wEQOGRGYOzvPHLNshfnHwgcMiMwyIzwxfkHAofMCAwyI3xx/oHAITMCI5wygyfGAUSMjKQM5aXn+YWI5J2jo3hksbrkdFFeep4ykjJsrhQAYDcyAwBgFZkBALCKzAAAWEVmhKcEuwsAAKuyUrI0e/hsVTZU7nUHVVFWkeZfM18ZSRnKSsmyqUIAQLggMwAAVpEZAACryAwAgFVkRniiMQ4gomSlZO0zKKJ9uBEAwMEhMwAAVpEZAACryAwAgFVkRvhhKHUAAAAAAAAAAAAAQFSjMQ4AAAAAAAAAAAAAiGo0xgHYwhhjdwkRjfMHIJZwzWsZzh+AWMI1r2U4fwBiCde8luH8AYglXPNaJpzOH41xACGVmJgoSaqpqbG5ksi2+/ztPp8AEI3IjMAgMwDEAjIjMMgMALGAzAgMMgNALCAzAiOcMiPB7gIAxJb4+HhlZ2ervLxckpSWliaHw2FzVZHDGKOamhqVl5crOztb8fHxdpcEAEFDZrQMmQEglpAZLUNmAIglZEbLkBkAYgmZ0TLhmBk0xgGEXEFBgST5wgQHLzs723ceASCakRktR2YAiBVkRsuRGQBiBZnRcmQGgFhBZrRcOGUGjXEAIedwONSuXTvl5eWpsbHR7nIiTmJiYljcWQUAoUBmtAyZASCWkBktQ2YAiCVkRsuQGQBiCZnRMmGXGcZG8+fPN+eee65p166dkWRmzpzpt15Ss6/HHnvMt02nTp32Wj9hwgS//SxfvtycdtppJjk52XTo0MH89a9/Pag6nU6nkWScTuchv1cACIVovl6RGQAQeNF6zSIzACDwovWaRWYAQOBF6zWLzACAwAv1NSuuxZ31FqiurlafPn00adKkZtdv3brV7zVlyhQ5HA5dfPHFfts9/PDDftvdeuutvnUul0uDBw9Wp06dtHTpUk2cOFEPPvigXnjhhaC+NwBAYJEZAACryAwAgFVkBgDAKjIDACKfrUOpDxs2TMOGDdvn+l+ON//uu+/qzDPPVJcuXfyWZ2Rk7HNs+unTp6uhoUFTpkxRUlKSevXqpWXLlunxxx/XDTfc0PI3AQAICTIDAGAVmQEAsIrMAABYRWYAQOSz9Ynxg1FWVqYPPvhAo0aN2mvdo48+qtatW+vYY4/VxIkT1dTU5Fu3aNEinXHGGUpKSvItGzJkiFavXq1du3aFpHYAQGiRGQAAq8gMAIBVZAYAwCoyAwDCk61PjB+Ml156SRkZGbrooov8lv/ud7/Tcccdp9zcXC1cuFDjx4/X1q1b9fjjj0uSSktL1blzZ7/vyc/P963LycnZ61j19fWqr6/3fe1yuQL9dgAAQURmAACsIjMAAFaRGQAAq8gMAAhPEdMYnzJlioYPH66UlBS/5XfccYfvz0cffbSSkpJ04403asKECUpOTj6kY02YMEEPPfRQi+oFANiHzAAAWEVmAACsIjMAAFaRGQAQniJiKPVPP/1Uq1ev1vXXX3/Abfv27aumpiZt2LBBkndej7KyMr9tdn+9r3k8xo8fL6fT6XuVlJS07A0AAEKGzAAAWEVmAACsIjMAAFaRGQAQviKiMf7iiy/q+OOPV58+fQ647bJlyxQXF6e8vDxJUr9+/bRgwQI1Njb6tpkzZ466d+/e7LAjkpScnKzMzEy/FwAgMpAZAACryAwAgFVkBgDAKjIDAMKXrY3xqqoqLVu2TMuWLZMkrV+/XsuWLdOmTZt827hcLr311lvN3l21aNEiPfnkk1q+fLnWrVun6dOn6/bbb9dvfvMbX0hcddVVSkpK0qhRo7Rq1Sq98cYbeuqpp/yGLAEAhD8yAwBgFZkBALCKzAAAWEVmAEAUMDaaN2+ekbTXa+TIkb5tnn/+eZOammoqKir2+v6lS5eavn37mqysLJOSkmJ69Ohh/vKXv5i6ujq/7ZYvX25OO+00k5ycbNq3b28effTRg6rT6XQaScbpdB7S+wSAUInm6xWZAQCBF63XLDIDAAIvWq9ZZAYABF60XrPIDAAIvFBfsxzGGBPk3nvEc7lcysrKktPpZBgSAGGN65X9+H8AIJJwzbIX5x9AJOGaZS/OP4BIwjXLXpx/AJEk1NesiJhjHAAAAAAAAAAAAACAQ0VjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUozEOAAAAAAAAAAAAAIhqNMYBAAAAAAAAAAAAAFGNxjgAAAAAAAAAAAAAIKrRGAcAAAAAAAAAAAAARDUa4wAAAAAAAAAAAACAqEZjHAAAAAAAAAAAAAAQ1WiMAwAAAAAAAAAAAACiGo1xAAAAAAAAAAAAAEBUs7UxvmDBAp133nkqLCyUw+HQrFmz/NZfc801cjgcfq+hQ4f6bbNz504NHz5cmZmZys7O1qhRo1RVVeW3zYoVK3T66acrJSVFRUVFeuyxx4L91gAAAUZmAACsIjMAAFaRGQAAq8gMAIh8tjbGq6ur1adPH02aNGmf2wwdOlRbt271vV577TW/9cOHD9eqVas0Z84cvf/++1qwYIFuuOEG33qXy6XBgwerU6dOWrp0qSZOnKgHH3xQL7zwQtDeFwAg8MgMAIBVZAYAwCoyAwBgFZkBAJEvwc6DDxs2TMOGDdvvNsnJySooKGh23ffff6/Zs2fryy+/1AknnCBJevrpp3X22Wfrb3/7mwoLCzV9+nQ1NDRoypQpSkpKUq9evbRs2TI9/vjjfoEDAAhvZAYAwCoyAwBgFZkBALCKzACAyBf2c4wXFxcrLy9P3bt310033aQdO3b41i1atEjZ2dm+EJGkQYMGKS4uTosXL/Ztc8YZZygpKcm3zZAhQ7R69Wrt2rUrdG8EABB0ZAYAwCoyAwBgFZkBALCKzACA8GbrE+MHMnToUF100UXq3Lmz1q5dq3vvvVfDhg3TokWLFB8fr9LSUuXl5fl9T0JCgnJzc1VaWipJKi0tVefOnf22yc/P963LycnZ67j19fWqr6/3fe1yuQL91gAAAUZmAACsIjMAAFaRGQAAq8gMAAh/Yd0Yv+KKK3x/7t27t44++mgdfvjhKi4u1sCBA4N23AkTJuihhx4K2v4BAIFHZgAArCIzAABWkRkAAKvIDAAIf2E/lPqeunTpojZt2mjNmjWSpIKCApWXl/tt09TUpJ07d/rm8SgoKFBZWZnfNru/3tdcH+PHj5fT6fS9SkpKAv1WAABBRmYAAKwiMwAAVpEZAACryAwACD8R1RjfvHmzduzYoXbt2kmS+vXrp4qKCi1dutS3zSeffCKPx6O+ffv6tlmwYIEaGxt928yZM0fdu3dvdtgRSUpOTlZmZqbfCwAQWcgMAIBVZAYAwCoyAwBgFZkBAOHH1sZ4VVWVli1bpmXLlkmS1q9fr2XLlmnTpk2qqqrSXXfdpS+++EIbNmzQ3Llzdf7556tr164aMmSIJKlHjx4aOnSoRo8erSVLlujzzz/XLbfcoiuuuEKFhYWSpKuuukpJSUkaNWqUVq1apTfeeENPPfWU7rjjDrveNgDgEJAZAACryAwAgFVkBgDAKjIDAKKAsdG8efOMpL1eI0eONDU1NWbw4MGmbdu2JjEx0XTq1MmMHj3alJaW+u1jx44d5sorrzStWrUymZmZ5tprrzWVlZV+2yxfvtycdtppJjk52bRv3948+uijB1Wn0+k0kozT6WzxewaAYIrm6xWZAQCBF63XLDIDAAIvWq9ZZAYABF60XrPIDAAIvFBfsxzGGBPk3nvEc7lcysrKktPpZBgSAGGN65X9+H8AIJJwzbIX5x9AJOGaZS/OP4BIwjXLXpx/AJEk1NesiJpjHEBsc9Y5tdm1udl1m12b5axzhrgiAAAAAAAAAAAARAIa4wAigrPOqaHTh6r/tP4qcZb4rStxlqj/tP4aOn0ozXEAAAAAAAAAAADshcY4gIhQ2VCp8upyrdu1TgNeGuBrjpc4SzTgpQFat2udyqvLVdlQaWudAAAAAAAAAAAACD80xgFEhA6ZHVQ8slhdcrr4muMLSxb6muJdcrqoeGSxOmR2sLtUAAAAAAAAAAAAhBka4wAiRlFWkV9z/NQpp/o1xYuyiuwuEQAAAAAAAAAAAGGIxjiAiFKUVaSXL3zZb9nLF75MUxwAAAAAAAAAAAD7RGMcQEQpcZZoxMwRfstGzBzhm3McABB6zjqnNrs2N7tus2uznHXOEFcEAAAAAAAAAP5ojAOIGCXOEr85xT+/7nM9vjjHN+c4zXEACD1nnVNDpw9V/2n997oOlzhL1H9afw2dPpTmOAAAAAAAAABb0RgHEBE2uzb7NcWLRxbrlKJT9FudoMOzOvua4/t6YhEAEByVDZUqry7f6yalPW9mKq8uV2VDpa11AgAAAAAAAIhtNMYBRISMpAzlpef5muK75xRPzWqt4ss+UpecLspLz1NGUobNlQJAbOmQ2UHFI4vVJaeLrzm+sGThXjczdcjsYHepAAAAAAAAAGJYgt0FAIAVWSlZmj18tiobKv2bK6mp6pCYq/nXzFdGUoayUrLsKxIAYlRRVpGKRxb7muGnTjlVkva6mQkAAAAAAAAA7MIT4wAiRlZK1t5PHNbWepvjmR1oigOAjYqyivTyhS/7LXv5wpdpigMAAAAAAAAICzTGAUS2qiopPd3uKgAg5pU4SzRi5gjf1xl10tzf/9o35zgAAAAAAAAA2InGOIDIZozkcNhdBQDEtBJnid+c4p9f97natuuiLmt26Pq/nkJzHAAAAAAAAIDtaIwDiFw0xQHAdptdm/2a4sUji3VK0SkqHlmsf15QpOEfeNdvdm22u1QAAAAAAAAAMYzGOIDItWWLVFhodxUAENMykjKUl57na4rvnlO8KKtIL9/5uXIS0nVUfZYykjJsrhQAAAAAAABALEuwuwAAOGQrV0q9etldBQDEtKyULM0ePluVDZXqkNnBb11RVpFSHp6qXy3+RikpWTZVCAAAAAAAAAA8MQ4gkn3xhXTyyXZXAQAxLysla6+m+G5th16klEVfeqe/AAAAAAAAAACb0BgHELmWLZOOOcbuKgAA+xMfL3XvLv34o92VAAAAAAAAAIhhNMYBRKbGRqmpSUpKsrsSAMCB/OpX0rx5dlcBAAAAAAAAIIbRGAcQmT7/XDrlFLurAABYccYZ0vz5dlcBAAAAAAAAIIbRGAcQmT76SBo2zO4qAABW5ORITqfdVQAAAAAAAACIYTTGAUQm5hcHgMhSUCCVltpdBQAAAAAAAIAYRWMcQOT58Uepc2fJ4bC7EgCAVX36SMuX210FAAAAAAAAgBhFYxxA5Hn9denKK+2uAgBwMI4+Wlq50u4qAAAAAAAAAMQoGuMAIosx0qefSqefbnclAICD0aWLtGGD3VUAAAAAAAAAiFE0xgFElmXLpN69pTguXwAQUdq3lzZvtrsKAAAAAAAAADGKzhKAyPLCC9L119tdBQDgYCUkSG633VUAAAAAAAAAiFG2NsYXLFig8847T4WFhXI4HJo1a5ZvXWNjo8aNG6fevXsrPT1dhYWFuvrqq7Vlyxa/fRx22GFyOBx+r0cffdRvmxUrVuj0009XSkqKioqK9Nhjj4Xi7QEItKoqadMmqWdPuyuBDcgMAIBVZAYAwCoyI7Y1uj0qr67X2l3V+nFHlX7aWaUNzhpV1DXKY4zd5QEIM2QGAEQ+Wxvj1dXV6tOnjyZNmrTXupqaGn399de6//779fXXX2vGjBlavXq1fv3rX++17cMPP6ytW7f6Xrfeeqtvncvl0uDBg9WpUyctXbpUEydO1IMPPqgXXnghqO8NQBC89pp05ZV2VwGbkBlAFEhIkJqa7K4CMYDMAABYRWbEnnq3R6t3VOn/1pXr32vK9NnmnVpe7tKq7ZX6dlulvi516pON2/Xej6VasGmHfq6spUkOQBKZAQDRIMHOgw8bNkzDhg1rdl1WVpbmzJnjt+yZZ57RSSedpE2bNqljx46+5RkZGSooKGh2P9OnT1dDQ4OmTJmipKQk9erVS8uWLdPjjz+uG264IXBvBkBwGSO98460x52YiC1kBhAFcnKkXbuktm3trgRRjswAAFhFZsSORrdH326r1EZnjTzNrP9l69sjaXttg7bXNiglPk4922SoU1aqHA5HCKoFEI7IDACIfBE1x7jT6ZTD4VB2drbf8kcffVStW7fWscceq4kTJ6ppjyeRFi1apDPOOENJSUm+ZUOGDNHq1au1a9euUJWOg2CMUU2jW7vqGrWztkEVdY1qcDf3KwtiymefScceK6Wk2F0JIgSZAYShjAypstLuKoC9kBkAAKvIjMi0raZec9Zv0/p9NMUPpM7t0ddlTi38eZfqmtwBrw9AdCIzACD82PrE+MGoq6vTuHHjdOWVVyozM9O3/He/+52OO+445ebmauHChRo/fry2bt2qxx9/XJJUWlqqzp07++0rPz/fty4nJ2evY9XX16u+vt73tcvlCsZbwh4aPR5tctaqrLpeO+sa1ODee4iqtMR4tU5JVPuMVLVrlcwdurHmqaekp5+2uwpECDIDCFPV1VJ6ut1VAH7IDACAVWRGZPppZ5VWbgvMzZnl1d4G++lFrZWdkhiQfQKITmQGAISniGiMNzY26rLLLpMxRs8995zfujvuuMP356OPPlpJSUm68cYbNWHCBCUnJx/S8SZMmKCHHnqoRTXDmromt1bvrNKGilq5DzBfU02jW7WNbpVU1ik1IU5dc9J1eE664miQR7/vvvMOv9uund2VIAKQGUAYq6ryPjUOhAkyAwBgFZkRmQLZFJe8w603eYwWlOxQ/46tlZVMcxzA3sgMAAhfYT+U+u4Q2bhxo+bMmeN3d1Vz+vbtq6amJm3YsEGSVFBQoLKyMr9tdn+9r3k8xo8fL6fT6XuVlJS0/I1gLz9X1mrO+m1at6vmgE3x3XZvVdvk0cptlfpkw3a56huDVyTCwxNPSHv8oxHYFzIDCHM1NVJqqt1VAJLIDACAdWRGZPq5sjagTfHdjCS3x+izkp0Mqw5gL2QGAIS3sG6M7w6Rn376SR9//LFat259wO9ZtmyZ4uLilJeXJ0nq16+fFixYoMbG/zVP58yZo+7duzc77IgkJScnKzMz0++FwPEYo6VbK7R4S4UaPUbWWuLNq2xo0twN27WhoiZg9SHMbNwouVxSjx52V4IwR2YAEaCxUWKkF4QBMgMAYBWZEZnqmzz6utQZtP0bSQ1uj5aXM1wxgP8hMwAg/Nk6lHpVVZXWrFnj+3r9+vVatmyZcnNz1a5dO11yySX6+uuv9f7778vtdqu0tFSSlJubq6SkJC1atEiLFy/WmWeeqYyMDC1atEi33367fvOb3/hC4qqrrtJDDz2kUaNGady4cfr222/11FNP6YknnrDlPcc6Y4yWbKnQlqq6wOzvv//9uswptzE6PId5S6POn/8s3Xef3VUgDJAZQBSgKY4QITMAAFaRGdFpRblTTZ6WPIpxYEbSz5V12lpVp3atUoJ6LADhgcwAgMjnMMbiGNZBUFxcrDPPPHOv5SNHjtSDDz6ozp07N/t98+bN04ABA/T111/r5ptv1g8//KD6+np17txZI0aM0B133OE3H8eKFSs0ZswYffnll2rTpo1uvfVWjRs3znKdLpdLWVlZcjqd3G3VQt+UVmi9szZo++9bmK32GQzRGjXWrpXuv1969VW7K4kY0Xy9IjOACFde7p0W45VX7K4Ee4jWaxaZAQCBF63XLDIj+lTUNeqTjdtDdry0xHgN6dxWDm4CBXyi9ZpFZgBA4IX6mmVrYzxSECSBsbWqTot+3hXUY8Q7HBrcpa1SE+KDehyEyKhR3iZKr152VxIxuF7Zj/8HwD6sWCH9v/8n/eMfdleCPXDNshfnH0Ak4ZplL86/dd+UOrXBWdOiqfsO1ulFuWqblnzgDYEYwTXLXpx/RCq3x8hZ36i6Jo88MopzOJSeEK+M5ATFcQNa1Ar1NcvWodQROxrdwZ3baTePMVpe5tTJ7XODfiwE2Y8/eueipSkOANHhxx+5pgMAACComjwebXLVhrQp7pC0vqKGxjgAIKicdU5VNlSqQ2aHvdZtdm1WRlKGslKybKisZWoa3VpfUa2tVfWqbGhqNsPjHFJWcqI6ZKTosKw0JcbHhbxORA8a4wiJH3ZUqd7tCfpxjKQtVfUqq65Xfjq/kES0P/5ReuQRu6sAgIjj9hhVNzbJY7xTeifFxSklIc7+oR0/+US69VZ7awAAAEBU21bTIHeIB8f0fhZVJ2OM/f/mBgBEJWedU0OnD1V5dbmKRxarKKvIt67EWaIBLw1QXnqeZg+fHTHN8cqGJq3aVqktVXVySPu9qc1jpF11jdpV16hV2yt1WFaaerTJUDINchwCGuMIOrfHaL2zJmTHc0hau6uaxngkmz9fKiyUunWzuxIACHvGGO2obVSJq1Y7ahuavbs2Kd6h3JQk5aUlq2NWqpLs+MVh3TqpoCD0xwUAAEDMqKhrPOCH68HgMVJVo1sZSXzUCgAIvMqGSpVXl2vdrnUa8NIAX3N8d1N83a51vu3CvTFujNH6ihqt2ObS7nvZDia3PcY7Usvmyjqd0C5LBekpQakT0YvbKRB0P1fVqckTul9JjKTS6nrVNrpDdkwEkMcj/fnP0v33210JAIQ1Y4w2OWs0d8N2LSjZoQ3OGrn2MeRUg9uotLpeK7a59OHaMn1T6lRNY1Poiq2t9f43Jyd0xwQAAEDMqahrDHlTfM9jAwAQDB0yO6h4ZLG65HTxNccXliz0NcW75HRR8cjiZodZDyduj9Gin3dpWblLHnPoN7IZSQ1ujxZu3qVvt7lkQjxaDCIbjXEEXYmz1pbjbq6057hooZdfls49l+YJAOxHbaNbn23eqa9KnXI1eBvcVn8F8Bhpg7NGc9Zv1wZnTWh+eZg1S7r44uAfBwAAADGtsiGEN3/uwSGpyqZjAwBiQ1FWkV9z/NQpp/o1xfccXj0cuT1GC3/eqdLq+oDu98ed1Vq5rZLmOCyjMY6gMsZoZ11DyI/rEHfqRqTqaumll6SbbrK7EgAIW6XVdZqzYZu21xx6vhpJbmP0dalTi7fskjvYI7ts2SK1bh3cYwAAACDmhXp+8T15+EAeABBkRVlFevnCl/2WvXzhy2HfFDfGaGlphba14LOs/Vmzq1o/7aoOyr4RfWiMI6jq3B41hnAY9d2MpJ11jWp0e+Ssb9SuugZV1DWqvonh1cPahAnS7bdLiYl2VwIAYWlrVZ0Wbd6lJo8J2BCRW6rq9fnmncFtjn/wgTR0aPD2DwAAAEhyOBwxeWwAQGwocZZoxMwRkqT2Tim9Xhoxc4RKnCU2V7Z/P1fWaXNlXVCPsWpbpVz1PCyJA6MxjqBy1ds3jFR1o1v/XlOmuRu2a97GHfpk43Z9sLZcH64p0xc/79QGZ03wn5CDdT/8IP34o3TeeXZXAgBhaUdtgxb/vCsocyZur23Qkq27gjPs1Jw50pFHSmlpgd83AAAAsIfEOHua00ZSgk3HBgDEhhJnid+c4guyb9Oluwp8c46Ha3O83u3RN2XOkBxr6VYnQ6rjgGiMI6iawrDxXOf2aEtVvb4uderDtWVatc2lBrfH7rJimzHSXXdJEyfaXQkAhKUmj0dLtuxSMNNqa1W9NjhrA7/j55+XRo4M/H4BAACAX8hOTpRd7emsZEa/AwAEx2bXZr+mePHIYnU55kz9vcdtvjnHB7w0QJtdm+0udS+rd1SFpE9kJO2qb1SJKwifbSGq0BhHkIVfY3xPjR6jH3dWa876bSqvrre7nNg1fbp0+ulSp052VwIAYen77VWqbQr+TVwryl2qDeS0IwsWSAkJUt++gdsnAAAAsA/ZKYm2fRKVlZxg05EBANEuIylDeel5vqZ4UVaR1KaNcmuMt0me00V56XnKSMqwu1Q/bo/RhoqakGbz2oqaEB4NkYh/sSGo4iNgGCkj73Aen23eqW456TqqbQbzQoXSrl3S1KnS7Nl2VwIAYclV36ifdlWH5FgeY7Sy3KWTCnNavjOnU7r3Xunf/275vgAAAAALclLseWo7JSFOKQnxthwbABD9slKyNHv4bFU2VKpDZgfvwlatpMpKFWUVaf4185WRlKGslCx7C/2Fnytr1RTioc131TXKWd/ISC7YJ54YR1BlJEbWvRc/7arWN2XMQxFS994rPfiglEhQAUBz1lfUhGw4SCPp58o61bX0qXFjpD//WbruOiknAE12AAAAwIKclESlJ4a2Qe2QdFhWWkiPCQCIPVkpWf9riktSfb2UnCxJ6pDZIeya4pJUUlkX8mM65P1sC9gXGuMIqrTEeCVE2NPXG5y1+n5Hld1lxIa5c6X4eO8w6gCAvbg9RhudtSEdcspI2tTS+ZheeUXauVMaMSIgNQEAAABWOBwOdckObZPaiMY4AMAGNTVSaqrdVezXrrrGkB/T2HRcRA4a4wgqh8OhbJuGsWqJH3ZUaUdNg91lRLfKSu/ThI8+anclABC2SqvrQj7klCRtdLagMf72297XxImMBgIAAICQ65iZpvgQPaThkNQuPVlpIX5KHQAAbd0qtWtndxX7VNfkVoPbY8uxK2iMYz9ojCPoOmSm2F3CQXNI+qq0Qm4PQ6oHzT33SH/4g3cuFABAs3bVNYZsGPU9VTY0HXwGut3eZvibb0ovvsgQ6gAAALBFckKceudlhORYcQ6pT374DV0LAIgBJSVSUZHdVexTZUOTbceud3vU6LGnKY/wR2McQVeUkaq4yBpNXUZSdaNba3ZV211KdPrkE+9/zzrL3joAIMxV1DWGdBj1PTnrD+Lu2pIS6bzzpNpaado0qU2boNUFAAAAHEjnrDTlpiQG/SbTo9pm8rQ4AMAemzaFdWPc7ocO7T4+wleC3QUg+iXGx6ljZmrI50gNhLUV1ToiN12OCJsnPaxVVUl/+pP07rt2VwIAYc9Zb9/dta6GJuWmJu1/o2+/lZ59VlqzRnrqKalHj9AUBwAAAOyHw+HQiYXZmrdxuxrdJiifR7VrlRzy+cwBAPDZsEHq1MnuKvbN5pYKHR3sC0+MIyR6tskI2fxOgVTX5FFZTb3dZUSXu+7yDqGeEZphzQAgkjXZeHfrfu+sXb1aGj1auvNO6fzzpf/7P5riAAAACCvpiQk6vUNrJcQ5Av7heNvUJJ3ULocHKQAA9jBG8nik+PAdtSTBYW/7MT7ShjFGyNAYR0ikJMTr6PxMu8s4aA5JJa46u8uIHrNmSenpDKEOABGg2bZ4WZm3If7730sXXOBtiA8ZIvGBIAAAAMJQVkqi+ndsHdDhzjtkpOiUDrl84A4AsM/GjVLHjnZXsV+ZyfYNWJ2SEKeEONqfaB5DqSNkOmWmaktlnUqrI+cJbCNpZ22D3WVEhy1bpEmTpPfft7sSAIgY8XGS223Tsfdsdn/7rfTMM94h0x98UDr5ZCmBf0YCAAAg/GUmJ2rgYW317TaX1lXUyKF93AS6Hw55nzw7Lj9LHTJTg1AlAAAH4dNPpdNOs7uK/UqKj1NKfJzq3J6QHzsnJTHkx0Tk4JYJhIzD4dBJhTnKTUmMqPkdqhvdavKE/uIdVTwe6aabpKeflpKT7a4GACJGRpJ9zeeMpHhvQ/z666WxY6XLLpPmzPH+4kVTHAAAABEkIc6hY/Kz1L+otQpaeT+XONBnU7vXJ8Q51C03Xb/q3JamOAAgPBQXSwMG2F3FAeWmhr4X5BCNcewfn2oipBLiHDqtKFeLft6lbTWR8yR2ZUOTclKS7C4jcj3+uHT22dKRR9pdCQBElJyUJO2sbTzoJ1paKqWsVLl/uUcqL5duvdV7DQcAAAAiXOu0JPVLy1VNY5M2ueq0s7ZBO+sa1OD2/xd3q8R45aYmqW1aktpnpCqBYdMBAOFkyxapfXu7qzigosxUbakK7QjCRlKHDG5kw77RGEfIJcTF6bQOuVqzq1rfbquUdPBDWIVakyfcKwxjX38tffml9PrrdlcCABEnOzkhpBmZseZHdf3X/1OrLZsV99c/S337SvGBm48RAAAACAdpiQk6snUrSZIxRo0eI7cxckhKjItj/nAAQPhav17q1MnuKixp1ypFSXEONYSov+KQ1CYtSa1sHIER4Y+/HbCFw+FQt9xWyktP1qptlSqtrj+kOZ5CxYRrYeHO6fQOv/vmm5KDXyoB4GAVtEqRQ87g56MxOuah8Ur7uUQ/3vg75Q4aoLb52cE+KgAAAGA7h8OhpHg+swAARIh335V+/Wu7q7AkzuFQ55x0rd5RFZLjGUldstNDcixELuYYh62ykhN1SodcDencVofnpKtVYng+lcaQWYfAGOnGG6U//1lq08buagAgIiXFx6lDRkrQ52Pq+O7bqsvL18J/Ttf2E/rqsJxWQT4iAAAAAAAADtrcudJZZ9ldhWXdc9OVkhD8VqRDUl5akgpbJQf9WIhsPDGOsJCelKCj8zJ1dF6mmjweVdQ1qd7tlsd47ypKT4zT/E075LbpyW2G3jgETz4pnXyy1K+f3ZUAQETrnJ2uksq6oO0/bfMmdfhglhY99xJDTgEAAAAAAISr7dulVq2klBS7K7EsIS5Oxxdk6/PNO4N6HIdDOrYgSw5GrsUB8Kknwk5CXJzapCXttTwnJUnbaxtCXk9KQpyS4hlc4aAsXCgtWSK9+qrdlQBAxGuTlqT2rVK0paou4EOqO5qadMzD92rZAxNkEhLkkHR0XmaAjwIAAAAAAIAWmzFDOv98u6s4aPnpyTo8O01rK2qCdoxj87OUnkjLEwdGtw8RIyclMehDyTYnNyXRhqNGsO3bpT/8QZo8mXnFASBA+uRnKj4I03oc+ewTKjnnAtV06ChJOqJ1K2Ulk3sAAAAAAABhZ9asiGyMS94HMTpkBOdJ96PaZKhTVlpQ9o3oY7kxvmXLloAffMGCBTrvvPNUWFgoh8OhWbNm+a03xuiPf/yj2rVrp9TUVA0aNEg//fST3zY7d+7U8OHDlZmZqezsbI0aNUpVVVV+26xYsUKnn366UlJSVFRUpMceeyzg7wXB1z4jJeBPylk7bqoNR41QHo80erT0xBNSVpbd1cBGZAYQWCkJ8TouP7DX1baLPlVqealKzr9EDklZyQk6Mpe5xRF6ZAYAwCoyAwBgFZmBqLNmjdS+vZQamf0Kh8OhE9tlq3NWYOp3/Pd1dF6mjmjN51mwznJjvFevXno1wMMiV1dXq0+fPpo0aVKz6x977DH94x//0OTJk7V48WKlp6dryJAhqqv73zybw4cP16pVqzRnzhy9//77WrBggW644QbfepfLpcGDB6tTp05aunSpJk6cqAcffFAvvPBCQN8Lgi8nJVGZIZ7zNDHOocJWkTNfh+3uv1/69a+lPn3srgQ2IzOAwOuQmarebTMCsq/kbeXq/vzTWn7vI3JISk2M16kdcoPyVDpwIGQGAMAqMgMAYBWZgajzr39JV19tdxUt4nA4dGxBtk4uzFFinKNFIwSnJ8ZrQKc26pqTHrD6EBscxhhLD+E+++yzGjdunIYOHarnn39eubm5gS3E4dDMmTN1wQUXSPLeXVVYWKg777xTY8eOlSQ5nU7l5+dr2rRpuuKKK/T999+rZ8+e+vLLL3XCCSdIkmbPnq2zzz5bmzdvVmFhoZ577jndd999Ki0tVVKSd97qe+65R7NmzdIPP/xgqTaXy6WsrCw5nU5lZjLvpp3WVVRrWZkrJMdySOqak67ezLVqzVtvSZ99Jj31lN2VxLRwuV6RGfb/P0D0WrOrWivKXXJIhzaSitutk28dpVW3j1dlt+7KSIrXaUWtlZoQH+BKEQnC4ZpFZpAZACJDOFyzyAwyA0BkCIdrFplBZkQVt1saPFj6+OOomb60rsmtH3dWa31FjdwW2pS7PwdLjo9T15x0dc1J5wGPKBHqa5blJ8ZvvvlmrVixQjt27FDPnj3173//O5h1af369SotLdWgQYN8y7KystS3b18tWrRIkrRo0SJlZ2f7QkSSBg0apLi4OC1evNi3zRlnnOELEUkaMmSIVq9erV27dgX1PSDwDstKU0ZSQkjmGk+Mc6g7Q3BYs3y59461v/3N7koQJsgMIHi65qSrf8dDb2Qf+fw/tHXgEFV2666uOek6q1NbmuKwFZkBALCKzAAAWEVmIKp8+KE0dGjUNMUl77SBR+dl6uyueeqTl6n89GQl7aPRnZoQp8JWKTqpXbaGHZ6n7q1b0RTHITuocak7d+6sTz75RM8884wuuugi9ejRQwkJ/rv4+uuvA1JYaWmpJCk/P99veX5+vm9daWmp8vLy/NYnJCQoNzfXb5vOnTvvtY/d63JycvY6dn19verr631fu1yheUIZBxbncOj4giwVb9oR9GMdW5ClpHjL947Eru3bpdtvl958U0pMtLsahBEyAwie1qlJGtS5jX7YUaW1u6zdWStJbZYsUnrJRpXffpdOz8tU27TkIFcKWENmAACsIjMAAFaRGYgaL74oTZlidxVBkRgXp8Nz0nV4TrqMMapt8qiuyS1jpLg4h9IS4pWcQJ8GgXPQEzZv3LhRM2bMUE5Ojs4///y9giQaTJgwQQ899JDdZWAfclOTdGTrVvphR1XQjtEhI4W5xa1obJSuvVZ64gmpTRu7q0EYIjOA4EmIi9NRbTN1ZOtW2uSs1UZXrZz1jfLso0eetXObjnnhKblnztKANnv/Ig3YjcwAAFhFZgAArCIzEPFWr5batpUCPB1AOHI4HEpLjFdaIiMbIngOKgX++c9/6s4779SgQYO0atUqtW3bNlh1qaCgQJJUVlamdu3a+ZaXlZXpmGOO8W1TXl7u931NTU3auXOn7/sLCgpUVlbmt83ur3dv80vjx4/XHXfc4fva5XKpqKioZW8IAdWjdSvVNrq10VUb8H23TUvS8QXZckTRsCRBc9dd0ogRUp8+dleCMERmAKGREBenLjnp6pKTLo8xctU3qaqhSW5jFOdwKCk+TtkOj5Jvu0765/MSTXGEITIDAGAVmQEAsIrMQFSYNEm6+Wa7qwCihuXxB4YOHapx48bpmWee0YwZM4IaIpJ3mJOCggLNnTvXt8zlcmnx4sXq16+fJKlfv36qqKjQ0qVLfdt88skn8ng86tu3r2+bBQsWqLGx0bfNnDlz1L1792aHHZGk5ORkZWZm+r0QXhwOh44ryFLnrNSA7rcgPVmntM9lfgorJk+WMjKkyy6zuxKEITIDsEecw6HslER1yExVp6w0FWWmKj89Wcl3jZVuvFE68ki7SwT2QmYAAKwiMwAAVpEZiArbt0sbNkjHHhv0QzV6PCpx1WpFuUvzNm7Xv38q1bs/btV7P5bqo7VlWvzzLv24s0oVdY0H3hkQxiw/Me52u7VixQp16NAhYAevqqrSmjVrfF+vX79ey5YtU25urjp27KjbbrtNf/rTn9StWzd17txZ999/vwoLC3XBBRdIknr06KGhQ4dq9OjRmjx5shobG3XLLbfoiiuuUGFhoSTpqquu0kMPPaRRo0Zp3Lhx+vbbb/XUU0/piSeeCNj7gD0cDoeOLchWXnqylpY65fYYWZth9Rf7keRwSL3bZqpLdhpPilsxe7b0+efSv/5ldyUIU2QGEEamTPEOt/XfnwUg3JAZAACryAwAgFVkBqLCU09Jt90W1ENUNTRpbUW1NlTUym2MHNIv+ixGTU1GW6rq9HNVnaRK5aQkqmtOutpnpCiOfgoijbHRvHnzjLw/Y36vkSNHGmOM8Xg85v777zf5+fkmOTnZDBw40KxevdpvHzt27DBXXnmladWqlcnMzDTXXnutqays9Ntm+fLl5rTTTjPJycmmffv25tFHHz2oOp1Op5FknE5ni94vgqemscl8uWWXmfHDFvOOxdfubRdu3mFc9Y12v4XIsWKFMUOHGlNba3claEY0X6/IDOAQLFlizEUXGdPUZHclCFPRes0iMwAg8KL1mkVmAEDgRes1i8xASFVUeD+H93iCsnu3x2O+2+YyM/bolRzsa876cuOsawhKfYgdob5mOYwxh/KQbUxxuVzKysqS0+lkGJIwV9/k1npnjX6urJOrvmmfT5C3SoxXu1Yp6pKTpvREywMnYOtWafhw6Y03pCAPP4RDw/XKfvw/QNgoL5euuEJ65x1pH8OtAVyz7MX5BxBJuGbZi/MPIJJwzbIX5z9KPPqo1KuXdN55Ad91dUOTFm+pUEV9y4ZF3/2s+NF5jMaLQxfqaxYdQUSV5IR4Hdk6Q0e2zpDbY+Ssb1RNk1sej1Gcw6GUhDhlpyQqIS7O7lIjT3W1NHKk9NxzNMUBINzV10vXXusdcoumOAAAAAAAQOSoqZHmzpXuvjvgu65saNKCTTvU4Pa0eF+7H0xcXu5SbZNbvdpk0BxH2KMxjqgVH+dQbmqScu0uJBq43dJ110n33it17253NQCA/TFGuvlm6be/lXr3trsaAAAAAAAAHIwXX5RGjZIC/IBfbZNbn/63KR7ooaR/3FmtxLg4dW/dKsB7BgKLx2YB7J8x0tix0tlnSwMG2F0NAOBAHntM6tkzKENtAQAAAAAAIIjq66VZs6RLLw3obo0xWrJll+qD0BTfbdX2Sm2rqQ/S3oHAoDEOYP8ee8w7dPrIkXZXAgA4kJkzpXXrpDvusLsSAAAAAAAAHKyJE6VrrpHi4wO62w3OWu2obQxaU3y3paVOuT3BPgpw6BhKHcC+TZsmbdkiPfmk3ZUAAA7k66+lKVOkd96RmM8JAAAAAAAg8nz8sTRvXkB3Wdfk1opyV0D3uS81jW6t3lmlnm0yQnI84GDxxDiA5n3wgTeEn3iCBgsAhLuff/ZOe/HSS1JSkt3VAAAAAAAA4GCtXSvl5wf88/iNzlq5Teie4l6zq5qnxhG2aIwD2NsXX0jPPy+9+KIUx2UCAMJadbV07bXS5MlSbq7d1QAAAAAAAOBQfPaZ1LdvQHdpjNG6iuqA7vNAmjxGP1fVhfSYgFV0vAD4+/576f77pVdekZKT7a4GALA/TU3S1VdL990nHXGE3dUAAAAAAADgUH35ZcAb49trG1Tb5AnoPq3YUFET8mMCVtAYB/A/JSXSmDHSyy9LmZl2VwMA2B9jpJtvli6/XOrf3+5qAAAAAAAA0BIrV0qnnhrQXe6obZAdE6XurGuQCeHw7YBVNMYBeJWVSSNHSv/8p1RQYHc1AIADeeghqVcv6bLL7K4EAAAAAAAALbFmjVRYGPDdVtQ1yo72tMdIlQ1NNhwZ2D8a4wCknTul4cOlSZOkww+3uxoAwIH885/eucV//3u7KwEAAAAAAEBLzZ8v9esX8N3uqmsM+D6tctbTGEf4oTEOxLrKSunKK6WJE6UePeyuBgBwIO+/L336qfTXv9pdCQAAAAAAAALhq6+kk04K+G4bPfYNZ97gDv3c5sCBJNhdAAAb1dR4m+IPPigde6zd1QAADmTxYumFF6S33pLiuL8RAAAAAAAg4hkjffeddPLJQdi1fY1xphhHOKIxDsSq+nrpN7+R7rgjKEO0AAACbNUq6Q9/kN5+W0pOtrsaAAAAAAAABMKaNVLHjkHZdZzDIbdNHeo4hy2HBfaLR42AWNTUJF1zjTRqlHTWWXZXAwA4kHXrpFtvlaZPl7Ky7K4GAAAAAAAAgfLJJ0F7eC01IT4o+7UiLdG+YwP7QmMciDVut7chftFF0jnn2F0NAOBAtmyRrr9eeuklKS/P7moAAAAAAAAQSEuWBGUYdUnKSU2UXQ9uZ6ck2nRkYN9ojAOxxO2WrrtOGjZMuvRSu6sBABzIjh3SiBHS5MlSUZHd1QAAAAAAACCQjJHWr5eOOy4ou89OTpQdA6knxTuUYuPT6sC+0BgHYsWeTfErrrC7GgDAgVRWSlddJT3+uHTEEXZXAwBAWHDWObXZtbnZdZtdm+Wsc4a4IgAAAKAFPvtMOvHEoO0+Pz05aPveF4ekgvSUkB8XsILGOBALdg+fPnQoTXEAiAS1td6m+AMPSH362F0NAABhwVnn1NDpQ9V/Wn+VOEv81pU4S9R/Wn8NnT6U5jiAiMHNPgAA/fvf0q9+FbTdt0pKUJvUpKDtvzlGUufstJAeE7CKxjgQ7XY3xYcMka680u5qAAAH0tAgXX219LvfSaecYnc1AACEjcqGSpVXl2vdrnUa8NIAX3O8xFmiAS8N0Lpd61ReXa7Khkpb6wQAK7jZBwAgt1tatEgaODCoh+kS4iZ1q6R45TK/OMIUjXEgmrnd0vXXS4MH0xQHgEjQ2Ohtiv/mN0G9WxgAgEjUIbODikcWq0tOF19zfGHJQl9TvEtOFxWPLFaHzA52lwoAB8TNPgAArVwp9e0rORxBPUxhRooykxMU3KP8z1FtMuUI8nsCDhWNcSBaNTV5m+K/+pV3OF4AQHhrapKuuUa67DLp/PPtrgYAgLBUlFXk1xw/dcqpfk3xoqwiu0sEAEu42QcAoMmTpeHDg36YOIdDxxdkywT5OA5Jha1SVJjB/OIIXzTGgWjU0CCNHOkdPp2mOACEP7dbuu466YILpIsusrsaAADCWlFWkV6+8GW/ZS9f+DJNcQARh5t9ACCGeTzShg3SsceG5HA5KYnqnpse1GMkxDl0TH5mUI8BtBSNcSDa1NV57zK77DLpiivsrgYAcCC7p70YNky69FK7qwEAIOyVOEs0YuYIv2UjZo7Ya45eAIgE3OwDADFq0SLpxBNDesiebTJU2Co4T3PHOaRTO+QqJSE+KPsHAoXGOBBNqqulyy+XRo9mGF4AiAQej3TjjdLAgdKVV9pdDQAAYW/PuXcnf5qlRVfO9RuGmOY4gEjDzT4AEKNefTUkw6jvyeFw6KTCbBW2Sg7cPiXFOxw6vUNr5aYmBWy/QLDQGAeihcvlfUr8zjulwYPtrgYAcCAej3TTTdLpp0u/+Y3d1QAAEPY2uzb7muL93UX6Tc4AnXzEWXvN0bvZtdnmSgHAmj1v9umS00U/pY7ToIYO3OwDANGuslJat0468siQHzrO4VDfwhwd1TZDDnkb2y2RnZyogYe1Ues0muKIDDTGgWiwc6d3+N0HHpDOOMPuagAAB+J2e5viJ50kjRxpdzUAAJsYY1TZ0KQtlXUqcdVqs6tWZdX1qm9y211aWMpIylBeep665HTRBz+eqPQH/yzJf47evPQ8ZSRl2FwpABzYnjf77J5TvGtNil4ePJmbfQAg2r3wgnfUV5s4HA4dkdtKZ3Vqo6zkRO+yg9xHvEPq2aaV+ndqrVZJCYEvEggS/rYCka6szPuk4d/+JvXpY3c1AIADaWry/vJz5pnS1VfbXQ0AIMTcHqOfK2u10VWrnbWNchvT7HYpCXHKS0tSl+x0hiT8r6yULM0ePluN77+r9A4rpF69fOuKsoo0/5r5ykjKUFZKlo1VAoA1u2/2kaTikcXeOcVdLhUUHqHiY4o14KUB3OwDANHI5ZI++kj6z3/srkRZKYk6s1Nr7axr1Npd1fq5sk67fzv5ZaN89/K0hHh1zU1Xp8xUJcbz7C0iT9j/rT3ssMPkcDj2eo0ZM0aSNGDAgL3W/fa3v/Xbx6ZNm3TOOecoLS1NeXl5uuuuu9TU1GTH2wECq6REuuoq6R//oCkOiMxABGhslK65Rho6lKY4YDMyA6HmMUard1Tpo7Vl+qrUqW01DftsiktSXZNHJa46FW/aoU82bFdZdX0Iqw1fWVWNavPcS9LDD++1rkNmB5riCAoyA8Gw+2af+dfM9zbFJWnTJql9e9/NPrOHz+a6BkQYMgMH9Kc/SePGSXHh0Z5zOBxqnZqkkwpz9OtuBRrQsbX65GWqc3aaOmal6rCsNB2Rm66+hdka2iVPQ7q0VdecdJriiFhh/8T4l19+Kbf7f8PIffvtt/rVr36lSy+91Lds9OjReniPX4rT0tJ8f3a73TrnnHNUUFCghQsXauvWrbr66quVmJiov/zlL6F5E0AwfP+9dMst0j//KXXpYnc1QFggMxDW6uu9zfArrpAuvNDuaoCYR2YglFz1jVqytUKu+oP7QHN327yivlGfb96pzlmp6p2XqYQw+RAt5Nxu6frrpb//Xdrj5xEINjIDwZKVkuXf+K6t9V3fOmR2sKkqAC1BZmC/fvxR2rhR+tWv7K6kWfFxDuWmJjFiFaJa2DfG27Zt6/f1o48+qsMPP1z9+/f3LUtLS1NBQUGz3/+f//xH3333nT7++GPl5+frmGOO0SOPPKJx48bpwQcfVFISP+CIQIsXS/ffL736qpSfb3c1QNggMxC2amul4cOlUaOkc86xuxoAIjMQOiWuWn21tSIg+1rvrFVZTYNOL8pVemLY/zofWMZIY8dKF18sHXOM3dUgxpAZCInqaiklxe4qALQQmYH9Gj9emjjR7iqAmBZRt5k3NDTolVde0XXXXSeH438zHEyfPl1t2rTRUUcdpfHjx6umpsa3btGiRerdu7fy92geDhkyRC6XS6tWrQpp/UBA/Oc/3uFW3nqLpjiwH2QGwkZ1tXT55dLNN9MUB8IUmYFgKXHV6sutFTL639PfLVXb6Nb8jTtU0xhjw2n+9a9SXp40YoTdlSDGkRkImi++kPr1s7sKAAFEZsDPBx9IPXsy+itgs4i6xXzWrFmqqKjQNddc41t21VVXqVOnTiosLNSKFSs0btw4rV69WjNmzJAklZaW+oWIJN/XpaWlzR6nvr5e9fX/m7/N5XIF+J0Ah+j116VZs7xNce4iBvaLzEBYqKz0Dp0+bpx0xhl2VwNgH8gMBMO2mvqAPSm+JyOp3u3RpyU7dVanNrExt9/zz0tlZdLjj9tdCUBmIHg++US64AK7qwAQQGQGfOrrvf+Wfe89uysBYl5ENcZffPFFDRs2TIWFhb5lN9xwg+/PvXv3Vrt27TRw4ECtXbtWhx9++CEdZ8KECXrooYdaXC8QUM88I61cKb3yipQQUT+6gC3IDNhuxw7v8OkPPMCTH0CYIzMQaE0ej77cUhGwp8R/yUiqbnTr222VOrYg64DbR7R//ENas0Z68klpjyetALuQGQiar76SHnnE7ioABBCZAZ+nnpJGj5bS0+2uBIh5EXNr+caNG/Xxxx/r+uuv3+92ffv2lSStWbNGklRQUKCysjK/bXZ/va95PMaPHy+n0+l7lZSUtLR84NAZ422qbNkiTZ5MUxywgMyA7TZvli67THrsMZriQJgjMxAMq7ZXqs7tCfpx1jtrtKOmIejHsYUx0p//LP38s/eDxLiI+fgCUYzMQND88IN0xBFc64AoQmbAZ+NG6dNPvdPsAbBdxPxra+rUqcrLy9M5B5ibc9myZZKkdu3aSZL69eunlStXqry83LfNnDlzlJmZqZ49eza7j+TkZGVmZvq9AFs0NXnnpM3Olv7yF56QACwiM2CrH3+Urr5aeuEF6eij7a4GwAGQGQi0yoYmrd1Vc+ANA8Ah6Zsyp4wJ1rPpNmlslG680Tt91KOP8nsQwgaZgaB54w3p4ovtrgJAAJEZkOS92fPOO6WJE/k3LRAmIuLRU4/Ho6lTp2rkyJFK2ONp2bVr1+rVV1/V2WefrdatW2vFihW6/fbbdcYZZ+jo/34QPXjwYPXs2VMjRozQY489ptLSUv3hD3/QmDFjlJycbNdbAg6sqkoaOdJ7J9lll9ldDRAxyAzY6ptvpLvu8k57scdQaQDCE5mBYNhQUSOHFLRh1PdkJLkamlRR36iclKQQHDEEKiq8N5hddx1z7SKskBkIGrfb+yTh/ffbXQmA/9/efcdHVeX/H39PekI6gYReLFhA2bUgLrKr8gURC8r6s60iuqgIrlRdBEVYVxAbqwvqKsUGCgoWBJQiCIpiQaSJgkBACT0F0mfO74+7ZI0GmCQzc2Ymr+fjkQckM5n7mQu579z7ueccHyEzUGHOHKltW+mUU2xXAuC/QqIxvmjRImVnZ+vWW2+t9PWYmBgtWrRIEyZM0OHDh9WsWTP16tVLI0eOrHhOZGSk5s6dq379+qljx46qV6+eevfurTFjxgT6bQDey8lxLgY98IB0wQW2qwFCCpkBaz7+WBo7Vpo5U0pPt10NAC+QGfA1t8doW15hQJriR7gkbc0tVFpWGDTG16yRBg1yliI5+2zb1QCVkBnwmwULpG7dmEYdCCNkBiRJ+fnSxInS++/brgTAL7hM2M255nv5+flKSUlRXl4e05DA/zZulPr3lyZN4k4yVBvHK/v4N6ij5s6VXnxReu01qV4929UAXuOYZRf7P/zsPlyiT3YeCPh2o1wuXX5SplyhPD3jlCnSe+9JL7wgZWTYrgZV4JhlF/s/jF1zjdM4adjQdiU+V+b26GBxmQ4Wl6mgtFxuj5HLJUVHRCg1LlppcdFKjo1SRCjnF6rEMcsuRs+ayQAARIVJREFU9n+QGDhQuuIK6aKLbFcCBLVAH7NCYsQ4UGd8/LH08MPS9OlSVpbtagAA3nj1VWeUx8yZUkwYjNYDANRYbnFZwKZR/6VyY1RY5la9mBA8xc/Lc0aJN28uvfmmFBlpuyIACJzvv5cSE8OqKW6M0b6iUm0+eFi7DpVIcmY3kf6Xjy5JJs/5e1SES61TE9Q6tZ4SoskAAGHiyy+d33NpigNBJwTPmoEw9frr0qxZ0uzZzkkRACC4GSM99piUnS299BIX8gEAyi0uC3hTvGLbJWWh1xhfulQaM8b56NTJdjUAEHjjx0vDhtmuwmfySsr05a5c5ZWU65djwH+djb/8vNxj9MOBw/r+wGG1TInXGQ2TFcW08gBCWVmZNHy4M6sggKATYmfNQBgyRnr8cWnrVumNN6QofiwBIOi53c7otsaNpWeekZj6DwAgKb+03Mp2XZIKLG27RoqKpJEjpUOHpHfekZKSbFcEAIG3Y4dUUCCddprtSmrNGKMfDh7W+r0F//tadb7/v39uyyvS7sMlOrdRmuonMBsXgBA1frzUu3dYzQYChBNuvwNsKiuTBgxwGiwTJ9IUB4BQUFQk3XijdM450t//TlMcAFCh3GNrvLjktrjtalm4UOrRw5lW8vnnaYoDqLvGj5eGDLFdRa15jNEXu3K1bm+BjGq/nEhxuUfLduzXzvwiX5QHAIG1fr30zTfOdSMAQYkuHGBLbq5z59hf/iJdc43tagAA3ti/X7rpJme0+P/9n+1qAABBxuqtUsF+o1ZOjnTvvVKDBtK777J8FIC6bfNmae9e6dxzbVdSK8YYfbUrVzsLin33mv/9c9WuXEW4XGqcFOez1wYAvyovlwYPlqZODf7fzYE6jMY4YMPmzdIdd0iPPiqdfbbtagAA3ti2TbrtNumJJ6T27W1XAwAIQlER9i6ARQXrxbfycuk//3Ga4ePGkaEAIEmjRkmjR9uuotY2HTisHT5siv/aqp8P6sKWGUqJjfbbNgDAZ558Urr+emfZPQBBi6nUgUBbtkzq10966SWa4gAQKlavdpriU6ZwQR8AcFQpcdFWRo0bScmxQXjf+4IFUrduzpJR779PhgKAJK1aJaWmSm3a2K6kVgpKyrVxX8Hxn1gLRtJXu/JkTIgsFwKg7vruO+mzz5wZYgEEtSA8cwbC2JQp0gcfSG+/LdWrZ7saAIA3PvxQ+te/pFmzpPR029UAAIJYamyUdljbdhCNplu7VnrwQen0051zH9YRBwCHMc5I8cmTbVdSK8YYfZWT6//tSMotKdOW3EKdmMZ1NABByu12ltz7z3+YQh0IATTGgUBwu6Xhw6XISGnGDCmCyRoAICQ895y0fLn01ltSHGvbAQCOLS3OTnM6JtKluKggOMfYulX65z+d6dOfflpq1sx2RQAQXF59VbroIikry3YltbKnsFQHissCtr2N+wrUOjVBETScAASjCROkXr343RcIETTGAX87dEi69Vbp0kulW26xXQ0AwBvl5dKQIVJamnPxigswAAAv1I+PUVxkhIrdnoBt0yWpRXKCXDazascOpyGemyuNGCG1a2evFgAIVgcPOsvqzZ9vu5Ja25p7WC45I7oDocxjtOtQsZokxQdoiwDgpbVrpZUrnVkGAYQEGuOAP23bJvXtKz3wgNS5s+1qAADeyM2V+vSRrr1Wuu4629UAAEKIy+VSq9QEbdx/KGDbNJJapiYEbHsV2zVGRdk7FDl+vCJ27ZLn739X7LnnBLwOAAgZI0dKDz0kRQfR0hc1UFzu1q5DJQFrikvOTWA/5hbSGAcQXEpKnCnUGVABhBQa44C/LFkijR0rvfii1KKF7WoAAN7YvFm64w7n+H3uubarAQCEoJYpCfpu/6GANAxcckapJ8UE5tQ+r6RM2XlFKv56tdKnv6K4nJ/1/V/76+CZv5ckxWzerfS4aDVIiFHzlATFRgbB9O4AEAw++8xpoHTqZLuSWttbWBrQprjk3AS2r7BUHmOYTh1A8HjgAenuu0N+eQygrqExDviaMc56el9+Kb3zjpQQ+NEbAIAaWLbMmQb2pZekpk1tVwMACFHx0ZE6NSNJG/YVBGR77TOT/b6NnEPFyv76W0WuWKEmC+aqsHFTbf/z9TrYrn2l55W6Pco5XKKcwyVav7dAzZLj1aZ+ohID1LgHgKBUVOQsM/Hmm7Yr8Ym8krKATqN+hJGUX1Ku1LjQHnEPIEwsW+bMOHjllbYrAVBNnJ0CvlRcLPXvL518svTyy0yhAgChYvJkaeFCac4cqV4929UAAELcyen1tCO/SIdKy/3aODilfqKSY/3UIDBGpXn5yp71tuJnzVSLwsP66ZLLtHLiVJmYmON+u0dSdn6RdhQUqV2DZLVOtbwOOgDY8sAD0uDBUlqa7Up84mBxWcCb4kfkFpfRGAdgX16eNHq0MygOQMihMQ74yk8/SbfeKg0cKHXvbrsaAIA33G7p3nuluDhp+nQpgilfAQC1F+Fy6dzGqVq6fb/cxvftA5ektLhotamf6LsXLS521kdcvlw6cEDl+/YrLyJaRRdcqB8H/V2HWp1Q7Rt/jZwJtdbsydeuQ8Xq0DhN0UyvDqAuWbHCaaD06GG7Ep8pLHNb2a5LUlG5nW0DQCX33CM9/LCUlGS7EgA1QGMc8IWVK6WRI6Vnn3VGiwMAgl9+vtSnj9Szp3TTTbarAQCEmZTYaJ3fNE2f7Dwgjw974y5JSbFROr9puu/WWfV4pAcflDIypDFjdNBEaMUhj8qjY3w2KnBvYamW7zigC5ql0xwHUDccPiyNGiXNnm27Ep/y+OGGL2/542YzAKiWN95wlt87/3zblQCoIRrjQG1NniwtWOBMv5vs//X9AAA+sHWr9Ne/Sv/4ByczAAC/aZAQq05N62vlTwdU7jE+aTKnx0erY5N0xfiyufzCC1JBgfTPf6rAuLR8+z6VR/u2+WDkrEu78qeD6tTMh019AAhWgwZJ998vpaTYrsSnbB6/yQ4AVv34ozRtmvTuu7YrAVAL3KYN1FRJibOe+Pbtzp1iNMUBIDSsWCH17StNmUJTHADgdxkJMeraqoGy6sXW+DVc//1o1yBJnZvV921TfNEiad48adw4magoffHzQbl9OcT9F4ykfUWl+v7AYb+8PgAEjRkzpIYNpYsvtl2Jz/k0g6rBSIplxhEAtpSWSv36OTPGRkfbrgZALTBiHKiJHTucpspdd0lXXGG7GgCAt156SZo715nlg7WgAAABEhsVqfOapOmngmJtOnBIeSXlcklejSB3SWqaFKc29ROVHOvji3A//CA99JD0+utSSoq2HDys3JJy326jChv3FahJUpySYrgkASAMbd4svfJK2I4oTIuLVm5xmc+W2qiOFF/nIAB4a8QI6fbbpZYtbVcCoJY4CwWqa9Ei6dFHnbvDTjzRdjUAAG94PM40hh6Pc/E/MtJ2RQCAOsblcqlpcryaJsfrQFGptucXaX9hqQpKy3/TXIhyuZQWF60G9WLUIiVB8VF+yK3cXOmmm6QXX5SaNlVxuVvr9ub7fjtH8c3uPF3QrH7AtgcAAVFS4gyiePFFKSo8L7umxkZbaYpLUkpceO5TAEFu/nypsFDq1ct2JQB8gN8mAG95PNK4cc6dv++8IyUk2K4IAOCNw4elW2+VunVz/gQAwLL0+Bilx8dIktweo0Nl5XJ7jFwul2IiXEqIjpTLn+uoFhdLV18tjR8vtW0rScrOK5KfZlD/DSNp739vCmDUOICwMmyYs+xe8+a2K/Gb+gkxVrabHBOl6AimUgcQYD//LD3xhPTee7YrAeAj/DYBeCM3V7ruOik9XZo8maY4AISKr7+WevZ0Rm3QFAcABKHICJdSYqOVHh+jtLho1YuJ8m9T3O2WbrjBadx07ixJMsbox9xC/22zCi5J2wK8TQDwq2nTpORk6corbVfiV0kxUUqPC/yU5q1SuRYHIMDcbunOO6VnnpHi421XA8BHuDUbOJ41a6RBg5zR4ueea7saAIC39uyR/vY3Z+r0pk1tVwMAgH3GOOc2nTpVmgryYHGZCsvdgS1F0vb8QrVtkOTfGwEAIBC+/NJZU3zWLNuVBETr1AQdyMkL2PYiXFKzZJpSAAJs7FjpqqukU0+1XQkAH6IxDhzLyy9Lb70lzZwpZWTYrgYA4K2yMmeE+PjxPm+KG2NUWO5WucfIJSkqIkLxURFc1AcABL+nn3ZGvgwaVOnLB4vLrJRT6jYqLvcoPtoPa6gDQKDs2SPde69z/SiybhzPmiTFa/2+AhWVewKyvVapCYqJZOJTAAG0ZIm0ZYs0YoTtSgD4GI1xoCpFRdKQIc7U6bNn15kTGwAIG48+Kl1wgXT++T55uYLScm3PK9T+olLlFpfJ/as1WKMiXEqPi1ZGQoxapCQoPorcAAAEmQULpI8+kmbMkH51M1ducZlcckZxB1puSRmNcQCh68gNuU8/LaWl2a4mYCIjXPp9Vqo+2XnA79uKi4zQaRlJft8OAFT46SfpkUecmUAYBAGEHRrjwK99/72z3t7gwVL37rarAQBU10cfOWuLT59e65faW1iiTfsPaU9h6TEbBuUeoz2FpdpTWKqN+w6pSVKcTk5PVKqFtfcAAPiNf//bueH3tdeqXB8xt6TMSlPcJSm/pFyNEi1sHABqyxhn6aY+faS2bW1XE3CZ9WLVLDlOO/OL/Zohv8tKUXQEo8UBBEhpqfTXv0qTJkkJCbarAeAHNMaBX5oxQ3rlFWnqVNajBYBQdOCANHy4s654XFyNX6bc49HaPfnamlekI/cGe3uxx0j6qaBYPxUU69SMRJ2cnqgI7jAGANiycKH06afOn0eZCavcY6Mt7ijzBGYaXgDwuSeekFq2lHr1sl2JNe0zU5RfUq78knK/NMdPqZ+oRok1P68DgGq7916pb1/p5JNtVwLAT2iMA5IzdfqgQc7U6e++K0XxowEAIemGG6SHH3YuUNVQQUm5PvnpgArL3JJqNq3ske/ZsO+QdhWUqGPTNMUxvToAINDKy6Vx46Q33zzm8lD22uIAEKLmzJF++EF67jnblVgVHRGhTk3TtXzHARWU+rY5fmJaPZ1anylFAATQG29I0dHS1VfbrgSAHwX1PDQPPfSQXC5XpY9TTjml4vHi4mL1799f9evXV2Jionr16qXdu3dXeo3s7Gz16NFDCQkJatiwoYYNG6by8vJAvxUEs02bpMsvl3r2dNYOoSkOhCQyA7r3XqlrV6lLlxq/RH5JmZZm71PRf5vivpBbUqZl2ftVXO671wRQO2QG6oxPPpFOP/24695GWpzZxOa2AW+QGfiNL76Qpk1zlqngGKbYqEh1bl5fWYmxtX4t138/zmiYrHYNkuRi/yLEkBkhbONG6eWXpbFjbVcCwM+CvgN4+umna9GiRRWfR/2iaTlo0CC9//77mjVrllJSUjRgwABdffXV+uSTTyRJbrdbPXr0UFZWlj799FPt2rVLN998s6Kjo/XII48E/L0gCL32mrMG7UsvSU2a2K4GQC2RGXXYG29IO3dK48fX+CWKytxavuOAyj3GpyMdjKTCMrdW7DygPzWvryjWxwOCApmBOmHcOGnChOM+LTkmSgWlgb/gaiQlxQT9ZQmAzMD/bNsm/f3v0ltvOaMKIUmKiYzQeY3TlJ1fpG9258ttanZGlRQbpXMbpSo5ln2L0EVmhKCCAmnAAOnVVxk0B9QBQf9THhUVpaysrN98PS8vT5MnT9b06dN10UUXSZKmTp2qU089VZ999pnOO+88ffjhh9qwYYMWLVqkzMxMtW/fXv/4xz9033336aGHHlJMTEyg3w6CRVGRNHCg1KCB9M47BB4QJsiMOmrdOumZZ6T582v8EsYYfZWTq1K3xy/TyRpJ+SXlWr+3QGdmpvhhCwCqi8xA2Fu0SMrKktq0Oe5TU+Oi9fOhYitTqqfE0fxA8CMzIEnas0e69VZp6lQpNdV2NUHH5XKpRUqCsurFaVteoTYfPKwSt0cuVb1kxy+/nh4frZPS6qlRYpwiGCWOEEdmhBhjpH79pFGjpEaNbFcDIACCfsjSDz/8oMaNG6t169a68cYblZ2dLUn66quvVFZWpi6/mC71lFNOUfPmzbVy5UpJ0sqVK9WuXTtlZmZWPKdbt27Kz8/X+vXrA/tGEDw2bnSmTu/Vy1mHlqY4EDbIjDqooMC50elf/5KSkmr8MjsLirWnsNTvDYEtuYU6UFTq560A8AaZgbA3aZJ0111ePTU1LtpKUzzSJSVGH33tcyBYkBlQQYH0l784N+S2aGG7mqAWGxWhNvUT1f2EhvpD03SdmpGorHqxSoiKVGxkhOIiI5QUE6VmyfFq3zBZXVpm6E/NM9QkKZ6mOMICmRFiHntMOvtsqXNn25UACJCg7gh26NBB06ZNU5s2bbRr1y6NHj1aF1xwgdatW6ecnBzFxMQo9Vd3aGZmZionJ0eSlJOTUylEjjx+5LGjKSkpUUlJScXn+fn5PnpHsMoY6YUXpLlznfVCGje2XREAHyIz6qj775euuUY666wav0S5x+ib3Xk+LOroXJK+yslTl5YZrJcHWERmIKwZI33+ufP3c87x6lsy4mMUHeFSmSdw7XGXpMZJceQhgh6ZARUXSzfeKI0eLZ1+uu1qQkaEy6XMerHKrFf7tceBUEFmhJj586XvvpMmT7ZdCYAACurGePfu3Sv+fsYZZ6hDhw5q0aKFZs6cqfj4eL9td+zYsRo9erTfXh8W7N8v9e/vNE7efltifVcg7JAZddCsWVJennTHHbV6mZ8KigLWCDCSCkrLdaCoTPUTmAINsIXMQFibPFlavlx69lmvvyUywpn+dsvBwwEbOW4ktUqpF6CtATVHZtRxbrfUp48zA0fHjrarARDkyIwQ8v330oQJzjKr3KgJ1Ckh1R1MTU3VySefrM2bNysrK0ulpaXKzc2t9Jzdu3dXrOGRlZWl3bt3/+bxI48dzfDhw5WXl1fxsWPHDt++EQTW0qXOaMKhQ6Vhw2iKA3UEmRHmvv1WmjhRevHFWr/Uj7mFPijIey5JP+YdDug2ARwbmYGwsWmT9MYbzjTqvxptdDytUhICOp16YnSk6sezvjhCD5lRhxjjDLK4/HLpkktsVwMgBJEZQSo/X7rzTmnKFCkuznY1AAIspDqEhw4d0pYtW9SoUSOdddZZio6O1uLFiyse37Rpk7Kzs9Xxv3dwduzYUWvXrtWePXsqnrNw4UIlJyfrtNNOO+p2YmNjlZycXOkDIaisTBoxQnr1Vendd521QgDUGWRGGDtwwBmx8fLLUkztRl0fLi3XweIyHxXmHSPpp/xiuQM4XS2AYyMzEBaMkebNk264QapX/ZHYSbFRap2S4IfCqtY+M4Vp1BGSyIw6whhp4EDpzDOd4yoA1ACZEYQ8HunWW6V//lNq0sR2NQAsCOqp1IcOHarLL79cLVq00M8//6xRo0YpMjJS119/vVJSUnTbbbdp8ODBSk9PV3Jysu6++2517NhR5513niSpa9euOu2003TTTTdp/PjxysnJ0ciRI9W/f3/FxrK+TVj78UenaXLLLU7IAQh7ZEYd4XZLw4c7IzeaN6/1ywW6KX6ER1J+abnS4hgpB9hAZiAsffKJ9MUXtZpN5fQGSfrpULFK3B4fFlaZS1Kz5Hg1ZM1ZhAgyow4yRrr3XumEE6R+/WxXAyCEkBkh4MEHpUsvZXkMoA4L6sb4zp07df3112v//v1q0KCBOnXqpM8++0wNGjSQJD311FOKiIhQr169VFJSom7dumnSpEkV3x8ZGam5c+eqX79+6tixo+rVq6fevXtrzJgxtt4SAuHVV6Xp06XnnpNatrRdDYAAITPqiNdek6KjpWuv9cnL5ZaUySUFdOrYI/KKy2iMA5aQGQhLS5c6Nwcn1HzUd3RkhM5ulKpPdh7wXV2/4JIUFxWhdg0ZxYTQQWbUMcY4sw9mZUl/+5vtagCEGDIjyM2a5UyjfuuttisBYJHLGMM8nseRn5+vlJQU5eXlMQ1JMMvPd05aWraURo6UooL6vg/ALzhe2ce/gR/9+KN0223SW29J6ek+eclPdh7Q7sMlPnmt6nBJap2WoDMbpgR828Avccyyi/0Pn7rySmnmTMkHo4m25RXq65w8HxT1Py5J0REu/bFFhpJiOFcLRRyz7GL/B8jo0c5yTcOH264ECGkcs+xi/1fhm2+cnsGcOc6ACwBBI9DHLM5GER5WrHCmQRkzRurUyXY1AABfKyuTeveWJk3yWVNcksr8OFXs8ZSzxjgAwFfy8yWXyydNcUlq+d+1xlf/tzle28RySYqNjNAFzerTFAcQvI4sxUdTHADCy88/S4MHOyPGaYoDdR5npAhtJSXSqFHS3r3S229L3AEHAOGpTx9p4ECpXTvblfgOfXEAgK+8+ab05z/79CVbpiQoNTZaq34+qENl7lq9VtOkOJ2ZmaKYyAgfVQcAPmSMc20pMtIZdAEACB+HD0u33CI9+6xUv77tagAEAc5KEbrWrZMuu0zq0EGaPJmmOACEq6eekjIzpV69fP7SkREun79mKGwbABBmZs+WrrrK5y+bGheti1s2UJv6iYqqRm4deWZidKQ6NE7VOY3TaIoDCE7GSPfdJyUmOs1xF7+jA0DY8HicJfmGD5fatLFdDYAgwYhxhB6Px2mSrFwpvfKKlJVluyIAgL8cPCh98IE0f75fXj4pJkr7CksDPnjbSEpkKlkAgC9s3iw1aiTVq+eXl4+McOn0jCS1SU/UjvwibcsrVF5xmY62GEl0hEsNEmLUOrWeGiTEyEWTCUCw8nike+6RTj5Zuvtu29UAAHzt/vul//s/6cILbVcCIIhwRRahZft2qX9/6YornDVBuMgCAOFt1CjnRMZPx/vUuGhrM5qnxrKuFQDABx5/3FlyxM+iIlxqlZqgVqkJ8hij/JJyFZSWq9xj5HJJ0RERSouLUnxUJM1wAMHP7Zb69ZPOOUfq29d2NQAAX5s61ZkV5LbbbFcCIMjQGEdoMMYZHT5jhvTMM9KJJ9quCADgb6tXS/n5UufOftuEzeZ0Shy/hgEAamn8eKlxY+n88wO62QiXS6lx0UqN4yYvACGovFz661+liy+WbrrJdjUAAF9butSZfXD6dNuVAAhCXJFF8Nu3z5nSql076b33pCj+2wJA2Dt8WBo61Lkhyo9SYqOUEBWpwnK3X7fzSy5JDRJiFB3BWqsAgFp4+GGpqMj5EwDgncJCqXdv6dprpT//2XY1AABf+/576ZFHpDlzJK67AKgCRwYEt7lznZOVYcOcqXRpigNA+DNGuusuacQIqWFDv27K5XKpdWqCX7fxa0ZSq1T/rAMLAKgDSkulAQOcC33//CfLSwGAtw4edJrh/frRFAeAcLR/v3TnndK0aVI9rrsAqBpdRgSngwelIUOk+vWd5nh8vO2KAACBYIwzUvz886WLLgrIJpunxGv9voKArTUeExmhRomxAdoaACCsbN/u3DzWt6/Us6ftagAgdPz0kzNS/NFHpbPOsl0NAMDXioqkv/xFeuopZ6khADgKGuMIPu+/Lz3xhDP6oWNH29UAAALF7ZbuvVfKzJTuuCNgm42LitQp9RO1cf+hgGzvzIbJimB0HwCgOsrLpf/8xzlXmjBBOukk2xUBQOjYtMkZJf788xw/ASAcud1Snz7OQLszz7RdDYAgx1TqCB65udJf/yp99JFzwYemOADUHQcPOktnnHGG0xwPsDb1E5UYEyl/tqtdkjITYtU0Kc6PWwEAhBVjpDfflLp1c5aVeu89mjoAUB1ffCH17y+99hrHTwAIR8ZIgwZJPXpIXbrYrgZACGDEOILD/PnSY49J//iH9Ic/2K4GABBI33wjDR7sTGt4zjlWSohwuXROo1Qty94v44c51V2SoiJc+l1WilyMFgcAHI8x0sKFzkxaXbo4DfGEBNtVAUBomTtXeu455waj1FTb1QAA/OGxx6RGjaSbbrJdCYAQQWMcduXlOWvJJic7Jyxc7AGAuuXll6XZs6WZM6WMDKulpMXFqEPjNH3200GfrjfuktN479SsvhKiI334ygCAsLRypfTII1L79k4+pqTYrggAQs+//y199ZVzrhETY7saAIA/TJ8uZWdLzzxjuxIAIYTGOOz54ANndOCYMVKnTrarAQAEUkmJM0o8LU166y0pMjgaxo0S43RekzR9/vNBGaNaN8iPjBTv1Ky+0uKifVEiACBcrVwpjR8vtWghvfiilJlpuyIACD1utzRsmHNT0ZQpErM1AUB4WrJEevttacYMjvUAqoXGOAIvL885SalXj1HiAFAX7dgh9e3rrPV3+eW2q/mNRolx6tKygVb9nKvckrJavVZmvVj9PitFcVHB0fgHAAShTz91GuKtWkmTJjlTQQIAqu/wYenWW6XLLmNKXQAIZ2vXOr8/z54dNAMtAIQOGuMIrLlzpSeflEaPli64wHY1AIBAW7xYGjvWWevvxBNtV3NUiTFR+lOL+tp88LC+P3BIpW7vxo675IwyT4iO1Kn1E9U8OZ41xQEAVVuxQnr8cemEE5xczMqyXREAhK6cHKl3b2n4cOlPf7JdDQDAX3bskO65x1lyiAF3AGqAxjgCY88eacgQqXFj6f33pfh42xUBAALJGGf5jO++k959NyROXiJcLp2cnqgT0+rpp4Jibc0t1MHiUh2tRx4V4VJGfIxapyYos14sDXEAQNWWL5cee0xq00Z6/nmmTAeA2lq71mmSTJoknXKK7WoAAP6yb590yy3OskMZGbarARCiaIzDv4yRXn1VevllpyHy+9/brggAEGh5edLtt0udO0tTp4bc2k8RLpeaJcerWXK8jDEqKC1Xfkm5yo2pWEM8NTZaCdGRNMMBAFUzRpo3z2nanH66czGvYUPbVQFA6Hv7beeY+sYbUoMGtqsBAPhLQYF0443ShAlS69a2qwEQwmiMw3+2b5cGDpTOO8+5CBQdbbsiAECgff21NHSo9MgjTh6EOJfLpeTYaCXHkmkAAC+UlzvNmmnTpAsvlF57TUpNtV0VAIQ+Y6SHH5Z27XLWmI2JsV0RAMBfSkqcpvioUVK7drarARDiaIzD99xuaeJE6cMPpaeekk46yXZFAIBAM0Z69llnTfFZs6T69W1XBABA4BQVObOkvPWW1KuXs4wIy0kBgG8cPiz17StdcIH0wAO2qwEA+JPb7Uyf3q+fdP75tqsBEAYibBeAMLNhg3TZZVJcnHPxh6Y4ANQ9+fnSzTdLhw7RFAcA1C25uc4sKVdc4YwM/+AD6a67aIoDgK9s3y717Oks1dSvn+1qAAD+ZIzzu/QVV0jdu9uuBkCYYMQ4fKO0VBo71mmMv/ii1KSJ7YoAADasWSMNGiSNGSN16mS7GgAAAmPLFumZZ6TNm52Ld8OHSy6X7aoAILwsXy6NHi298ILUqpXtagAA/jZ8uDN1+vXX264EQBihMY7aW7nSmbrqjjukBx/kAhAA1EXGODdGvf++s5Zqgwa2KwIAwL+McZo0Eyc6M2b97W/SWWfZrgoAwo8x0tNPS599Js2ZIyUl2a4IAOBvjz3mzLo0YIDtSgCEGRrjqLmDB6X773dOUGbOlNLTbVcEALDh0CHnROWkk6TZs6UIVmoBAISx0lLn/OeVV6Szz5aeekpq3Nh2VQAQng4dcqZMb99emj6dwRgAUBdMmSLt2CH961+2KwEQhmiMo/qMkWbMcAJq9GjpD3+wXREAwJZ166R77nFmDvnTn2xXAwCA/+zfLz3/vLRkiXTNNc6oxYQE21UBQPjatMlZnuLBB6U//tF2NQCAQJg1S1q2TJo6lZuhAPgFjXFUz+bN0tCh0nnnSfPmSTExtisCANhgjHOS8vbbzsiNzEzbFQEA4B9r1kjPPivt3i3dfrv0978zOwoA+Ntbb0mTJzuzczArBwDUDXPnOsf/V1/l920AfkNjHN4pKZHGj5dWr5aefFJq3dp2RQAAW/LynKnTTznFGS0XGWm7IgAAfKukxLko99pr0gknOLOjnHqq7aoAIPyVl0vDhzt/vvOOFB1tuyIAQCAsWuTMUPv661IUbSsA/sNtNzi+ZcukSy+V2rRxLg7RFAeAuuvzz6WrrnLW+RsxgqY4ACC8bN8u3X+/dPnlzrq2b7whPf00TXEACISff5Z69pTOOkt66ima4gBQVyxf7qwn/tprzFALwO+CujE+duxYnXPOOUpKSlLDhg3Vs2dPbdq0qdJz/vSnP8nlclX6uPPOOys9Jzs7Wz169FBCQoIaNmyoYcOGqby8PJBvJTTt2yfddpv05pvS7NnS//t/rOsBIGiRGX7m8UjjxjknKrNnS+efb7siAKgxMgOVeDzSggXSn/8sPfCA1KOH9MEHzrTpiYm2qwNgGZkRIAsWSL17S48/Ll13ne1qAKBGyIwaWLVKeuQRacYMKT7edjUA6oCgnpNi2bJl6t+/v8455xyVl5fr/vvvV9euXbVhwwbVq1ev4nl9+/bVmDFjKj5PSEio+Lvb7VaPHj2UlZWlTz/9VLt27dLNN9+s6OhoPfLIIwF9PyHDGOmll5w7tP75T+ncc21XBADHRWb40a5d0p13Spdd5mQDN0kBCHFkBiQ5a4a//LLTjOnSRZo0SWrY0HZVAIIMmeFnZWXSyJFSQYH07rs0RQCENDKjmtascTJg1ixuSAUQMC5jjLFdhLf27t2rhg0batmyZercubMk5w6r9u3ba8KECVV+z/z583XZZZfp559/VmZmpiTpueee03333ae9e/cqxoupOfLz85WSkqK8vDwlJyf77P0EpbVrpb//XbroImcdPdbzAEJKnTpeHQeZ4SPz50tPPOFMI3vaabarAeBjYXfMqiEyow5xu53R4C+95Hzeu7fUrRtLgwBe4JjlIDN8aNs25wbc226TrrnGdjUAfCgsj1k1QGYcw8aN0t13O0sX1a9vuxoAFgX6mBXUU6n/Wl5eniQpPT290tdfe+01ZWRkqG3btho+fLgKCwsrHlu5cqXatWtXESKS1K1bN+Xn52v9+vWBKTwU5OVJgwZJjz4qPf+8NGQITXEAIY3MqKWSEicL3n9feu89muIAwhqZUQds2yY9+KDTBF+/3rnh6403pEsvpSkOoFrIDB+ZM0e64w5ntg6a4gDCFJlxFFu2SP37O7MS0hQHEGAh0/n0eDwaOHCg/vCHP6ht27YVX7/hhhvUokULNW7cWN9++63uu+8+bdq0SbNnz5Yk5eTkVAoRSRWf5+TkVLmtkpISlZSUVHyen5/v67cTPIyRpk+XpkyRRoxwRooDQIgjM2rphx+cE5S77pJ69rRdDQD4FZkRxkpKpHfecc530tKkW2+VRo9mSRAANUZm+EBxsXTvvVJ0tHMDrhcjHwEgFJEZR7Ftm9S3r7Ok0a/eJwAEQsg0xvv3769169ZpxYoVlb5+++23V/y9Xbt2atSokS6++GJt2bJFJ5xwQo22NXbsWI0ePbpW9YaEdeucadP/+EdnqlxORgCECTKjhoxxTkxmzpQmT5aaNbNdEQD4HZkRhtaskaZNc6ZnvPJK5++pqZaLAhAOyIxa2rhR+tvfnKX7LrvMdjUA4FdkRhW2b5f69JGmTpWaNrVdDYA6KiSmUh8wYIDmzp2rjz76SE2Pc8Ds0KGDJGnz5s2SpKysLO3evbvSc458npWVVeVrDB8+XHl5eRUfO3bsqO1bCC75+dLgwdK4cdJzz0nDhtEUBxA2yIwaOnBA+stfpJ07ndF1NMUB1AFkRhjZtUt64gmpa1dn/fDevaUFC6R+/WiKA/AJMqMWjJEmTpTuv985RtMUBxDmyIwq/LIp3rKl7WoA1GFB3Rg3xmjAgAGaM2eOlixZolatWh33e7755htJUqNGjSRJHTt21Nq1a7Vnz56K5yxcuFDJyck67SjrpcbGxio5ObnSR1g4Mm36VVdJPXpIr77KnVkAwgaZUQuLFjnr+g0c6CyrERUyE8oAQI2QGWGisFCaMcM5vxk6VDr1VGnePOnJJ6X27W1XByBMkBm1lJMj9eolud3SW29JjRvbrggA/IbMOIrsbKcpPmUKTXEA1gX1le/+/ftr+vTpeuedd5SUlFSxhkZKSori4+O1ZcsWTZ8+XZdeeqnq16+vb7/9VoMGDVLnzp11xhlnSJK6du2q0047TTfddJPGjx+vnJwcjRw5Uv3791dsbKzNtxdY69c706Z37sy06QDCEplRA8XFzqiN4mLp3XelevVsVwQAAUFmhDCPR1q+XHrlFafZ0rOnM+qEUeEA/ITMqIV33pH+/W9pwgTp9NNtVwMAfkdmVCE7W7rlFpriAIKHCWKSqvyYOnWqMcaY7Oxs07lzZ5Oenm5iY2PNiSeeaIYNG2by8vIqvc62bdtM9+7dTXx8vMnIyDBDhgwxZWVlXteRl5dnJP3mdUNCbq4xgwcbc+ONxuzYYbsaAH4W0serWiIzqunbb425+GJj3n3XdiUALAqZY5aPkRkhaO1aY+6/38mu0aON2bLFdkVAnVNXj1lkRg0UFBhz++3GDB1qTHGx7WoAWBBSxywfIjN+JTvbmAsvNObHH+3WASCoBfqY5TLGGP+13cNDfn6+UlJSlJeXF3zTkByN2+2MnHj9dWc04EUX2a4IQACE5PEqzAT9v4HHI/3rX9Innzjr/GVm2q4IgEVBf8wKc+z/49i82Tmf+fhj6bTTpOuukzp0kFwu25UBdRLHLLtCZv+vWuXMWDhyJNeigDosZI5ZYSoo9v+OHVLv3tKLL0qtW9upAUBICPQxK6inUkcNrVghjRnjTCu4YAFrxQIAHDt3Sv37S927S7Nm0VgAAASfnTulN96QFi6UWrSQrr9eGj5cioy0XRkA4FhKSqR//EPavt1ZSzwtzXZFAABbdu6kKQ4gaNExDSc7dkgjRkgpKc7IivR02xUBAILFrFnSCy9IzzwjtWljuxoAAP5n714np+bNkzIypGuvlf72Nyk62nZlAABvrF4tDRsm9esnPfyw7WoAADbt3CndfLNzDYqmOIAgRGM8HBQVSY8/Ln3xhfTII1LbtrYrAgAEi/x8adAgqVEjae5cKSbGdkUAADjN8Lfflt5/X4qPl/78Z6c5Hh9vuzIAgLfKypzrUN9/L82YITVoYLsiAIBN27dLffo4TfETTrBdDQBUicZ4KDPGmZ5q0iTp7rud9ZuYFhcAcMSSJc6IjTFjpE6dbFcDAKjrfvpJmj1b+vBDKSnJWfrp1VelxETblQEAqmvNGmnoUKlvX2nUKNvVAABs27zZyYSpU6WWLW1XAwBHRWM8VH37rTNteseOzpSDcXG2KwIABIvCQmc91vJy6d13aTgAAOzZutW5mXfJEqlhQ+mqq6SZMxkZDgChqqxMevRRae1a5+amzEzbFQEAbPvuO+muu6RXXpGaNrVdDQAcE43xULNvn/Tgg07T47nnpCZNbFcEAAgmK1c6M4jcd5/UtavtagAAddF33znN8OXLpebNpauvdtYMZzkPAAht69ZJgwdLt9ziDNZg1kIAwNq10sCB0vTpUlaW7WoA4LhojIeKkhLp3/92ph186CFnpDgAAEeUlDj5sHu304xITbVdEQCgrnC7pc8+c2Yp+eYb6aSTpF69nJu0ojjlBICQV1IijR0rbdggvfSS1KiR7YoAAMHg66+le++VXn9datDAdjUA4JUI2wXgOIyRZs2Sund3Tjzmz6cpDgCobPVqqUcPqUMHacoUmuIAAP87dMhZL7xPH+myy6TFi6XrrpMWLHBu6L3wQpriABAOPv1UuvRSqV076Y03aIoDAByff+4s4zdrFk1xACGFKxXB7LPPpDFjpM6dpfffZx0+AEBlZWXSuHHSxo3SjBmciAAA/GvnTmnuXOmDD5zPu3WT/vEP1hEEgHCUny/df79UWiq9+aaUlma7IgBAsFi+3JlJZNYsKTnZdjUAUC00xoPR1q3SAw9IKSnStGlSw4a2KwIABJsNG6RBg6TevZ01xVnfDwDgax6PMyvJ3LnSypXOKMHLL5deeUVKTLRdHQDAX+bOlZ580jnPuOgi29UAAILJ4sXShAnSzJmcEwAISTTGg0lurvTII9KPPzojL0491XZFAIBg43ZL//qXtGKFNHWq1Lix7YoAAOEkJ0f68ENp4UJp717pd79zlusYOVKKjLRdHQDAn/bskYYMkZo0YeZCAMBvzZsnvfCC0xQnIwCEKBrjwaCsTHr+eendd6X77pMuvth2RQCAYLRpkzNK/KqrpLfeYpQ4AKD2Skqcm60++ED69lspM1Pq2lV6/HHn7wCA8GeM9NJL0vTp0qOPOjdFAQDwSzNnSrNnS6+/LsXG2q4GAGqMxrhNxkjvveeM/LvpJmn+fEZhAAB+y+12pqlasUJ67jmpeXPbFQEAQpUxzo1WH34oLV3q3KTbqZN0ww3SuHFSRITtCgEAgbRhg3Tvvc6U6fPmSVFcKgQA/MrkydInn0ivvkpOAAh5HMVs+fpradQo6ZxznJHi9erZrggAEIw2bpQGD5auuca5M5dR4gCA6srNddYC/OADaft2qU0bqVs36bbbOA8BgLqqsNBZxm/rVunZZ6VmzWxXBAAIRk8+KW3bJr34IjfRAggLNMYDbedO6cEHpeho6T//kRo1sl0RACAYlZdLTzwhrVrlrN/UtKntigAAocLtlr74wmmEr1olJSY6yzXdf7/UsqXt6gAAtr3/vnOuMWSINHas7WoAAMHIGOmhhySPx5nxloEaAMIEjfFAOXhQeuwxZ4qqMWOkM86wXREAIFh9+60zneENN0hvvsnJBwDg+HbscKZHX7RIysuTzj7bGRU+YgTTHQIAHNu3S/fdJ7VuLc2dKyUk2K4IABCM3G5p4ECpRQtp6FDb1QCAT3GFJBAmT5ZmznSaHI88YrsaAEAwe+opZ5Tf1KnMKgIAODpjpCVLnPVg161zZhbp1k2aOFFKT7ddHQAg2Eya5NxANXasdOqptqsBAASze+6RzjtP+stfbFcCAD5HY9zfPB4pMlKaP581OAAAx+bxSA0aSK+9xihxAMCxuVzS999LffpIp59ObgAAjs7jkZKSpDlzyAsAwLF5PFKnTtJ119muBAD8gsa4v0VESLfcYrsKAEAoiIjgblwAgPf69bNdAQAgFERESDfdZLsKAEAoiIigKQ4grDGEGQAAAAAAAAAAAAAQ1miMAwAAAAAAAAAAAADCGo1xAAAAAAAAAAAAAEBYozEOAAAAAAAAAAAAAAhrNMYBAAAAAAAAAAAAAGGNxjgAAAAAAAAAAAAAIKzRGAcAAAAAAAAAAAAAhDUa4wAAAAAAAAAAAACAsEZjHAAAAAAAAAAAAAAQ1upUY3zixIlq2bKl4uLi1KFDB61atcp2SQCAIEVmAAC8RWYAALxFZgAAvEVmAIDv1ZnG+BtvvKHBgwdr1KhR+vrrr3XmmWeqW7du2rNnj+3SAABBhswAAHiLzAAAeIvMAAB4i8wAAP+oM43xJ598Un379lWfPn102mmn6bnnnlNCQoKmTJliuzQAQJAhMwAA3iIzAADeIjMAAN4iMwDAP+pEY7y0tFRfffWVunTpUvG1iIgIdenSRStXrrRYGQAg2JAZAABvkRkAAG+RGQAAb5EZAOA/UbYLCIR9+/bJ7XYrMzOz0tczMzP13Xff/eb5JSUlKikpqfg8Ly9PkpSfn+/fQgGglo4cp4wxlisJXWQGgLqE3KgdMgNAXUJm1A6ZAaAuITNqh8wAUJcEOjPqRGO8usaOHavRo0f/5uvNmjWzUA0AVN/+/fuVkpJiu4w6gcwAEA7IjcAgMwCEAzIjMMgMAOGAzAgMMgNAOAhUZtSJxnhGRoYiIyO1e/fuSl/fvXu3srKyfvP84cOHa/DgwRWf5+bmqkWLFsrOzibIjyM/P1/NmjXTjh07lJycbLucoMa+8h77ynt5eXlq3ry50tPTbZcSssiMwOFn23vsK++xr6qH3KgdMiNw+Nn2HvvKe+yr6iEzaofMCBx+tr3HvvIe+6p6yIzaITMCh59t77GvvMe+qp5AZ0adaIzHxMTorLPO0uLFi9WzZ09Jksfj0eLFizVgwIDfPD82NlaxsbG/+XpKSgr/ib2UnJzMvvIS+8p77CvvRURE2C4hZJEZgcfPtvfYV95jX1UPuVEzZEbg8bPtPfaV99hX1UNm1AyZEXj8bHuPfeU99lX1kBk1Q2YEHj/b3mNfeY99VT2Byow60RiXpMGDB6t37946++yzde6552rChAk6fPiw+vTpY7s0AECQITMAAN4iMwAA3iIzAADeIjMAwD/qTGP82muv1d69e/Xggw8qJydH7du314IFC5SZmWm7NABAkCEzAADeIjMAAN4iMwAA3iIzAMA/6kxjXJIGDBhQ5VQjxxMbG6tRo0ZVOR0JKmNfeY995T32lffYV75DZvgf+8p77Cvvsa+qh/3lG2SG/7GvvMe+8h77qnrYX75BZvgf+8p77Cvvsa+qh/3lG2SG/7GvvMe+8h77qnoCvb9cxhgTkC0BAAAAAAAAAAAAAGBBYFYyBwAAAAAAAAAAAADAEhrjAAAAAAAAAAAAAICwRmMcAAAAAAAAAAAAABDWaIx7YeLEiWrZsqXi4uLUoUMHrVq1ynZJAfXQQw/J5XJV+jjllFMqHi8uLlb//v1Vv359JSYmqlevXtq9e3el18jOzlaPHj2UkJCghg0batiwYSovLw/0W/G5jz/+WJdffrkaN24sl8ult99+u9Ljxhg9+OCDatSokeLj49WlSxf98MMPlZ5z4MAB3XjjjUpOTlZqaqpuu+02HTp0qNJzvv32W11wwQWKi4tTs2bNNH78eH+/NZ873r665ZZbfvP/7JJLLqn0nLqyr8aOHatzzjlHSUlJatiwoXr27KlNmzZVeo6vfu6WLl2q3//+94qNjdWJJ56oadOm+fvthT0yg8w4GjLDe2SG98iM0EZmkBlHQ2Z4j8zwHpkR2sgMMuNoyAzvkRneIzNCG5lBZhwNmeE9MsN7IZcZBsf0+uuvm5iYGDNlyhSzfv1607dvX5Oammp2795tu7SAGTVqlDn99NPNrl27Kj727t1b8fidd95pmjVrZhYvXmy+/PJLc95555nzzz+/4vHy8nLTtm1b06VLF7N69Wozb948k5GRYYYPH27j7fjUvHnzzIgRI8zs2bONJDNnzpxKj48bN86kpKSYt99+26xZs8ZcccUVplWrVqaoqKjiOZdccok588wzzWeffWaWL19uTjzxRHP99ddXPJ6Xl2cyMzPNjTfeaNatW2dmzJhh4uPjzfPPPx+ot+kTx9tXvXv3Npdcckml/2cHDhyo9Jy6sq+6detmpk6datatW2e++eYbc+mll5rmzZubQ4cOVTzHFz93P/74o0lISDCDBw82GzZsMM8884yJjIw0CxYsCOj7DSdkBplxLGSG98gM75EZoYvMIDOOhczwHpnhPTIjdJEZZMaxkBneIzO8R2aELjKDzDgWMsN7ZIb3Qi0zaIwfx7nnnmv69+9f8bnb7TaNGzc2Y8eOtVhVYI0aNcqceeaZVT6Wm5troqOjzaxZsyq+tnHjRiPJrFy50hjjHEAiIiJMTk5OxXOeffZZk5ycbEpKSvxaeyD9+uDo8XhMVlaWeeyxxyq+lpuba2JjY82MGTOMMcZs2LDBSDJffPFFxXPmz59vXC6X+emnn4wxxkyaNMmkpaVV2lf33XefadOmjZ/fkf8cLUiuvPLKo35PXd1XxhizZ88eI8ksW7bMGOO7n7t7773XnH766ZW2de2115pu3br5+y2FLTKDzPAWmeE9MqN6yIzQQWaQGd4iM7xHZlQPmRE6yAwyw1tkhvfIjOohM0IHmUFmeIvM8B6ZUT3BnhlMpX4MpaWl+uqrr9SlS5eKr0VERKhLly5auXKlxcoC74cfflDjxo3VunVr3XjjjcrOzpYkffXVVyorK6u0j0455RQ1b968Yh+tXLlS7dq1U2ZmZsVzunXrpvz8fK1fvz6wbySAtm7dqpycnEr7JiUlRR06dKi0b1JTU3X22WdXPKdLly6KiIjQ559/XvGczp07KyYmpuI53bp106ZNm3Tw4MEAvZvAWLp0qRo2bKg2bdqoX79+2r9/f8VjdXlf5eXlSZLS09Ml+e7nbuXKlZVe48hz6trxzVfIjP8hM6qPzKg+MqNqZEZoIDP+h8yoPjKj+siMqpEZoYHM+B8yo/rIjOojM6pGZoQGMuN/yIzqIzOqj8yoWrBnBo3xY9i3b5/cbnelfwhJyszMVE5OjqWqAq9Dhw6aNm2aFixYoGeffVZbt27VBRdcoIKCAuXk5CgmJkapqamVvueX+ygnJ6fKfXjksXB15L0d6/9PTk6OGjZsWOnxqKgopaen17n9d8kll+jll1/W4sWL9eijj2rZsmXq3r273G63pLq7rzwejwYOHKg//OEPatu2rST57OfuaM/Jz89XUVGRP95OWCMzHGRGzZAZ1UNmVI3MCB1khoPMqBkyo3rIjKqRGaGDzHCQGTVDZlQPmVE1MiN0kBkOMqNmyIzqITOqFgqZEVWtd4Q6qXv37hV/P+OMM9ShQwe1aNFCM2fOVHx8vMXKEE6uu+66ir+3a9dOZ5xxhk444QQtXbpUF198scXK7Orfv7/WrVunFStW2C4F8AqZgUAgM6pGZiDUkBkIBDKjamQGQg2ZgUAgM6pGZiDUkBkIBDKjaqGQGYwYP4aMjAxFRkZq9+7dlb6+e/duZWVlWarKvtTUVJ188snavHmzsrKyVFpaqtzc3ErP+eU+ysrKqnIfHnksXB15b8f6/5OVlaU9e/ZUery8vFwHDhyo8/uvdevWysjI0ObNmyXVzX01YMAAzZ07Vx999JGaNm1a8XVf/dwd7TnJycn8klgDZEbVyAzvkBm1Q2aQGaGGzKgameEdMqN2yAwyI9SQGVUjM7xDZtQOmUFmhBoyo2pkhnfIjNohM0InM2iMH0NMTIzOOussLV68uOJrHo9HixcvVseOHS1WZtehQ4e0ZcsWNWrUSGeddZaio6Mr7aNNmzYpOzu7Yh917NhRa9eurXQQWLhwoZKTk3XaaacFvP5AadWqlbKysirtm/z8fH3++eeV9k1ubq6++uqriucsWbJEHo9HHTp0qHjOxx9/rLKysornLFy4UG3atFFaWlqA3k3g7dy5U/v371ejRo0k1a19ZYzRgAEDNGfOHC1ZskStWrWq9Livfu46duxY6TWOPKcuH99qg8yoGpnhHTKjdsgMMiPUkBlVIzO8Q2bUDplBZoQaMqNqZIZ3yIzaITPIjFBDZlSNzPAOmVE7ZEYIZYbBMb3++usmNjbWTJs2zWzYsMHcfvvtJjU11eTk5NguLWCGDBlili5darZu3Wo++eQT06VLF5ORkWH27NljjDHmzjvvNM2bNzdLliwxX375penYsaPp2LFjxfeXl5ebtm3bmq5du5pvvvnGLFiwwDRo0MAMHz7c1lvymYKCArN69WqzevVqI8k8+eSTZvXq1Wb79u3GGGPGjRtnUlNTzTvvvGO+/fZbc+WVV5pWrVqZoqKiite45JJLzO9+9zvz+eefmxUrVpiTTjrJXH/99RWP5+bmmszMTHPTTTeZdevWmddff90kJCSY559/PuDvtzaOta8KCgrM0KFDzcqVK83WrVvNokWLzO9//3tz0kknmeLi4orXqCv7ql+/fiYlJcUsXbrU7Nq1q+KjsLCw4jm++Ln78ccfTUJCghk2bJjZuHGjmThxoomMjDQLFiwI6PsNJ2QGmXEsZIb3yAzvkRmhi8wgM46FzPAemeE9MiN0kRlkxrGQGd4jM7xHZoQuMoPMOBYyw3tkhvdCLTNojHvhmWeeMc2bNzcxMTHm3HPPNZ999pntkgLq2muvNY0aNTIxMTGmSZMm5tprrzWbN2+ueLyoqMjcddddJi0tzSQkJJirrrrK7Nq1q9JrbNu2zXTv3t3Ex8ebjIwMM2TIEFNWVhbot+JzH330kZH0m4/evXsbY4zxeDzmgQceMJmZmSY2NtZcfPHFZtOmTZVeY//+/eb66683iYmJJjk52fTp08cUFBRUes6aNWtMp06dTGxsrGnSpIkZN25coN6izxxrXxUWFpquXbuaBg0amOjoaNOiRQvTt2/f3/zCVlf2VVX7SZKZOnVqxXN89XP30Ucfmfbt25uYmBjTunXrSttAzZAZZMbRkBneIzO8R2aENjKDzDgaMsN7ZIb3yIzQRmaQGUdDZniPzPAemRHayAwy42jIDO+RGd4Ltcxw/bdoAAAAAAAAAAAAAADCEmuMAwAAAAAAAAAAAADCGo1xAAAAAAAAAAAAAEBYozEOAAAAAAAAAAAAAAhrNMYBAAAAAAAAAAAAAGGNxjgAAAAAAAAAAAAAIKzRGAcAAAAAAAAAAAAAhDUa4wAAAAAAAAAAAACAsEZjHAAAAAAAAAAAAAAQ1miMAwAAAAAAAAAAAADCGo1xIIDcbrfOP/98XX311ZW+npeXp2bNmmnEiBGWKgMABBsyAwDgLTIDAOAtMgMA4C0yA+HIZYwxtosA6pLvv/9e7du31wsvvKAbb7xRknTzzTdrzZo1+uKLLxQTE2O5QgBAsCAzAADeIjMAAN4iMwAA3iIzEG5ojAMWPP3003rooYe0fv16rVq1Stdcc42++OILnXnmmbZLAwAEGTIDAOAtMgMA4C0yAwDgLTID4YTGOGCBMUYXXXSRIiMjtXbtWt19990aOXKk7bIAAEGIzAAAeIvMAAB4i8wAAHiLzEA4oTEOWPLdd9/p1FNPVbt27fT1118rKirKdkkAgCBFZgAAvEVmAAC8RWYAALxFZiBcRNguAKirpkyZooSEBG3dulU7d+60XQ4AIIiRGQAAb5EZAABvkRkAAG+RGQgXjBgHLPj000/1xz/+UR9++KEefvhhSdKiRYvkcrksVwYACDZkBgDAW2QGAMBbZAYAwFtkBsIJI8aBACssLNQtt9yifv366cILL9TkyZO1atUqPffcc7ZLAwAEGTIDAOAtMgMA4C0yAwDgLTID4YYR40CA3XPPPZo3b57WrFmjhIQESdLzzz+voUOHau3atWrZsqXdAgEAQYPMAAB4i8wAAHiLzAAAeIvMQLihMQ4E0LJly3TxxRdr6dKl6tSpU6XHunXrpvLycqYgAQBIIjMAAN4jMwAA3iIzAADeIjMQjmiMAwAAAAAAAAAAAADCGmuMAwAAAAAAAAAAAADCGo1xAAAAAAAAAAAAAEBYozEOAAAAAAAAAAAAAAhrNMYBAAAAAAAAAAAAAGGNxjgAAAAAAAAAAAAAIKzRGAcAAAAAAAAAAAAAhDUa4wAAAAAAAAAAAACAsEZjHAAAAAAAAAAAAAAQ1miMAwAAAAAAAAAAAADCGo1xAAAAAAAAAAAAAEBYozEOAAAAAAAAAAAAAAhrNMYBAAAAAAAAAAAAAGHt/wNaiPlD9i9LYQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个包含10个子图的窗口\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axs = axs.ravel()  # 将二维数组展平，方便通过索引访问每个子图\n",
    "\n",
    "env = env_test1.DroneEnv()\n",
    "for i in range(10):\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    trajectory_x = [env.xy_p[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_y = [env.xy_p[1]]  # 存储无人机路径的y坐标\n",
    "    trajectory_ex = [env.xy_e[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_ey = [env.xy_e[1]]  # 存储无人机路径的y坐标\n",
    "    \n",
    "    # 在第i个子图中绘制环境和障碍物\n",
    "    axs[i].scatter(env.xy_e[0], env.xy_e[1], marker='x', color='green', label='Goal')\n",
    "    for k in env.obstacles:\n",
    "        obstacle_circle = plt.Circle(k, env.r_obstacles, color='lightblue', fill=True)\n",
    "        axs[i].add_patch(obstacle_circle)\n",
    "    axs[i].set_xlim(env.space1.low[0], env.space1.high[0])\n",
    "    axs[i].set_ylim(env.space1.low[1], env.space1.high[1])\n",
    "    axs[i].set_xlabel('X')\n",
    "    axs[i].set_ylabel('Y')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Episode {i+1}')\n",
    "\n",
    "    # 通过预训练模型控制无人机执行任务并绘制路径\n",
    "    model = PPO.load(\"best_model\") \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        count += 1\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        next_state, reward, done, t, info = env.step(action)\n",
    "        #if reward < -10:\n",
    "            #print(state, action, reward)\n",
    "        if count > 500:\n",
    "            done = True\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory_x.append(env.xy_p[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_y.append(env.xy_p[1])  # 更新无人机路径的y坐标\n",
    "        trajectory_ex.append(env.xy_e[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_ey.append(env.xy_e[1])  # 更新无人机路径的y坐标\n",
    "\n",
    "    # 绘制无人机路径\n",
    "    axs[i].plot(trajectory_x, trajectory_y, color='red', linewidth=0.5)\n",
    "    axs[i].plot(trajectory_ex, trajectory_ey, color='red', linewidth=0.5)\n",
    "\n",
    "    # 打印每个episode的总奖励\n",
    "    print(f'Episode {i+1} total reward:', total_reward)\n",
    "\n",
    "# 显示子图窗口\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b679efa-70da-4b5f-acb5-227aff558c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
