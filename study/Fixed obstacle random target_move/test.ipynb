{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-14T06:55:49.261565100Z",
     "start_time": "2024-05-14T06:55:45.263313300Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env_test1\n",
    "env = env_test1.DroneEnv()\n",
    "#vec_env = make_vec_env(env, n_envs=4)\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c32894c611bf66",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1728df1-593d-441c-a1e0-6c2c493ead86",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-14T06:55:50.266800700Z",
     "start_time": "2024-05-14T06:55:50.261789100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "callbacks = []\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=8,\n",
    "    best_model_save_path=\".\",\n",
    "    log_path=\".\",\n",
    "    eval_freq=4000,\n",
    ")\n",
    "\n",
    "callbacks.append(eval_callback)\n",
    "kwargs = {}\n",
    "kwargs[\"callback\"] = callbacks\n",
    "\n",
    "log_name = \"ppo_run_\" + str(time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6364eb784437bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T12:30:03.983094500Z",
     "start_time": "2024-04-20T12:03:47.079933900Z"
    },
    "collapsed": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard/ppo_run_1713687030.4351685_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1434 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-13154.16 +/- 5178.12\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.32e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4000          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013020312 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -7.27e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.93e+06      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000276     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 4.26e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 722  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00115988 |\n",
      "|    clip_fraction        | 0.00107    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.000109   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.09e+05   |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00188   |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 7.05e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-27653.30 +/- 11351.86\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -2.77e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8000          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00062321324 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -4.27e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.42e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00111      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 9.58e+05      |\n",
      "-------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 700  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 726          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001914009 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 3.75e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.04e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000387    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-22110.59 +/- 16649.96\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.21e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.525155e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.31e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000216    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 2.42e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 676   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 703           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.9605444e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 1.13e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.91e+06      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000445     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 1.9e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-27960.50 +/- 10651.71\n",
      "Episode length: 479.00 +/- 34.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 479          |\n",
      "|    mean_reward          | -2.8e+04     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029242923 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -1.67e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.92e+04     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.92e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 677   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 698           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1957246e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 2.44e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.95e+06      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000212     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 6.96e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-24750.53 +/- 9492.65\n",
      "Episode length: 478.75 +/- 45.25\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 479           |\n",
      "|    mean_reward          | -2.48e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7509245e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 3.34e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+06      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000478     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.79e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 677   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 692         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001221641 |\n",
      "|    clip_fraction        | 0.000977    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 3.93e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.65e+05    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 6.63e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-12681.04 +/- 11263.09\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.27e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.848313e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 2.32e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000262    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.17e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 674   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 688           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7873265e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.08e+06      |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.00049      |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 5.32e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-5684.77 +/- 1341.32\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -5.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.952757e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+06     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000263    |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 3.24e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 673   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 686           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036954653 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.26e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.35e+04      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000512     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.52e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-6239.40 +/- 2423.14\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -6.24e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.821699e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.11e+06     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000303    |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 4.37e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 673   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 684           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 50            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019757028 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.13e+05      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000423     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 3.25e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-12069.94 +/- 8725.25\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.21e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 36000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033381724 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.74e+04      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000496     |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 1.15e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 681          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001192918 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.95e+05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.03e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-12343.45 +/- 12051.51\n",
      "Episode length: 496.50 +/- 11.91\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 496           |\n",
      "|    mean_reward          | -1.23e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.2632225e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000263     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 2.48e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 681          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.703011e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+06     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000352    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-15856.66 +/- 11999.32\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.59e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 44000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015633131 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.05e+06      |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.00087      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.91e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 680           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 69            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013010448 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.49e+06      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000892     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 5.32e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-6497.46 +/- 3077.42\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -6.5e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.122578e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+06     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000148    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 2.31e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 677           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 75            |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016159046 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.54e+06      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000505     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.25e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-21756.87 +/- 12949.71\n",
      "Episode length: 484.12 +/- 28.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -2.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018584207 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 3.51e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 670   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 677        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01545974 |\n",
      "|    clip_fraction        | 0.0948     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | -1.07e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.82e+04   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.933      |\n",
      "|    value_loss           | 5.45e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-7250.38 +/- 2340.96\n",
      "Episode length: 491.12 +/- 18.78\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 491           |\n",
      "|    mean_reward          | -7.25e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 56000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.7813474e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.29e+06      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000295     |\n",
      "|    std                  | 0.926         |\n",
      "|    value_loss           | 5.55e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012515229 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00097     |\n",
      "|    std                  | 0.922        |\n",
      "|    value_loss           | 7.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-22080.54 +/- 8698.13\n",
      "Episode length: 462.25 +/- 51.42\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 462           |\n",
      "|    mean_reward          | -2.21e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010047809 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.34         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.41e+06      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000695     |\n",
      "|    std                  | 0.922         |\n",
      "|    value_loss           | 6.05e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.229212e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+06     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000325    |\n",
      "|    std                  | 0.925        |\n",
      "|    value_loss           | 5.54e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-22175.84 +/- 10367.73\n",
      "Episode length: 473.38 +/- 30.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 473          |\n",
      "|    mean_reward          | -2.22e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002587432 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67e+06     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000988    |\n",
      "|    std                  | 0.934        |\n",
      "|    value_loss           | 6.61e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 97    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 676          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002874367 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.49e+06     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000739    |\n",
      "|    std                  | 0.93         |\n",
      "|    value_loss           | 3.55e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-14789.97 +/- 11645.07\n",
      "Episode length: 487.62 +/- 27.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -1.48e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0105978735 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8e+04      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0115      |\n",
      "|    std                  | 0.89         |\n",
      "|    value_loss           | 3.67e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 676           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 105           |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.2491926e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.3          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.42e+06      |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.000571     |\n",
      "|    std                  | 0.886         |\n",
      "|    value_loss           | 2.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-18318.52 +/- 14533.00\n",
      "Episode length: 485.38 +/- 41.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008573481 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+06     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000911    |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 2.72e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 109   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 675           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 112           |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015151853 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.3          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.44e+05      |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.000692     |\n",
      "|    std                  | 0.892         |\n",
      "|    value_loss           | 1.98e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-21467.39 +/- 9185.24\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.15e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038753268 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.66e+04     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 1.16e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 669   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 116   |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 674          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020816643 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.43e+05     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    std                  | 0.892        |\n",
      "|    value_loss           | 1.2e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-20054.72 +/- 14752.27\n",
      "Episode length: 491.25 +/- 16.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 491          |\n",
      "|    mean_reward          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064291977 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+04     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    std                  | 0.868        |\n",
      "|    value_loss           | 4.42e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 669   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 122   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 674          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015830671 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.85e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000618    |\n",
      "|    std                  | 0.866        |\n",
      "|    value_loss           | 5.19e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-21727.84 +/- 13639.05\n",
      "Episode length: 478.00 +/- 40.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 478           |\n",
      "|    mean_reward          | -2.17e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 84000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1076665e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.52e+06      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000137     |\n",
      "|    std                  | 0.868         |\n",
      "|    value_loss           | 4.63e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 670   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 128   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-27641.76 +/- 10284.69\n",
      "Episode length: 444.38 +/- 61.33\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 444           |\n",
      "|    mean_reward          | -2.76e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 88000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027906775 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.43e+05      |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -8.09e-05     |\n",
      "|    std                  | 0.87          |\n",
      "|    value_loss           | 2.38e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 667   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 670          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036826394 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56e+04     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    std                  | 0.866        |\n",
      "|    value_loss           | 1.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-22012.30 +/- 14164.83\n",
      "Episode length: 495.12 +/- 15.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -2.2e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001530151 |\n",
      "|    clip_fraction        | 0.00083     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11e+05    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 2.15e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 667   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 670          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020424803 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35e+06     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.864        |\n",
      "|    value_loss           | 1.8e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-19967.06 +/- 8586.18\n",
      "Episode length: 483.50 +/- 32.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012932631 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.06e+04    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 2.1e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 665   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 144   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 146          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003261554 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+06     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000309    |\n",
      "|    std                  | 0.846        |\n",
      "|    value_loss           | 4.44e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-26703.79 +/- 12295.09\n",
      "Episode length: 484.38 +/- 23.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -2.67e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046064975 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.68e+03     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 8.39e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 150    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019467704 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+06     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 2.38e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-13910.43 +/- 12665.47\n",
      "Episode length: 492.75 +/- 16.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | -1.39e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008265653 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+06     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000985    |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 2.91e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 157    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 668           |\n",
      "|    iterations           | 52            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033188544 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.26         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.96e+06      |\n",
      "|    n_updates            | 510           |\n",
      "|    policy_gradient_loss | -0.000988     |\n",
      "|    std                  | 0.848         |\n",
      "|    value_loss           | 3.57e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-27529.20 +/- 13132.47\n",
      "Episode length: 491.88 +/- 16.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 492           |\n",
      "|    mean_reward          | -2.75e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 108000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088315294 |\n",
      "|    clip_fraction        | 0.000684      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.25         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.74e+05      |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | -0.00147      |\n",
      "|    std                  | 0.838         |\n",
      "|    value_loss           | 1.65e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 163    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012033752 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.44e+05     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 2.9e-05      |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 2.25e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-24232.10 +/- 12748.62\n",
      "Episode length: 471.25 +/- 38.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 471          |\n",
      "|    mean_reward          | -2.42e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031102104 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.28e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 169    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 171          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028162287 |\n",
      "|    clip_fraction        | 0.00278      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    std                  | 0.84         |\n",
      "|    value_loss           | 1.78e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-21789.34 +/- 13859.67\n",
      "Episode length: 499.25 +/- 4.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | -2.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044581005 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+06     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 2.85e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 175    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011180914 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000454    |\n",
      "|    std                  | 0.837        |\n",
      "|    value_loss           | 1.47e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-17455.98 +/- 8250.77\n",
      "Episode length: 494.00 +/- 18.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.537119e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.53e+06     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000121    |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 3.08e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 181    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007365042 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35e+06     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 1.44e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-22929.55 +/- 12745.78\n",
      "Episode length: 477.12 +/- 44.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 477          |\n",
      "|    mean_reward          | -2.29e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020703473 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    std                  | 0.859        |\n",
      "|    value_loss           | 2.62e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 187    |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 190          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002860201 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.15e+05     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000661    |\n",
      "|    std                  | 0.865        |\n",
      "|    value_loss           | 2.63e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-25257.20 +/- 13473.17\n",
      "Episode length: 479.62 +/- 37.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -2.53e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032707094 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+06     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    std                  | 0.875        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 194    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026172572 |\n",
      "|    clip_fraction        | 0.00483      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.23e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 6.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-31096.03 +/- 15369.35\n",
      "Episode length: 480.75 +/- 36.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 481          |\n",
      "|    mean_reward          | -3.11e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009766872 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04e+06     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.878        |\n",
      "|    value_loss           | 3.26e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 200    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 667           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048130722 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.29         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.97e+04      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | -0.00115      |\n",
      "|    std                  | 0.878         |\n",
      "|    value_loss           | 8.64e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-30397.61 +/- 13300.01\n",
      "Episode length: 494.50 +/- 10.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -3.04e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010595629 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+06     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000804    |\n",
      "|    std                  | 0.887        |\n",
      "|    value_loss           | 1.97e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 206    |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 208          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001694673 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.3e+06      |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000484    |\n",
      "|    std                  | 0.876        |\n",
      "|    value_loss           | 2.63e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-28552.66 +/- 14688.69\n",
      "Episode length: 492.50 +/- 14.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -2.86e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030356161 |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    std                  | 0.894        |\n",
      "|    value_loss           | 1.44e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 212    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011041071 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+06     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.903        |\n",
      "|    value_loss           | 2.69e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-30116.53 +/- 12813.17\n",
      "Episode length: 490.88 +/- 26.79\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 491           |\n",
      "|    mean_reward          | -3.01e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 144000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00083319494 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.31         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2e+06         |\n",
      "|    n_updates            | 700           |\n",
      "|    policy_gradient_loss | -0.002        |\n",
      "|    std                  | 0.883         |\n",
      "|    value_loss           | 2.14e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 218    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032886472 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+06      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.879        |\n",
      "|    value_loss           | 2.65e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-13791.77 +/- 8407.52\n",
      "Episode length: 500.00 +/- 2.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -1.38e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 148000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065398443 |\n",
      "|    clip_fraction        | 0.00103       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.09e+06      |\n",
      "|    n_updates            | 720           |\n",
      "|    policy_gradient_loss | -0.00176      |\n",
      "|    std                  | 0.864         |\n",
      "|    value_loss           | 3.06e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 224    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002634658 |\n",
      "|    clip_fraction        | 0.0041      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.13e+05    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    std                  | 0.874       |\n",
      "|    value_loss           | 2.35e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-30423.13 +/- 12848.79\n",
      "Episode length: 470.25 +/- 47.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 470           |\n",
      "|    mean_reward          | -3.04e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 152000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00061391434 |\n",
      "|    clip_fraction        | 0.000586      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.29         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.98e+05      |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | -0.00149      |\n",
      "|    std                  | 0.88          |\n",
      "|    value_loss           | 2.39e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 230    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 668           |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 232           |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048908486 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.3          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | -0.00052      |\n",
      "|    std                  | 0.891         |\n",
      "|    value_loss           | 2.13e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-17447.73 +/- 15471.95\n",
      "Episode length: 471.12 +/- 53.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 471         |\n",
      "|    mean_reward          | -1.74e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000597862 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.07e+05    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.000456   |\n",
      "|    std                  | 0.885       |\n",
      "|    value_loss           | 2.33e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 236    |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019784016 |\n",
      "|    clip_fraction        | 0.00469      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.43e+04     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    std                  | 0.868        |\n",
      "|    value_loss           | 2.54e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-12846.14 +/- 12481.85\n",
      "Episode length: 500.25 +/- 1.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.28e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008086888 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000339    |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 6.07e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 242    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 668         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011497587 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.03e+03    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    std                  | 0.867       |\n",
      "|    value_loss           | 1.48e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-6247.38 +/- 3324.18\n",
      "Episode length: 486.12 +/- 33.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 486         |\n",
      "|    mean_reward          | -6.25e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 164000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005377339 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.79e+06    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    std                  | 0.866       |\n",
      "|    value_loss           | 2.45e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 248    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 668         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006754908 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.74e+03    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.854       |\n",
      "|    value_loss           | 1.35e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-7950.96 +/- 6915.75\n",
      "Episode length: 483.50 +/- 33.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -7.95e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001060005 |\n",
      "|    clip_fraction        | 0.00103     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.47e+06    |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    std                  | 0.849       |\n",
      "|    value_loss           | 2.03e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 254    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-18345.84 +/- 13332.41\n",
      "Episode length: 491.50 +/- 18.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045906636 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.29e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.83         |\n",
      "|    value_loss           | 1.22e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 258    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 261          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025056163 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.48e+04     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.823        |\n",
      "|    value_loss           | 3.2e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-9193.37 +/- 7330.35\n",
      "Episode length: 484.62 +/- 29.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -9.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032050412 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.93e+05     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    std                  | 0.819        |\n",
      "|    value_loss           | 3.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 264    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 267           |\n",
      "|    total_timesteps      | 178176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029042925 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.22         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.92e+05      |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.000964     |\n",
      "|    std                  | 0.824         |\n",
      "|    value_loss           | 3.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7297.29 +/- 5855.33\n",
      "Episode length: 488.50 +/- 21.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | -7.3e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004279569 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.92e+05    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 1.11e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 270    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 273          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049220906 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.31e+05     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.816        |\n",
      "|    value_loss           | 4.83e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-6534.11 +/- 2400.90\n",
      "Episode length: 487.62 +/- 23.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -6.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007096815 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.13e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.824        |\n",
      "|    value_loss           | 3.98e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 277    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039603673 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.49e+03     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    std                  | 0.804        |\n",
      "|    value_loss           | 1.15e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-14246.51 +/- 12750.45\n",
      "Episode length: 490.00 +/- 29.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | -1.42e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 188000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001633557 |\n",
      "|    clip_fraction        | 0.00278     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.26e+05    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.000794   |\n",
      "|    std                  | 0.805       |\n",
      "|    value_loss           | 1.76e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 283    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009334973 |\n",
      "|    clip_fraction        | 0.0823      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.83e+04    |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 1.87e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-15271.22 +/- 13513.36\n",
      "Episode length: 485.88 +/- 40.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 486          |\n",
      "|    mean_reward          | -1.53e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033958037 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.27e+03     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.000643    |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 1.03e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 289    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 291          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014584798 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.9e+05      |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.807        |\n",
      "|    value_loss           | 2.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-5092.41 +/- 1314.80\n",
      "Episode length: 485.50 +/- 35.35\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 486           |\n",
      "|    mean_reward          | -5.09e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 196000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016845518 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.2          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.52e+06      |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -0.000758     |\n",
      "|    std                  | 0.807         |\n",
      "|    value_loss           | 3.67e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 295    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 297          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023074336 |\n",
      "|    clip_fraction        | 0.00688      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+06     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    std                  | 0.807        |\n",
      "|    value_loss           | 1.31e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-13808.04 +/- 13784.14\n",
      "Episode length: 483.00 +/- 31.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | -1.38e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013785039 |\n",
      "|    clip_fraction        | 0.00288      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.55e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    std                  | 0.799        |\n",
      "|    value_loss           | 1.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 301    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 303          |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011976499 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 2.43e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-19316.66 +/- 11148.67\n",
      "Episode length: 473.12 +/- 31.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 473          |\n",
      "|    mean_reward          | -1.93e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 204000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025078813 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61e+06     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    std                  | 0.81         |\n",
      "|    value_loss           | 2.77e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 307    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 101          |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 206848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015677777 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.39e+05     |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.00076     |\n",
      "|    std                  | 0.811        |\n",
      "|    value_loss           | 2.81e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-14382.54 +/- 14318.65\n",
      "Episode length: 484.25 +/- 31.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -1.44e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006846187 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.69e+05     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00079     |\n",
      "|    std                  | 0.82         |\n",
      "|    value_loss           | 2.99e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 313    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 316          |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031853649 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.41e+05     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 8.56e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-10986.90 +/- 8368.17\n",
      "Episode length: 493.88 +/- 18.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.1e+04     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 212000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039849663 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5e+05        |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    std                  | 0.811        |\n",
      "|    value_loss           | 1.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 320    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 105          |\n",
      "|    time_elapsed         | 322          |\n",
      "|    total_timesteps      | 215040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011408073 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.69e+05     |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    std                  | 0.821        |\n",
      "|    value_loss           | 3.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-4692.74 +/- 683.07\n",
      "Episode length: 480.50 +/- 21.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | -4.69e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 216000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003029942 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.95e+05    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 2.94e+06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 326    |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 328          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036032023 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+04     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 1.04e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-15149.16 +/- 16831.29\n",
      "Episode length: 493.38 +/- 20.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -1.51e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 220000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097625965 |\n",
      "|    clip_fraction        | 0.0116        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.22         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.23e+05      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -0.00504      |\n",
      "|    std                  | 0.818         |\n",
      "|    value_loss           | 9.92e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 332    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 665          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 335          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038561758 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.97e+05     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    std                  | 0.82         |\n",
      "|    value_loss           | 1.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-17180.38 +/- 15079.48\n",
      "Episode length: 496.38 +/- 12.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | -1.72e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 224000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004814559 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.86e+04    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    std                  | 0.851       |\n",
      "|    value_loss           | 5.13e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 339    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 341          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003198779 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.55e+05     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.853        |\n",
      "|    value_loss           | 2.41e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-10718.09 +/- 10269.97\n",
      "Episode length: 483.62 +/- 39.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -1.07e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 228000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005798389 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.76e+03    |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    std                  | 0.855       |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 346    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007444833 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17e+04     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00098     |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 6.95e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-10868.18 +/- 9147.14\n",
      "Episode length: 498.00 +/- 7.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 498          |\n",
      "|    mean_reward          | -1.09e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 232000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025443337 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+05     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    std                  | 0.835        |\n",
      "|    value_loss           | 1.97e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 352    |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009779369 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.35e+05     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.84         |\n",
      "|    value_loss           | 2.52e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-11877.58 +/- 9727.00\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -1.19e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 236000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011151107 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.49e+03    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 1.41e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 358    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 663         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000906163 |\n",
      "|    clip_fraction        | 0.002       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.8e+05     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 1.41e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-9185.25 +/- 9481.48\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -9.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033173054 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.51e+04     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.000835    |\n",
      "|    std                  | 0.81         |\n",
      "|    value_loss           | 5.3e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 365    |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 367          |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026006352 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94e+05     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    std                  | 0.81         |\n",
      "|    value_loss           | 3.62e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-10105.09 +/- 13568.49\n",
      "Episode length: 497.38 +/- 9.59\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 497           |\n",
      "|    mean_reward          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 244000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020036194 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.21         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.05e+05      |\n",
      "|    n_updates            | 1190          |\n",
      "|    policy_gradient_loss | -0.000332     |\n",
      "|    std                  | 0.812         |\n",
      "|    value_loss           | 8.83e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 371    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 373          |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051721046 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.68e+05     |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.000823    |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 1.12e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-6792.43 +/- 2786.84\n",
      "Episode length: 492.88 +/- 12.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | -6.79e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 248000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055593764 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36e+05     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.803        |\n",
      "|    value_loss           | 4.35e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 377    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 380          |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013168362 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+04     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 6.7e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-6105.53 +/- 1898.05\n",
      "Episode length: 493.50 +/- 13.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | -6.11e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012337711 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.21e+03    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.000319   |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 1.17e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 384    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-5438.64 +/- 1832.15\n",
      "Episode length: 484.12 +/- 36.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -5.44e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 256000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013936909 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.27e+03     |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.772        |\n",
      "|    value_loss           | 6.49e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 388    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004838069 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.48e+03    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.000412   |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 1.16e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-9611.49 +/- 7857.62\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -9.61e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 260000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034426135 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.14         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.02e+05      |\n",
      "|    n_updates            | 1260          |\n",
      "|    policy_gradient_loss | -0.000341     |\n",
      "|    std                  | 0.759         |\n",
      "|    value_loss           | 4.96e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 394    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 396          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002975829 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+06     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 2.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-6798.20 +/- 2555.93\n",
      "Episode length: 492.25 +/- 15.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -6.8e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 264000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006966621 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.3e+05      |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    std                  | 0.761        |\n",
      "|    value_loss           | 7.13e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 400    |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 402           |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032852456 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.15         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8e+05         |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | -0.00104      |\n",
      "|    std                  | 0.76          |\n",
      "|    value_loss           | 1.43e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-7332.31 +/- 5549.11\n",
      "Episode length: 489.25 +/- 31.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 489          |\n",
      "|    mean_reward          | -7.33e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056573283 |\n",
      "|    clip_fraction        | 0.0766       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09e+03     |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    std                  | 0.739        |\n",
      "|    value_loss           | 1.19e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 406    |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061010495 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18e+03     |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | 0.000413     |\n",
      "|    std                  | 0.736        |\n",
      "|    value_loss           | 9.57e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-5974.95 +/- 2460.78\n",
      "Episode length: 490.25 +/- 24.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | -5.97e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010335114 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.31e+05    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.000186   |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 7.88e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 412    |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 134           |\n",
      "|    time_elapsed         | 415           |\n",
      "|    total_timesteps      | 274432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015446532 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.31e+05      |\n",
      "|    n_updates            | 1330          |\n",
      "|    policy_gradient_loss | -0.000172     |\n",
      "|    std                  | 0.739         |\n",
      "|    value_loss           | 5.87e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-7157.89 +/- 5597.39\n",
      "Episode length: 496.88 +/- 7.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | -7.16e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 276000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013745778 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.43e+03     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    std                  | 0.736        |\n",
      "|    value_loss           | 1.22e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 419    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 421           |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036808394 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.65e+05      |\n",
      "|    n_updates            | 1350          |\n",
      "|    policy_gradient_loss | 0.000316      |\n",
      "|    std                  | 0.736         |\n",
      "|    value_loss           | 1.85e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-10568.28 +/- 10139.59\n",
      "Episode length: 491.62 +/- 24.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 492           |\n",
      "|    mean_reward          | -1.06e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 280000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012617788 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.04e+05      |\n",
      "|    n_updates            | 1360          |\n",
      "|    policy_gradient_loss | -0.000558     |\n",
      "|    std                  | 0.736         |\n",
      "|    value_loss           | 2.36e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 425    |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 282624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002901785 |\n",
      "|    clip_fraction        | 0.00308     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.02e+05    |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.000611   |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 1.79e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-14921.56 +/- 11135.07\n",
      "Episode length: 482.62 +/- 25.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | -1.49e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 284000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018839605 |\n",
      "|    clip_fraction        | 0.0669      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13e+05    |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 4.63e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 431    |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 433          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003423942 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78e+05     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.000738    |\n",
      "|    std                  | 0.737        |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-14095.54 +/- 13102.82\n",
      "Episode length: 469.75 +/- 45.21\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 470           |\n",
      "|    mean_reward          | -1.41e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 288000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6938152e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.18e+05      |\n",
      "|    n_updates            | 1400          |\n",
      "|    policy_gradient_loss | -0.000233     |\n",
      "|    std                  | 0.737         |\n",
      "|    value_loss           | 1.69e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 437    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 439           |\n",
      "|    total_timesteps      | 290816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027844342 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.53e+05      |\n",
      "|    n_updates            | 1410          |\n",
      "|    policy_gradient_loss | 5.91e-05      |\n",
      "|    std                  | 0.738         |\n",
      "|    value_loss           | 3.79e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-5546.71 +/- 2165.99\n",
      "Episode length: 483.62 +/- 32.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 484        |\n",
      "|    mean_reward          | -5.55e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 292000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03702683 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.68e+03   |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | 0.00349    |\n",
      "|    std                  | 0.725      |\n",
      "|    value_loss           | 8.83e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 443    |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 445          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006942353 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04e+05     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -8.55e-05    |\n",
      "|    std                  | 0.725        |\n",
      "|    value_loss           | 6.85e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-5246.68 +/- 1391.89\n",
      "Episode length: 485.88 +/- 36.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 486         |\n",
      "|    mean_reward          | -5.25e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005361247 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.65e+03    |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.000419   |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 1.18e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 449    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 451          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061685364 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+03     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.000564    |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 8.14e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-12705.76 +/- 10770.38\n",
      "Episode length: 492.25 +/- 15.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 492         |\n",
      "|    mean_reward          | -1.27e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010486906 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.49e+04    |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | 0.000234    |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 1.68e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 455    |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 662           |\n",
      "|    iterations           | 148           |\n",
      "|    time_elapsed         | 457           |\n",
      "|    total_timesteps      | 303104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018379898 |\n",
      "|    clip_fraction        | 0.000537      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.7e+05       |\n",
      "|    n_updates            | 1470          |\n",
      "|    policy_gradient_loss | 0.000387      |\n",
      "|    std                  | 0.699         |\n",
      "|    value_loss           | 8e+05         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-18546.75 +/- 12102.91\n",
      "Episode length: 476.88 +/- 38.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 477         |\n",
      "|    mean_reward          | -1.85e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 304000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011224788 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.34e+03    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | 0.00386     |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 2.08e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 461    |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 464          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026150402 |\n",
      "|    clip_fraction        | 0.00542      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.71e+05     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 1.09e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-17392.60 +/- 12999.54\n",
      "Episode length: 472.88 +/- 36.76\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 473           |\n",
      "|    mean_reward          | -1.74e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 308000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1205064e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+06      |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -0.000362     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 2.16e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 467    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 470          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004568533 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38e+06     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 1.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-21449.54 +/- 11617.66\n",
      "Episode length: 500.00 +/- 2.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -2.14e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 312000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017124688 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.25e+05      |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | -0.000805     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 1.62e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 474    |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 154           |\n",
      "|    time_elapsed         | 476           |\n",
      "|    total_timesteps      | 315392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5273952e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.75e+05      |\n",
      "|    n_updates            | 1530          |\n",
      "|    policy_gradient_loss | -7.84e-05     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 3.37e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-23572.66 +/- 6947.40\n",
      "Episode length: 490.25 +/- 19.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -2.36e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032038935 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.2e+04      |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.000186    |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 1.2e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 480    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 482          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037179133 |\n",
      "|    clip_fraction        | 0.097        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.21e+04     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | 0.0018       |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 4.71e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-11589.93 +/- 11911.38\n",
      "Episode length: 475.25 +/- 27.68\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 475           |\n",
      "|    mean_reward          | -1.16e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 320000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025247928 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 1560          |\n",
      "|    policy_gradient_loss | -0.000642     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 486    |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045241587 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.52e+04    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 2e+05       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-18767.82 +/- 5104.22\n",
      "Episode length: 475.75 +/- 42.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 476           |\n",
      "|    mean_reward          | -1.88e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 324000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9576763e-05 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.18e+05      |\n",
      "|    n_updates            | 1580          |\n",
      "|    policy_gradient_loss | -8.85e-05     |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 7.64e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 492    |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 495           |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0920194e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.05e+05      |\n",
      "|    n_updates            | 1590          |\n",
      "|    policy_gradient_loss | 3.88e-05      |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 7.34e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-21545.90 +/- 10380.43\n",
      "Episode length: 497.88 +/- 8.27\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 498           |\n",
      "|    mean_reward          | -2.15e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 328000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2429653e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.98e+06      |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | -0.000208     |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 4.19e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 499    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 162           |\n",
      "|    time_elapsed         | 501           |\n",
      "|    total_timesteps      | 331776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016570062 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.37e+05      |\n",
      "|    n_updates            | 1610          |\n",
      "|    policy_gradient_loss | -0.000469     |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-26439.47 +/- 10066.18\n",
      "Episode length: 456.00 +/- 46.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | -2.64e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.717249e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.53e+06     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -7.89e-05    |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 3.78e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 505    |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 507          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016339404 |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.27e+05     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-21458.30 +/- 5601.63\n",
      "Episode length: 497.12 +/- 10.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | -2.15e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007918802 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.82e+05    |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | 1.04e-05    |\n",
      "|    std                  | 0.69        |\n",
      "|    value_loss           | 2.11e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 511    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 514          |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024592797 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.77e+04     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | 0.00521      |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.14e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-20664.86 +/- 8373.19\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.07e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015787362 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.19e+05     |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.000605    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 6.56e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 518    |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-16171.00 +/- 9128.50\n",
      "Episode length: 493.62 +/- 13.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.62e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 344000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025265627 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+06     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 1.64e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 521    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 169         |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 346112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014689185 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.25e+05    |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.000534   |\n",
      "|    std                  | 0.687       |\n",
      "|    value_loss           | 4.5e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-10803.63 +/- 8047.17\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.08e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056329295 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.29e+05     |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | -0.000127    |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 3.24e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 528    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 660        |\n",
      "|    iterations           | 171        |\n",
      "|    time_elapsed         | 530        |\n",
      "|    total_timesteps      | 350208     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02273851 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.48e+05   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | 0.00465    |\n",
      "|    std                  | 0.688      |\n",
      "|    value_loss           | 1.01e+06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-13403.64 +/- 11006.05\n",
      "Episode length: 496.12 +/- 12.90\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 496           |\n",
      "|    mean_reward          | -1.34e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 352000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7123122e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.5e+05       |\n",
      "|    n_updates            | 1710          |\n",
      "|    policy_gradient_loss | -4.65e-05     |\n",
      "|    std                  | 0.688         |\n",
      "|    value_loss           | 1.56e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 534    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 536          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.153574e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5e+05        |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000185    |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 1.78e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-19401.33 +/- 18374.87\n",
      "Episode length: 499.88 +/- 2.98\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -1.94e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 356000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014267818 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.44e+06      |\n",
      "|    n_updates            | 1730          |\n",
      "|    policy_gradient_loss | -0.00114      |\n",
      "|    std                  | 0.688         |\n",
      "|    value_loss           | 2.46e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 540    |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 542         |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010395614 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.17e+04    |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    std                  | 0.692       |\n",
      "|    value_loss           | 1.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-27209.87 +/- 9833.75\n",
      "Episode length: 493.62 +/- 19.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -2.72e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039253635 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.65e+05     |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | 0.00389      |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 546    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 548          |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003382783 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02e+06     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 2.24e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-9677.84 +/- 11473.41\n",
      "Episode length: 487.00 +/- 37.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 487          |\n",
      "|    mean_reward          | -9.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 364000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013278944 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.8e+05      |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.99e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 552    |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 555          |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.694167e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.69e+05     |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.000434    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.38e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-15281.70 +/- 13035.92\n",
      "Episode length: 499.25 +/- 3.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | -1.53e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 368000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018809382 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+05     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | 0.00174      |\n",
      "|    std                  | 0.69         |\n",
      "|    value_loss           | 1.24e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 558    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 181         |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 370688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022530261 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.03e+05    |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.687       |\n",
      "|    value_loss           | 4.69e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-9656.12 +/- 7949.19\n",
      "Episode length: 495.38 +/- 10.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -9.66e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 372000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002159676 |\n",
      "|    clip_fraction        | 0.00605     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.24e+04    |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 6.54e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 564    |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 183           |\n",
      "|    time_elapsed         | 567           |\n",
      "|    total_timesteps      | 374784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6748366e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+06      |\n",
      "|    n_updates            | 1820          |\n",
      "|    policy_gradient_loss | -0.000426     |\n",
      "|    std                  | 0.683         |\n",
      "|    value_loss           | 2.63e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-19065.06 +/- 12677.46\n",
      "Episode length: 490.38 +/- 19.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 490           |\n",
      "|    mean_reward          | -1.91e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 376000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0473916e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6e+05         |\n",
      "|    n_updates            | 1830          |\n",
      "|    policy_gradient_loss | -7.22e-05     |\n",
      "|    std                  | 0.683         |\n",
      "|    value_loss           | 2.81e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 571    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 573           |\n",
      "|    total_timesteps      | 378880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1276165e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.17e+05      |\n",
      "|    n_updates            | 1840          |\n",
      "|    policy_gradient_loss | -0.000407     |\n",
      "|    std                  | 0.683         |\n",
      "|    value_loss           | 4.53e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-17456.12 +/- 13707.41\n",
      "Episode length: 491.75 +/- 24.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -1.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074129533 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+04     |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 2.11e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 577    |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 187           |\n",
      "|    time_elapsed         | 579           |\n",
      "|    total_timesteps      | 382976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016522151 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.53e+06      |\n",
      "|    n_updates            | 1860          |\n",
      "|    policy_gradient_loss | -0.00104      |\n",
      "|    std                  | 0.678         |\n",
      "|    value_loss           | 3.56e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-17091.26 +/- 18247.09\n",
      "Episode length: 488.25 +/- 22.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 488           |\n",
      "|    mean_reward          | -1.71e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 384000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011381961 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.4e+06       |\n",
      "|    n_updates            | 1870          |\n",
      "|    policy_gradient_loss | -0.000816     |\n",
      "|    std                  | 0.677         |\n",
      "|    value_loss           | 4.59e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 583    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 585           |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010268629 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.43e+06      |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | -0.00024      |\n",
      "|    std                  | 0.677         |\n",
      "|    value_loss           | 3.37e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-25145.99 +/- 14651.26\n",
      "Episode length: 480.50 +/- 29.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 480        |\n",
      "|    mean_reward          | -2.51e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 388000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03418546 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 1.79e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.33e+06   |\n",
      "|    n_updates            | 1890       |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    std                  | 0.67       |\n",
      "|    value_loss           | 1.75e+06   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 589    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 191           |\n",
      "|    time_elapsed         | 592           |\n",
      "|    total_timesteps      | 391168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030226613 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+06       |\n",
      "|    n_updates            | 1900          |\n",
      "|    policy_gradient_loss | -0.000588     |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 2.61e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-17900.62 +/- 13468.67\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -1.79e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 392000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024188383 |\n",
      "|    clip_fraction        | 0.0867      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.94e+05    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | 0.00304     |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 1.16e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 596    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 598           |\n",
      "|    total_timesteps      | 395264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011006501 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.17e+06      |\n",
      "|    n_updates            | 1920          |\n",
      "|    policy_gradient_loss | -0.000191     |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 5.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-11184.31 +/- 8980.47\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.12e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 396000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072518125 |\n",
      "|    clip_fraction        | 0.00249       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 1930          |\n",
      "|    policy_gradient_loss | -0.00335      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 2.95e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 602    |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 604          |\n",
      "|    total_timesteps      | 399360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071867565 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36e+05     |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | 0.00338      |\n",
      "|    std                  | 0.671        |\n",
      "|    value_loss           | 9.33e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-36847.25 +/- 14676.73\n",
      "Episode length: 498.88 +/- 5.62\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 499           |\n",
      "|    mean_reward          | -3.68e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 400000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032312225 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.03e+05      |\n",
      "|    n_updates            | 1950          |\n",
      "|    policy_gradient_loss | -0.000949     |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 2.49e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 608    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 610          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030640713 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.25e+04     |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | 0.00137      |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 1.18e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-17532.02 +/- 14914.40\n",
      "Episode length: 493.62 +/- 19.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 404000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010037206 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.15e+05     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.000113    |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 3.79e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 198    |\n",
      "|    time_elapsed    | 614    |\n",
      "|    total_timesteps | 405504 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 407552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005602197 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.85e+05    |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | 0.00269     |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 4.97e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-26483.65 +/- 15379.58\n",
      "Episode length: 498.62 +/- 6.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | -2.65e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 408000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056150774 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+04     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 2.47e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 200    |\n",
      "|    time_elapsed    | 620    |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 201           |\n",
      "|    time_elapsed         | 622           |\n",
      "|    total_timesteps      | 411648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014550227 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+06      |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -0.00063      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 3.56e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-24916.91 +/- 14255.16\n",
      "Episode length: 493.12 +/- 20.84\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -2.49e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 412000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031040955 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.45e+06      |\n",
      "|    n_updates            | 2010          |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 4.14e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 202    |\n",
      "|    time_elapsed    | 626    |\n",
      "|    total_timesteps | 413696 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 203           |\n",
      "|    time_elapsed         | 629           |\n",
      "|    total_timesteps      | 415744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047162434 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.28e+06      |\n",
      "|    n_updates            | 2020          |\n",
      "|    policy_gradient_loss | -0.00147      |\n",
      "|    std                  | 0.675         |\n",
      "|    value_loss           | 4.86e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-17418.10 +/- 8429.95\n",
      "Episode length: 493.38 +/- 20.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | -1.74e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004892613 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.2e+06      |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 5.11e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 204    |\n",
      "|    time_elapsed    | 632    |\n",
      "|    total_timesteps | 417792 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 419840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023012047 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 3.33e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-26086.16 +/- 16490.87\n",
      "Episode length: 499.75 +/- 3.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -2.61e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015894965 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+04     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | 4.79e-05     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 2.68e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 206    |\n",
      "|    time_elapsed    | 638    |\n",
      "|    total_timesteps | 421888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 641         |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025889292 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.31e+03    |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | 0.000964    |\n",
      "|    std                  | 0.673       |\n",
      "|    value_loss           | 1.42e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-22770.56 +/- 17078.47\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.28e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 424000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048455084 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.62e+05     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 1.53e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 208    |\n",
      "|    time_elapsed    | 645    |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-19945.39 +/- 12479.22\n",
      "Episode length: 495.12 +/- 15.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 428000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008233644 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.37e+05    |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 209    |\n",
      "|    time_elapsed    | 649    |\n",
      "|    total_timesteps | 428032 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 651           |\n",
      "|    total_timesteps      | 430080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040298267 |\n",
      "|    clip_fraction        | 0.00317       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.22e+06      |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | -0.000684     |\n",
      "|    std                  | 0.678         |\n",
      "|    value_loss           | 2.41e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-14983.01 +/- 8948.41\n",
      "Episode length: 498.75 +/- 5.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 499         |\n",
      "|    mean_reward          | -1.5e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 432000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003657674 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1e+06     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | 0.00343     |\n",
      "|    std                  | 0.68        |\n",
      "|    value_loss           | 1.17e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 211    |\n",
      "|    time_elapsed    | 655    |\n",
      "|    total_timesteps | 432128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 212          |\n",
      "|    time_elapsed         | 657          |\n",
      "|    total_timesteps      | 434176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023164393 |\n",
      "|    clip_fraction        | 0.00239      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+03     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.000921    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-11776.73 +/- 9099.82\n",
      "Episode length: 498.25 +/- 7.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 498          |\n",
      "|    mean_reward          | -1.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 436000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002647189 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21e+06     |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.000734    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 2.88e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 213    |\n",
      "|    time_elapsed    | 662    |\n",
      "|    total_timesteps | 436224 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 214          |\n",
      "|    time_elapsed         | 664          |\n",
      "|    total_timesteps      | 438272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020073534 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.05e+05     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | 0.000382     |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 1.27e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-22134.54 +/- 17973.73\n",
      "Episode length: 477.25 +/- 42.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 477          |\n",
      "|    mean_reward          | -2.21e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 440000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005852623 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+06     |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 3.08e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 668    |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023426993 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.01e+04    |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | 0.00412     |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 1.47e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-16139.54 +/- 9598.10\n",
      "Episode length: 493.00 +/- 12.69\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -1.61e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 444000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016413032 |\n",
      "|    clip_fraction        | 0.00518       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.28e+05      |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | 6.93e-06      |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 1.8e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 217    |\n",
      "|    time_elapsed    | 675    |\n",
      "|    total_timesteps | 444416 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 677           |\n",
      "|    total_timesteps      | 446464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5620324e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.66e+06      |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -3.47e-06     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 2.24e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-20255.89 +/- 12846.78\n",
      "Episode length: 478.38 +/- 38.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 478          |\n",
      "|    mean_reward          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020081336 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+06     |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 1.85e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 219    |\n",
      "|    time_elapsed    | 681    |\n",
      "|    total_timesteps | 448512 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 220           |\n",
      "|    time_elapsed         | 683           |\n",
      "|    total_timesteps      | 450560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4647757e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.52e+06      |\n",
      "|    n_updates            | 2190          |\n",
      "|    policy_gradient_loss | -2.76e-05     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 4.15e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-20032.59 +/- 10024.25\n",
      "Episode length: 494.75 +/- 16.54\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 495           |\n",
      "|    mean_reward          | -2e+04        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 452000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6812125e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.62e+06      |\n",
      "|    n_updates            | 2200          |\n",
      "|    policy_gradient_loss | -0.000295     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 4.13e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 221    |\n",
      "|    time_elapsed    | 687    |\n",
      "|    total_timesteps | 452608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 689          |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.818316e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93e+05     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.000372    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 2.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-14963.75 +/- 10932.20\n",
      "Episode length: 500.62 +/- 0.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -1.5e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 456000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006918802 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.14e+05    |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 6.07e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 223    |\n",
      "|    time_elapsed    | 693    |\n",
      "|    total_timesteps | 456704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 695          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045513865 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49e+06     |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 2.23e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-27500.73 +/- 14782.35\n",
      "Episode length: 480.25 +/- 34.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -2.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037978985 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+05     |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | 0.00938      |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 1.54e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 225    |\n",
      "|    time_elapsed    | 699    |\n",
      "|    total_timesteps | 460800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 702          |\n",
      "|    total_timesteps      | 462848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005164938 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.27e+05     |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 2.39e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-38989.67 +/- 13776.17\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -3.9e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 464000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016199782 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.15e+05    |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | 0.00235     |\n",
      "|    std                  | 0.7         |\n",
      "|    value_loss           | 1.77e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 227    |\n",
      "|    time_elapsed    | 705    |\n",
      "|    total_timesteps | 464896 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 228           |\n",
      "|    time_elapsed         | 708           |\n",
      "|    total_timesteps      | 466944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3846405e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.2e+06       |\n",
      "|    n_updates            | 2270          |\n",
      "|    policy_gradient_loss | -0.000154     |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 5.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-34061.17 +/- 15367.85\n",
      "Episode length: 477.50 +/- 40.70\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 478           |\n",
      "|    mean_reward          | -3.41e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 468000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1566706e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.7e+06       |\n",
      "|    n_updates            | 2280          |\n",
      "|    policy_gradient_loss | -3.4e-05      |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 5.46e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 229    |\n",
      "|    time_elapsed    | 712    |\n",
      "|    total_timesteps | 468992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 714          |\n",
      "|    total_timesteps      | 471040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.388304e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+06     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -4.64e-05    |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 4.56e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-24601.34 +/- 14478.23\n",
      "Episode length: 484.88 +/- 29.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -2.46e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 472000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001479286 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+06     |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 3.27e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 231    |\n",
      "|    time_elapsed    | 718    |\n",
      "|    total_timesteps | 473088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 720          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017681828 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+06     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 2.24e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-13380.90 +/- 8831.52\n",
      "Episode length: 480.88 +/- 30.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 481          |\n",
      "|    mean_reward          | -1.34e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 476000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004776791 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+06      |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 2.55e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 233    |\n",
      "|    time_elapsed    | 724    |\n",
      "|    total_timesteps | 477184 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 658        |\n",
      "|    iterations           | 234        |\n",
      "|    time_elapsed         | 727        |\n",
      "|    total_timesteps      | 479232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00666592 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.41e+04   |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | 0.00116    |\n",
      "|    std                  | 0.704      |\n",
      "|    value_loss           | 9.35e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-26338.28 +/- 15152.13\n",
      "Episode length: 490.50 +/- 21.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -2.63e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008908616 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+06     |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 3.72e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 235    |\n",
      "|    time_elapsed    | 733    |\n",
      "|    total_timesteps | 481280 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 656           |\n",
      "|    iterations           | 236           |\n",
      "|    time_elapsed         | 736           |\n",
      "|    total_timesteps      | 483328        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00063671026 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+06      |\n",
      "|    n_updates            | 2350          |\n",
      "|    policy_gradient_loss | -0.00208      |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 3.5e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-26609.99 +/- 11699.88\n",
      "Episode length: 491.75 +/- 17.91\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 492           |\n",
      "|    mean_reward          | -2.66e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 484000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038404987 |\n",
      "|    clip_fraction        | 0.00107       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.09e+06      |\n",
      "|    n_updates            | 2360          |\n",
      "|    policy_gradient_loss | -0.0021       |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 4.74e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 237    |\n",
      "|    time_elapsed    | 742    |\n",
      "|    total_timesteps | 485376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 746          |\n",
      "|    total_timesteps      | 487424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011282825 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.4e+05      |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 2.58e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-29286.22 +/- 13987.26\n",
      "Episode length: 478.25 +/- 32.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 478           |\n",
      "|    mean_reward          | -2.93e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 488000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030250003 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.8e+05       |\n",
      "|    n_updates            | 2380          |\n",
      "|    policy_gradient_loss | -0.000122     |\n",
      "|    std                  | 0.703         |\n",
      "|    value_loss           | 1.68e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 650    |\n",
      "|    iterations      | 239    |\n",
      "|    time_elapsed    | 752    |\n",
      "|    total_timesteps | 489472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 756          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007804972 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+06     |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 3.75e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-21645.40 +/- 11799.18\n",
      "Episode length: 496.88 +/- 8.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 497           |\n",
      "|    mean_reward          | -2.16e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 492000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069258374 |\n",
      "|    clip_fraction        | 0.000928      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.8e+06       |\n",
      "|    n_updates            | 2400          |\n",
      "|    policy_gradient_loss | -0.0011       |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 4.27e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 647    |\n",
      "|    iterations      | 241    |\n",
      "|    time_elapsed    | 762    |\n",
      "|    total_timesteps | 493568 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 765         |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026510742 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.67e+05    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.0203      |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 1.96e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-24503.31 +/- 10176.73\n",
      "Episode length: 469.00 +/- 42.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | -2.45e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.03612e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.16e+05    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.000109   |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 3.54e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 243    |\n",
      "|    time_elapsed    | 771    |\n",
      "|    total_timesteps | 497664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 775          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010296686 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+06     |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 3.83e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-20083.42 +/- 12435.70\n",
      "Episode length: 491.38 +/- 25.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | -2.01e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008786217 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.59e+05    |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 1.3e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 642    |\n",
      "|    iterations      | 245    |\n",
      "|    time_elapsed    | 781    |\n",
      "|    total_timesteps | 501760 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 246          |\n",
      "|    time_elapsed         | 784          |\n",
      "|    total_timesteps      | 503808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013513712 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+06      |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 3.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-26335.50 +/- 9554.09\n",
      "Episode length: 498.12 +/- 7.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 498         |\n",
      "|    mean_reward          | -2.63e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002640095 |\n",
      "|    clip_fraction        | 0.0163      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.22e+05    |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.000656   |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 1.32e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 639    |\n",
      "|    iterations      | 247    |\n",
      "|    time_elapsed    | 790    |\n",
      "|    total_timesteps | 505856 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 639           |\n",
      "|    iterations           | 248           |\n",
      "|    time_elapsed         | 794           |\n",
      "|    total_timesteps      | 507904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037070463 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.86e+06      |\n",
      "|    n_updates            | 2470          |\n",
      "|    policy_gradient_loss | -0.000578     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 3.34e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-29958.49 +/- 13563.76\n",
      "Episode length: 482.00 +/- 31.19\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 482           |\n",
      "|    mean_reward          | -3e+04        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 508000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019819659 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.38e+05      |\n",
      "|    n_updates            | 2480          |\n",
      "|    policy_gradient_loss | -0.000328     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 2.08e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 638    |\n",
      "|    iterations      | 249    |\n",
      "|    time_elapsed    | 798    |\n",
      "|    total_timesteps | 509952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-20252.10 +/- 11889.25\n",
      "Episode length: 464.12 +/- 46.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 464          |\n",
      "|    mean_reward          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025982987 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.29e+06     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.04e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 638    |\n",
      "|    iterations      | 250    |\n",
      "|    time_elapsed    | 802    |\n",
      "|    total_timesteps | 512000 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 638           |\n",
      "|    iterations           | 251           |\n",
      "|    time_elapsed         | 805           |\n",
      "|    total_timesteps      | 514048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049335917 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.72e+06      |\n",
      "|    n_updates            | 2500          |\n",
      "|    policy_gradient_loss | -0.000842     |\n",
      "|    std                  | 0.706         |\n",
      "|    value_loss           | 2.71e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-26540.90 +/- 14191.16\n",
      "Episode length: 494.25 +/- 12.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -2.65e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009273534 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+06     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.00058     |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 4.61e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 636    |\n",
      "|    iterations      | 252    |\n",
      "|    time_elapsed    | 811    |\n",
      "|    total_timesteps | 516096 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 253         |\n",
      "|    time_elapsed         | 814         |\n",
      "|    total_timesteps      | 518144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013567127 |\n",
      "|    clip_fraction        | 0.0779      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75e+06    |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | 0.000902    |\n",
      "|    std                  | 0.702       |\n",
      "|    value_loss           | 1.42e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-32523.69 +/- 9638.32\n",
      "Episode length: 482.00 +/- 35.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 482           |\n",
      "|    mean_reward          | -3.25e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 520000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058445265 |\n",
      "|    clip_fraction        | 0.00107       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.84e+05      |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | 0.000332      |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 1.79e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 254    |\n",
      "|    time_elapsed    | 820    |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 255          |\n",
      "|    time_elapsed         | 824          |\n",
      "|    total_timesteps      | 522240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017897319 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38e+06     |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 3.87e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-37320.19 +/- 15569.04\n",
      "Episode length: 498.25 +/- 6.55\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 498           |\n",
      "|    mean_reward          | -3.73e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 524000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022220131 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.19e+06      |\n",
      "|    n_updates            | 2550          |\n",
      "|    policy_gradient_loss | -0.000559     |\n",
      "|    std                  | 0.702         |\n",
      "|    value_loss           | 7.67e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 256    |\n",
      "|    time_elapsed    | 830    |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 631           |\n",
      "|    iterations           | 257           |\n",
      "|    time_elapsed         | 833           |\n",
      "|    total_timesteps      | 526336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013813944 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.56e+06      |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | -0.000967     |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 5.95e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-26248.89 +/- 13370.35\n",
      "Episode length: 493.25 +/- 12.83\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -2.62e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 528000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019220595 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.05e+06      |\n",
      "|    n_updates            | 2570          |\n",
      "|    policy_gradient_loss | -0.000562     |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 3.64e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 258    |\n",
      "|    time_elapsed    | 839    |\n",
      "|    total_timesteps | 528384 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 628           |\n",
      "|    iterations           | 259           |\n",
      "|    time_elapsed         | 843           |\n",
      "|    total_timesteps      | 530432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026081243 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.18e+06      |\n",
      "|    n_updates            | 2580          |\n",
      "|    policy_gradient_loss | -0.00016      |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 4.33e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-18325.25 +/- 6836.85\n",
      "Episode length: 500.25 +/- 1.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 532000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033047916 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.18e+04     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 2.28e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 626    |\n",
      "|    iterations      | 260    |\n",
      "|    time_elapsed    | 849    |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 626           |\n",
      "|    iterations           | 261           |\n",
      "|    time_elapsed         | 852           |\n",
      "|    total_timesteps      | 534528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011933569 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.43e+06      |\n",
      "|    n_updates            | 2600          |\n",
      "|    policy_gradient_loss | -0.000275     |\n",
      "|    std                  | 0.707         |\n",
      "|    value_loss           | 5.31e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-13451.35 +/- 8232.97\n",
      "Episode length: 481.25 +/- 31.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 481          |\n",
      "|    mean_reward          | -1.35e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 536000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010914741 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.74e+06     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.000866    |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 4.01e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 624    |\n",
      "|    iterations      | 262    |\n",
      "|    time_elapsed    | 858    |\n",
      "|    total_timesteps | 536576 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 263         |\n",
      "|    time_elapsed         | 862         |\n",
      "|    total_timesteps      | 538624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003238821 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24e+05    |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 9.3e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-25742.27 +/- 16109.93\n",
      "Episode length: 480.50 +/- 33.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | -2.57e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026791897 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.48e+05    |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.00384     |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 1.4e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 264    |\n",
      "|    time_elapsed    | 868    |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 265          |\n",
      "|    time_elapsed         | 871          |\n",
      "|    total_timesteps      | 542720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010218545 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45e+06     |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 5.31e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-18322.00 +/- 9662.29\n",
      "Episode length: 496.50 +/- 11.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 496          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.930867e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.56e+06     |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 5.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 620    |\n",
      "|    iterations      | 266    |\n",
      "|    time_elapsed    | 877    |\n",
      "|    total_timesteps | 544768 |\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 616    |\n",
      "|    iterations      | 270    |\n",
      "|    time_elapsed    | 897    |\n",
      "|    total_timesteps | 552960 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 615           |\n",
      "|    iterations           | 271           |\n",
      "|    time_elapsed         | 901           |\n",
      "|    total_timesteps      | 555008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025207584 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.59e+06      |\n",
      "|    n_updates            | 2700          |\n",
      "|    policy_gradient_loss | -0.000256     |\n",
      "|    std                  | 0.695         |\n",
      "|    value_loss           | 4.28e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-27267.56 +/- 10968.18\n",
      "Episode length: 493.50 +/- 10.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -2.73e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 556000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021876087 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | 0.00267      |\n",
      "|    std                  | 0.684        |\n",
      "|    value_loss           | 9.2e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 614    |\n",
      "|    iterations      | 272    |\n",
      "|    time_elapsed    | 907    |\n",
      "|    total_timesteps | 557056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 273          |\n",
      "|    time_elapsed         | 910          |\n",
      "|    total_timesteps      | 559104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004686089 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25e+06     |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.000521    |\n",
      "|    std                  | 0.684        |\n",
      "|    value_loss           | 3.19e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-29624.85 +/- 20648.12\n",
      "Episode length: 495.88 +/- 13.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 496           |\n",
      "|    mean_reward          | -2.96e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 560000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054304744 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+06      |\n",
      "|    n_updates            | 2730          |\n",
      "|    policy_gradient_loss | -0.00182      |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 2.78e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 612    |\n",
      "|    iterations      | 274    |\n",
      "|    time_elapsed    | 916    |\n",
      "|    total_timesteps | 561152 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 612           |\n",
      "|    iterations           | 275           |\n",
      "|    time_elapsed         | 920           |\n",
      "|    total_timesteps      | 563200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012044067 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.75e+06      |\n",
      "|    n_updates            | 2740          |\n",
      "|    policy_gradient_loss | -0.000407     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 4.24e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-20960.60 +/- 14421.07\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -2.1e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 564000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018746211 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+06      |\n",
      "|    n_updates            | 2750          |\n",
      "|    policy_gradient_loss | -0.000494     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 4.32e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 610    |\n",
      "|    iterations      | 276    |\n",
      "|    time_elapsed    | 926    |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 610          |\n",
      "|    iterations           | 277          |\n",
      "|    time_elapsed         | 929          |\n",
      "|    total_timesteps      | 567296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005320124 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.87e+05     |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    std                  | 0.683        |\n",
      "|    value_loss           | 1.99e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-31668.97 +/- 14841.82\n",
      "Episode length: 495.25 +/- 9.68\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 495           |\n",
      "|    mean_reward          | -3.17e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 568000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047682915 |\n",
      "|    clip_fraction        | 0.000537      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 2770          |\n",
      "|    policy_gradient_loss | -3.53e-05     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 2.57e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 278    |\n",
      "|    time_elapsed    | 935    |\n",
      "|    total_timesteps | 569344 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 608           |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 939           |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027846778 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.08e+06      |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | -0.000363     |\n",
      "|    std                  | 0.685         |\n",
      "|    value_loss           | 3.38e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=-28510.05 +/- 13578.49\n",
      "Episode length: 494.50 +/- 11.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | -2.85e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 572000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002920227 |\n",
      "|    clip_fraction        | 0.0158      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.79e+05    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | 0.00122     |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 2.39e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 606    |\n",
      "|    iterations      | 280    |\n",
      "|    time_elapsed    | 945    |\n",
      "|    total_timesteps | 573440 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 281         |\n",
      "|    time_elapsed         | 948         |\n",
      "|    total_timesteps      | 575488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023991866 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.48e+05    |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | 0.00297     |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 1.36e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-19431.99 +/- 12583.42\n",
      "Episode length: 496.00 +/- 13.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | -1.94e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 576000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003556307 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.84e+05    |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.000813   |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 1.49e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 604    |\n",
      "|    iterations      | 282    |\n",
      "|    time_elapsed    | 954    |\n",
      "|    total_timesteps | 577536 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 604           |\n",
      "|    iterations           | 283           |\n",
      "|    time_elapsed         | 958           |\n",
      "|    total_timesteps      | 579584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038180698 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.5e+05       |\n",
      "|    n_updates            | 2820          |\n",
      "|    policy_gradient_loss | -0.000368     |\n",
      "|    std                  | 0.691         |\n",
      "|    value_loss           | 3.54e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-22375.76 +/- 10832.71\n",
      "Episode length: 495.00 +/- 15.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | -2.24e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 580000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004866657 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 2.36e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 603    |\n",
      "|    iterations      | 284    |\n",
      "|    time_elapsed    | 964    |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 967          |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003164097 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+06     |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 2.23e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-19130.55 +/- 13959.20\n",
      "Episode length: 468.62 +/- 46.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | -1.91e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002944519 |\n",
      "|    clip_fraction        | 0.00918     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.38e+05    |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.000471   |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 1.32e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 601    |\n",
      "|    iterations      | 286    |\n",
      "|    time_elapsed    | 973    |\n",
      "|    total_timesteps | 585728 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 601           |\n",
      "|    iterations           | 287           |\n",
      "|    time_elapsed         | 977           |\n",
      "|    total_timesteps      | 587776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043682492 |\n",
      "|    clip_fraction        | 0.000586      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.21e+06      |\n",
      "|    n_updates            | 2860          |\n",
      "|    policy_gradient_loss | -0.00126      |\n",
      "|    std                  | 0.691         |\n",
      "|    value_loss           | 3.48e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-18325.70 +/- 18210.94\n",
      "Episode length: 477.25 +/- 35.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 477          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 588000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006139282 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+06     |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.000667    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.76e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 599    |\n",
      "|    iterations      | 288    |\n",
      "|    time_elapsed    | 983    |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 986          |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.705423e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.48e+06     |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -1.88e-05    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.74e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-18599.18 +/- 12305.94\n",
      "Episode length: 490.88 +/- 26.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | -1.86e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 592000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014879089 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.23e+06    |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.000481   |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 2.67e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 598    |\n",
      "|    iterations      | 290    |\n",
      "|    time_elapsed    | 992    |\n",
      "|    total_timesteps | 593920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 996         |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009422754 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.19e+05    |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | 0.00697     |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 8.06e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-23685.42 +/- 17641.96\n",
      "Episode length: 483.38 +/- 29.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | -2.37e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 596000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025964323 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24e+05     |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 2.04e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 596    |\n",
      "|    iterations      | 292    |\n",
      "|    time_elapsed    | 1002   |\n",
      "|    total_timesteps | 598016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-21639.49 +/- 10432.63\n",
      "Episode length: 494.62 +/- 16.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -2.16e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009212188 |\n",
      "|    clip_fraction        | 0.0607      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.86e+05    |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    std                  | 0.702       |\n",
      "|    value_loss           | 3.59e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 595    |\n",
      "|    iterations      | 293    |\n",
      "|    time_elapsed    | 1008   |\n",
      "|    total_timesteps | 600064 |\n",
      "-------------------------------\n",
      "Before training: mean_reward:-20834.55 +/- 11996.51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#vec_env = VecNormalize(env, norm_obs=True, norm_reward=True,clip_obs=1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=600000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Before training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bab43037b8499f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T06:51:03.775963200Z",
     "start_time": "2024-04-18T06:51:00.227698Z"
    },
    "collapsed": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard/ppo_run_1713689451.8989656_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -7.91e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 1362      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-7367.56 +/- 6005.28\n",
      "Episode length: 497.75 +/- 8.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 498          |\n",
      "|    mean_reward          | -7.37e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043718563 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.77        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+04     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 7.3e+04      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.15e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 735       |\n",
      "|    iterations      | 2         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 4096      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 779          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045188316 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.765       |\n",
      "|    explained_variance   | 0.0393       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00322     |\n",
      "|    std                  | 0.521        |\n",
      "|    value_loss           | 1.58e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-8195.65 +/- 2355.63\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -8.2e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005994937 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.768       |\n",
      "|    explained_variance   | 0.0353       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.73e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.00047      |\n",
      "|    std                  | 0.522        |\n",
      "|    value_loss           | 1.57e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 688       |\n",
      "|    iterations      | 4         |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 8192      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 713          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009513359 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.769       |\n",
      "|    explained_variance   | 0.0739       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+04     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.00039      |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 1.72e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-7958.83 +/- 4660.71\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.96e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011465271 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.761       |\n",
      "|    explained_variance   | 0.0693       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.58e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.516        |\n",
      "|    value_loss           | 7.82e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.29e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 669       |\n",
      "|    iterations      | 6         |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 12288     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.57e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 693          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039432365 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.755       |\n",
      "|    explained_variance   | 0.0416       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.85e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000177    |\n",
      "|    std                  | 0.515        |\n",
      "|    value_loss           | 1.16e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-4902.77 +/- 697.81\n",
      "Episode length: 491.25 +/- 17.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | -4.9e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000696138 |\n",
      "|    clip_fraction        | 0.00557     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.755      |\n",
      "|    explained_variance   | 0.056       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.2e+04     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.515       |\n",
      "|    value_loss           | 4.16e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -5.1e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 660      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 679          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059226886 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.754       |\n",
      "|    explained_variance   | 0.0606       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+04     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 9.12e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-5967.94 +/- 1889.91\n",
      "Episode length: 484.00 +/- 31.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -5.97e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003105682 |\n",
      "|    clip_fraction        | 0.00625     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.0603      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0005     |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 8.72e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.16e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 659       |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 31        |\n",
      "|    total_timesteps | 20480     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.12e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017803528 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.753       |\n",
      "|    explained_variance   | 0.0432       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.96e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 6.95e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-4995.22 +/- 1075.80\n",
      "Episode length: 484.12 +/- 37.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -5e+03      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004713544 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.75       |\n",
      "|    explained_variance   | 0.00162     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.01e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000398   |\n",
      "|    std                  | 0.511       |\n",
      "|    value_loss           | 2.03e+04    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | -1.3e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 657      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -6.1e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 671          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037124318 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.745       |\n",
      "|    explained_variance   | 0.0895       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.5e+04      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000694    |\n",
      "|    std                  | 0.507        |\n",
      "|    value_loss           | 5.21e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-9352.61 +/- 5726.00\n",
      "Episode length: 486.62 +/- 35.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 487          |\n",
      "|    mean_reward          | -9.35e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041166293 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.742       |\n",
      "|    explained_variance   | 0.0767       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    std                  | 0.51         |\n",
      "|    value_loss           | 6.32e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -1.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 655       |\n",
      "|    iterations      | 14        |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 28672     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.12e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029931196 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.734       |\n",
      "|    explained_variance   | 0.0714       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.28e+03     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    std                  | 0.5          |\n",
      "|    value_loss           | 1.25e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-4947.24 +/- 974.73\n",
      "Episode length: 478.88 +/- 24.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 479          |\n",
      "|    mean_reward          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015261171 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.727       |\n",
      "|    explained_variance   | 0.04         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    std                  | 0.5          |\n",
      "|    value_loss           | 6.34e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 539       |\n",
      "|    ep_rew_mean     | -5.41e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 655       |\n",
      "|    iterations      | 16        |\n",
      "|    time_elapsed    | 49        |\n",
      "|    total_timesteps | 32768     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.14e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072448683 |\n",
      "|    clip_fraction        | 0.00176       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.728        |\n",
      "|    explained_variance   | 0.049         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.64e+05      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.00125      |\n",
      "|    std                  | 0.503         |\n",
      "|    value_loss           | 3.05e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-5015.75 +/- 1451.15\n",
      "Episode length: 476.50 +/- 32.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | -5.02e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055962075 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.0339       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+04     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 1.71e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -4.85e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 649       |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 56        |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.99e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 657          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018193745 |\n",
      "|    clip_fraction        | 0.00547      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.718       |\n",
      "|    explained_variance   | 0.0445       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62e+05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 9.61e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-7709.74 +/- 3348.60\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -7.71e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058575615 |\n",
      "|    clip_fraction        | 0.0022        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.716        |\n",
      "|    explained_variance   | 0.0265        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.99e+05      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | 0.00013       |\n",
      "|    std                  | 0.493         |\n",
      "|    value_loss           | 6.63e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 994      |\n",
      "|    ep_rew_mean     | -1.6e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.38e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012557252 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.712       |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.89e+05     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00083     |\n",
      "|    std                  | 0.494        |\n",
      "|    value_loss           | 1.45e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-5151.65 +/- 1140.17\n",
      "Episode length: 494.88 +/- 16.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | -5.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055267895 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.706       |\n",
      "|    explained_variance   | 0.0212       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.69e+05     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    std                  | 0.488        |\n",
      "|    value_loss           | 2.89e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.33e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 647       |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 69        |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.48e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018154065 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.7         |\n",
      "|    explained_variance   | 0.0181       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.17e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    std                  | 0.486        |\n",
      "|    value_loss           | 1.51e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-6386.48 +/- 4371.39\n",
      "Episode length: 479.12 +/- 39.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | -6.39e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011442861 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.702      |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.69e+04    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.489       |\n",
      "|    value_loss           | 1.71e+05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -2.5e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.59e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022734911 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.701       |\n",
      "|    explained_variance   | 0.0258       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    std                  | 0.487        |\n",
      "|    value_loss           | 4.88e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-5836.70 +/- 1272.82\n",
      "Episode length: 499.75 +/- 3.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -5.84e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005748086 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 0.0332      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.48e+05    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    std                  | 0.484       |\n",
      "|    value_loss           | 1.9e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.13e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 647       |\n",
      "|    iterations      | 26        |\n",
      "|    time_elapsed    | 82        |\n",
      "|    total_timesteps | 53248     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 501        |\n",
      "|    ep_rew_mean          | -5.13e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 84         |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00165405 |\n",
      "|    clip_fraction        | 0.0105     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.692     |\n",
      "|    explained_variance   | 0.0114     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.64e+03   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.000126   |\n",
      "|    std                  | 0.486      |\n",
      "|    value_loss           | 1.48e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-7313.55 +/- 5947.10\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.31e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048461584 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | 0.0153       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.02e+03     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000583    |\n",
      "|    std                  | 0.48         |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.08e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 646       |\n",
      "|    iterations      | 28        |\n",
      "|    time_elapsed    | 88        |\n",
      "|    total_timesteps | 57344     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -2.45e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008133566 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.0633      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36e+04    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 1.3e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-7879.59 +/- 3303.57\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.88e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013425966 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.655       |\n",
      "|    explained_variance   | 0.0369       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.28e+05     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    std                  | 0.466        |\n",
      "|    value_loss           | 1.15e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -1.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 646       |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 94        |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.08e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011129319 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.661       |\n",
      "|    explained_variance   | 0.0302       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+04     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.473        |\n",
      "|    value_loss           | 8.37e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-6043.26 +/- 1619.36\n",
      "Episode length: 497.12 +/- 10.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | -6.04e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055721086 |\n",
      "|    clip_fraction        | 0.00713      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.0432       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.59e+04     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 9.73e-05     |\n",
      "|    std                  | 0.47         |\n",
      "|    value_loss           | 9.57e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.23e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 640       |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 102       |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.18e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005744797 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.0388      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.23e+04    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    std                  | 0.465       |\n",
      "|    value_loss           | 1.67e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-5146.74 +/- 954.66\n",
      "Episode length: 489.12 +/- 31.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 489          |\n",
      "|    mean_reward          | -5.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021356475 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.647       |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+04     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -1.33e-05    |\n",
      "|    std                  | 0.46         |\n",
      "|    value_loss           | 1.92e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.94e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 621       |\n",
      "|    iterations      | 34        |\n",
      "|    time_elapsed    | 112       |\n",
      "|    total_timesteps | 69632     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 619          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025869492 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.632       |\n",
      "|    explained_variance   | 0.0815       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.92e+03     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.451        |\n",
      "|    value_loss           | 1.74e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-6694.57 +/- 2805.69\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -6.69e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003753365 |\n",
      "|    clip_fraction        | 0.0158      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.57e+03    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.000378   |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 1.65e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.23e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 36        |\n",
      "|    time_elapsed    | 121       |\n",
      "|    total_timesteps | 73728     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.3e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035547698 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.0656       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.14e+03     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00083     |\n",
      "|    std                  | 0.449        |\n",
      "|    value_loss           | 1.23e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-6770.33 +/- 2546.61\n",
      "Episode length: 488.25 +/- 33.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -6.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021324586 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.618       |\n",
      "|    explained_variance   | 0.0795       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.7e+04      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    std                  | 0.449        |\n",
      "|    value_loss           | 1.62e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.25e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 590       |\n",
      "|    iterations      | 38        |\n",
      "|    time_elapsed    | 131       |\n",
      "|    total_timesteps | 77824     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 590          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011437298 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.0793       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.14e+03     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 1.95e-05     |\n",
      "|    std                  | 0.448        |\n",
      "|    value_loss           | 5.52e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-7606.44 +/- 3932.12\n",
      "Episode length: 496.88 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | -7.61e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007590077 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.82e+03    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.52e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -2.13e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 578       |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 141       |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.04e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 578          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006594521 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.6         |\n",
      "|    explained_variance   | 0.0538       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000111    |\n",
      "|    std                  | 0.441        |\n",
      "|    value_loss           | 3.26e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-5184.87 +/- 1456.03\n",
      "Episode length: 485.00 +/- 27.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -5.18e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011976447 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.601       |\n",
      "|    explained_variance   | 0.0927       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84e+04     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.441        |\n",
      "|    value_loss           | 1.42e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.13e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 569       |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 151       |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-5575.05 +/- 2128.84\n",
      "Episode length: 492.00 +/- 12.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -5.58e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014541766 |\n",
      "|    clip_fraction        | 0.00303      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | 0.0573       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+05     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.442        |\n",
      "|    value_loss           | 3.63e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 466      |\n",
      "|    ep_rew_mean     | -4.7e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -2.54e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 560          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075456826 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.0709       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.14e+03     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.435        |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-6422.91 +/- 3053.98\n",
      "Episode length: 499.75 +/- 3.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -6.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010512683 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.0673       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.12e+04     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    std                  | 0.435        |\n",
      "|    value_loss           | 4.81e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.19e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 551       |\n",
      "|    iterations      | 45        |\n",
      "|    time_elapsed    | 167       |\n",
      "|    total_timesteps | 92160     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.27e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 552         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006210123 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.7e+03     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 3.72e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-4774.98 +/- 848.00\n",
      "Episode length: 476.12 +/- 31.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | -4.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017014747 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.091        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.33e+03     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    std                  | 0.428        |\n",
      "|    value_loss           | 1.68e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 544       |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 176       |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -8.32e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017926497 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.0143       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.24e+03     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000532    |\n",
      "|    std                  | 0.423        |\n",
      "|    value_loss           | 1.01e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-5487.95 +/- 1800.57\n",
      "Episode length: 495.12 +/- 15.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | -5.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029440613 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.0668       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.7e+04      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.423        |\n",
      "|    value_loss           | 4.49e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 537       |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 186       |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.01e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 537           |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 190           |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053785666 |\n",
      "|    clip_fraction        | 0.00146       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.558        |\n",
      "|    explained_variance   | 0.0867        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.71e+04      |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | -0.000162     |\n",
      "|    std                  | 0.422         |\n",
      "|    value_loss           | 9.59e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-4998.69 +/- 574.60\n",
      "Episode length: 493.25 +/- 20.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 493         |\n",
      "|    mean_reward          | -5e+03      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005159573 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.71e+03    |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    std                  | 0.42        |\n",
      "|    value_loss           | 1.66e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 531       |\n",
      "|    iterations      | 51        |\n",
      "|    time_elapsed    | 196       |\n",
      "|    total_timesteps | 104448    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -4.97e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 531         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006094318 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.0495      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14e+06    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 7.67e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-6653.71 +/- 5708.60\n",
      "Episode length: 478.25 +/- 31.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 478          |\n",
      "|    mean_reward          | -6.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047028577 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.543       |\n",
      "|    explained_variance   | 0.0562       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+04      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.000418    |\n",
      "|    std                  | 0.414        |\n",
      "|    value_loss           | 1.73e+04     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 718      |\n",
      "|    ep_rew_mean     | -7.4e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.11e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 210           |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028663865 |\n",
      "|    clip_fraction        | 0.038         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.539        |\n",
      "|    explained_variance   | 0.0955        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+04      |\n",
      "|    n_updates            | 530           |\n",
      "|    policy_gradient_loss | -0.000249     |\n",
      "|    std                  | 0.415         |\n",
      "|    value_loss           | 1.5e+04       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-5909.79 +/- 1868.77\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -5.91e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001069733 |\n",
      "|    clip_fraction        | 0.00132     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.0442      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.78e+05    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00059    |\n",
      "|    std                  | 0.416       |\n",
      "|    value_loss           | 5.79e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -1.69e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 521       |\n",
      "|    iterations      | 55        |\n",
      "|    time_elapsed    | 216       |\n",
      "|    total_timesteps | 112640    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.12e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 521         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002202771 |\n",
      "|    clip_fraction        | 0.00337     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.0652      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11e+04    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.415       |\n",
      "|    value_loss           | 1.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-7204.28 +/- 4664.47\n",
      "Episode length: 488.38 +/- 33.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | -7.2e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 116000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005357297 |\n",
      "|    clip_fraction        | 0.0108      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.0976      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02e+04    |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.000134   |\n",
      "|    std                  | 0.412       |\n",
      "|    value_loss           | 1.47e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -9.91e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 516       |\n",
      "|    iterations      | 57        |\n",
      "|    time_elapsed    | 225       |\n",
      "|    total_timesteps | 116736    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.05e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 229          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044694655 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.532       |\n",
      "|    explained_variance   | 0.092        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.62e+04     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    std                  | 0.412        |\n",
      "|    value_loss           | 8.09e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-5134.46 +/- 1294.38\n",
      "Episode length: 484.12 +/- 21.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060984846 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 0.0572       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+04     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    std                  | 0.406        |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 551       |\n",
      "|    ep_rew_mean     | -5.45e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 513       |\n",
      "|    iterations      | 59        |\n",
      "|    time_elapsed    | 235       |\n",
      "|    total_timesteps | 120832    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.09e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 239          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034583462 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | 0.0209       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+04     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.397        |\n",
      "|    value_loss           | 1.32e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-9325.46 +/- 9089.29\n",
      "Episode length: 480.25 +/- 34.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -9.33e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042910306 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.0425       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.77e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.397        |\n",
      "|    value_loss           | 2.36e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 994      |\n",
      "|    ep_rew_mean     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    fps             | 509      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030813026 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.0622       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 657          |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    std                  | 0.394        |\n",
      "|    value_loss           | 1.35e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-5619.94 +/- 1678.79\n",
      "Episode length: 499.75 +/- 3.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -5.62e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027328103 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.0491       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.65e+03     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.391        |\n",
      "|    value_loss           | 1.65e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 511       |\n",
      "|    ep_rew_mean     | -5.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 505       |\n",
      "|    iterations      | 63        |\n",
      "|    time_elapsed    | 255       |\n",
      "|    total_timesteps | 129024    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -2.07e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 258          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026115838 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.0301       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+04     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.388        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-7220.95 +/- 3740.10\n",
      "Episode length: 489.62 +/- 30.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -7.22e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015535294 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.045        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.87e+05     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    std                  | 0.389        |\n",
      "|    value_loss           | 7.43e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.99e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 502       |\n",
      "|    iterations      | 65        |\n",
      "|    time_elapsed    | 264       |\n",
      "|    total_timesteps | 133120    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -7e+03        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 503           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 268           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.8751706e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.476        |\n",
      "|    explained_variance   | 0.0494        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.51e+06      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | 0.000297      |\n",
      "|    std                  | 0.39          |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-6583.87 +/- 2745.94\n",
      "Episode length: 488.00 +/- 34.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -6.58e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059500895 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | -0.00576     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56e+03     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000253    |\n",
      "|    std                  | 0.382        |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -5.7e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 499      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -3.7e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 278          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021517812 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0473       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.08e+05     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    std                  | 0.382        |\n",
      "|    value_loss           | 9.86e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-8370.61 +/- 3923.30\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -8.37e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 140000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032453547 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.457        |\n",
      "|    explained_variance   | 0.0278        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.69e+05      |\n",
      "|    n_updates            | 680           |\n",
      "|    policy_gradient_loss | -0.000494     |\n",
      "|    std                  | 0.382         |\n",
      "|    value_loss           | 1.2e+06       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -3.68e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 496       |\n",
      "|    iterations      | 69        |\n",
      "|    time_elapsed    | 284       |\n",
      "|    total_timesteps | 141312    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.01e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058652125 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.78e+05     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    std                  | 0.383        |\n",
      "|    value_loss           | 1.93e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-8128.91 +/- 9691.12\n",
      "Episode length: 484.88 +/- 23.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -8.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010654543 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.462       |\n",
      "|    explained_variance   | 0.0144       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+04     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.000709    |\n",
      "|    std                  | 0.385        |\n",
      "|    value_loss           | 2.29e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 454       |\n",
      "|    ep_rew_mean     | -1.23e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 494       |\n",
      "|    iterations      | 71        |\n",
      "|    time_elapsed    | 294       |\n",
      "|    total_timesteps | 145408    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -4.95e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 494           |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 297           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028154056 |\n",
      "|    clip_fraction        | 0.000635      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.467        |\n",
      "|    explained_variance   | 0.0381        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.75e+05      |\n",
      "|    n_updates            | 710           |\n",
      "|    policy_gradient_loss | -5.35e-05     |\n",
      "|    std                  | 0.387         |\n",
      "|    value_loss           | 6.19e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-8963.16 +/- 6486.09\n",
      "Episode length: 486.25 +/- 36.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 486          |\n",
      "|    mean_reward          | -8.96e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018029745 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.043        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.83e+04     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.39         |\n",
      "|    value_loss           | 4.29e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -2.76e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 491       |\n",
      "|    iterations      | 73        |\n",
      "|    time_elapsed    | 304       |\n",
      "|    total_timesteps | 149504    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.49e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 307          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017900508 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.479       |\n",
      "|    explained_variance   | 0.0358       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+05     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00252     |\n",
      "|    std                  | 0.39         |\n",
      "|    value_loss           | 1.09e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-5991.74 +/- 2437.61\n",
      "Episode length: 479.88 +/- 38.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -5.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048378184 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.0256       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.76e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 1.57e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -3.51e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 489       |\n",
      "|    iterations      | 75        |\n",
      "|    time_elapsed    | 313       |\n",
      "|    total_timesteps | 153600    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.07e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 490           |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 317           |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081118103 |\n",
      "|    clip_fraction        | 0.00464       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.468        |\n",
      "|    explained_variance   | 0.0183        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.14e+05      |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | 0.000382      |\n",
      "|    std                  | 0.386         |\n",
      "|    value_loss           | 1.1e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-6889.36 +/- 3925.05\n",
      "Episode length: 479.88 +/- 40.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | -6.89e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002692502 |\n",
      "|    clip_fraction        | 0.0582      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.0347      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46e+04    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    std                  | 0.386       |\n",
      "|    value_loss           | 2.42e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -1.79e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 487       |\n",
      "|    iterations      | 77        |\n",
      "|    time_elapsed    | 323       |\n",
      "|    total_timesteps | 157696    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -2.62e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 326          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029994943 |\n",
      "|    clip_fraction        | 0.00864      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.0546       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 2.51e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-9329.95 +/- 8339.43\n",
      "Episode length: 496.25 +/- 12.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 496          |\n",
      "|    mean_reward          | -9.33e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024951617 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.62e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    std                  | 0.381        |\n",
      "|    value_loss           | 8.93e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.01e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 485       |\n",
      "|    iterations      | 79        |\n",
      "|    time_elapsed    | 333       |\n",
      "|    total_timesteps | 161792    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.88e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 486          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031740274 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0695       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.17e+03     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 1.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-7244.82 +/- 5241.39\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.24e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041970583 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | 0.0683       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.83e+03     |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.000254    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 2.06e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -6.79e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 483       |\n",
      "|    iterations      | 81        |\n",
      "|    time_elapsed    | 342       |\n",
      "|    total_timesteps | 165888    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.79e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005268968 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.0541      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.71e+04    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    std                  | 0.381       |\n",
      "|    value_loss           | 5.95e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-10141.12 +/- 7144.65\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013853812 |\n",
      "|    clip_fraction        | 0.00439      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | 0.0611       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.381        |\n",
      "|    value_loss           | 5.42e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.96e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 482       |\n",
      "|    iterations      | 83        |\n",
      "|    time_elapsed    | 352       |\n",
      "|    total_timesteps | 169984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-6382.08 +/- 3707.87\n",
      "Episode length: 490.50 +/- 19.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -6.38e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065498874 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.00817      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.00016      |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 1.4e+04      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 494       |\n",
      "|    ep_rew_mean     | -5.85e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 479       |\n",
      "|    iterations      | 84        |\n",
      "|    time_elapsed    | 358       |\n",
      "|    total_timesteps | 172032    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.6e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 362          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051843203 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.00508      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21e+04     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000413    |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 2.21e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-5802.71 +/- 2141.08\n",
      "Episode length: 491.25 +/- 19.63\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 491           |\n",
      "|    mean_reward          | -5.8e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 176000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7313392e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.46         |\n",
      "|    explained_variance   | 0.0465        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.88e+05      |\n",
      "|    n_updates            | 850           |\n",
      "|    policy_gradient_loss | 9.45e-05      |\n",
      "|    std                  | 0.383         |\n",
      "|    value_loss           | 8.42e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.93e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 478       |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 368       |\n",
      "|    total_timesteps | 176128    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 371          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014938596 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0563       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54e+04     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    std                  | 0.38         |\n",
      "|    value_loss           | 2.47e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-8782.10 +/- 7246.18\n",
      "Episode length: 500.00 +/- 2.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -8.78e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 180000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039029226 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.453        |\n",
      "|    explained_variance   | 0.0708        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.76e+05      |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | 0.000122      |\n",
      "|    std                  | 0.381         |\n",
      "|    value_loss           | 5.44e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 476       |\n",
      "|    ep_rew_mean     | -4.71e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 476       |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 378       |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 447          |\n",
      "|    ep_rew_mean          | -4.32e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 381          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072109727 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | 0.0441       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.7e+03      |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    std                  | 0.376        |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-5036.11 +/- 1353.04\n",
      "Episode length: 475.50 +/- 31.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | -5.04e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013252601 |\n",
      "|    clip_fraction        | 0.00396      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.0614       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.41e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000402    |\n",
      "|    std                  | 0.375        |\n",
      "|    value_loss           | 3.27e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 90        |\n",
      "|    time_elapsed    | 387       |\n",
      "|    total_timesteps | 184320    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.03e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001021493 |\n",
      "|    clip_fraction        | 0.00239     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.438      |\n",
      "|    explained_variance   | 0.0567      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+06    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.000983   |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 1.18e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-4937.15 +/- 682.47\n",
      "Episode length: 488.50 +/- 33.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -4.94e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042472114 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.0208       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+04     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | 5.02e-05     |\n",
      "|    std                  | 0.37         |\n",
      "|    value_loss           | 2.09e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -1.56e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 473       |\n",
      "|    iterations      | 92        |\n",
      "|    time_elapsed    | 397       |\n",
      "|    total_timesteps | 188416    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 461          |\n",
      "|    ep_rew_mean          | -4.44e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 401          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036777584 |\n",
      "|    clip_fraction        | 0.00952      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.426       |\n",
      "|    explained_variance   | 0.0763       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.59e+05     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    std                  | 0.371        |\n",
      "|    value_loss           | 7.01e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-5809.92 +/- 2685.15\n",
      "Episode length: 495.75 +/- 9.09\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 496        |\n",
      "|    mean_reward          | -5.81e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 192000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00660877 |\n",
      "|    clip_fraction        | 0.0348     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | 0.0142     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.04e+03   |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.00228   |\n",
      "|    std                  | 0.369      |\n",
      "|    value_loss           | 4.59e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 505       |\n",
      "|    ep_rew_mean     | -4.98e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 472       |\n",
      "|    iterations      | 94        |\n",
      "|    time_elapsed    | 407       |\n",
      "|    total_timesteps | 192512    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.17e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 473          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 411          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016286611 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | 0.0741       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+04      |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.000445    |\n",
      "|    std                  | 0.373        |\n",
      "|    value_loss           | 1.97e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-6467.68 +/- 3384.60\n",
      "Episode length: 488.88 +/- 21.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 489         |\n",
      "|    mean_reward          | -6.47e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001782655 |\n",
      "|    clip_fraction        | 0.000879    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.052       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.64e+04    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.000392   |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 7.57e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 903       |\n",
      "|    ep_rew_mean     | -9.09e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 470       |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 417       |\n",
      "|    total_timesteps | 196608    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -4.93e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 421         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001806881 |\n",
      "|    clip_fraction        | 0.00728     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.072       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.52e+05    |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 4.24e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-4696.01 +/- 749.74\n",
      "Episode length: 470.00 +/- 36.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 470          |\n",
      "|    mean_reward          | -4.7e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012703352 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.0531       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.373        |\n",
      "|    value_loss           | 7.24e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 660       |\n",
      "|    ep_rew_mean     | -3.36e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 469       |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 427       |\n",
      "|    total_timesteps | 200704    |\n",
      "----------------------------------\n",
      "After training: mean_reward:-6787.99 +/- 4464.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#del model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "model.set_parameters(\"last_model\")\n",
    "model.learn(\n",
    "    total_timesteps=200000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "# 评估训练后的 policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"After training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de305199a42f8d",
   "metadata": {},
   "source": [
    "### 绘图检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b9e88968d1a4a5",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-14T06:57:29.448728300Z",
     "start_time": "2024-05-14T06:57:25.919208500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 total reward: -6245.555073284907\n",
      "Episode 2 total reward: -4255.66386464431\n",
      "Episode 3 total reward: -4151.408883358096\n",
      "Episode 4 total reward: -4368.518640803928\n",
      "Episode 5 total reward: -4131.161859586723\n",
      "Episode 6 total reward: -4647.9817101721355\n",
      "Episode 7 total reward: -13595.564539928255\n",
      "Episode 8 total reward: -4772.59292190357\n",
      "Episode 9 total reward: -15943.961281821785\n",
      "Episode 10 total reward: -3922.3640478566467\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2000x800 with 10 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8YAAAMWCAYAAACDduxsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeZyN5f/H8fcMZsY2Yzdkkql+QtKipKzlG5K0R9aylCwhWdpsZS2lQpQ9MZSdlKwtlhKKStmJsc+G2a/fH1emJtsw58x9ltfz8ZgHc8597vtzTrnf55zPfV1XgDHGCAAAAAAAAAAAAAAAHxXodAEAAAAAAAAAAAAAALgTjXEAAAAAAAAAAAAAgE+jMQ4AAAAAAAAAAAAA8Gk0xgEAAAAAAAAAAAAAPo3GOAAAAAAAAAAAAADAp9EYBwAAAAAAAAAAAAD4NBrjAAAAAAAAAAAAAACfRmMcAAAAAAAAAAAAAODTaIwDAAAAAAAAAAAAAHwajXHgCvTv318BAQE5esw9e/YoICBAkydPztHjAgCyh8wAAGQVmQEAyCoyAwCQVWQG8A8a4/B5kydPVkBAwAV/1q1b53SJjomKilKLFi10/fXXKyAgQHXq1HG6JABwFJlxfsePH9eIESNUq1YtFS9eXIUKFdKdd96pqKgop0sDAMeQGRfWvXt33XrrrSpSpIjy5cunChUqqH///kpISHC6NABwBJmRNTt37lRISIgCAgL0448/Ol0OADiCzLiwa6655ryvyXPPPed0afAiuZ0uAMgpAwcOVLly5c65/brrrrvsfb366qvq06ePK8py1NixY7Vx40bdfvvtOn78uNPlAIDHIDMyW7t2rV555RXdf//9evXVV5U7d259/vnnatq0qX799VcNGDDA6RIBwDFkxrl++OEH1axZU08//bRCQkK0adMmDR06VF9//bXWrFmjwECu0Qfgn8iMi+vevbty586tpKQkp0sBAMeRGed3880368UXX8x02//93/85VA28EY1x+I2GDRuqatWqLtlX7ty5lTu39//zmTZtmq666ioFBgbqxhtvdLocAPAYZEZmlSpV0p9//qmyZctm3Pb888+rXr16GjZsmHr16qX8+fM7WCEAOIfMONe33357zm3XXnutevbsqQ0bNujOO+90oCoAcB6ZcWFffvmlvvzyS/Xq1UtvvPGG0+UAgOPIjPO76qqr1KJFC6fLgBfjMm3gb2fXvHjrrbf0zjvvqGzZssqbN69q166trVu3Ztr2fGtyLFu2TDVq1FChQoVUoEABlS9fXi+//HKmbY4cOaK2bduqZMmSCgkJUZUqVTRlypRzaomJiVGbNm0UFhamQoUKqXXr1oqJiTlv3b///rsee+wxFSlSRCEhIapataoWLFiQpeccERHBaA0AuAL+lhnlypXL1BSXpICAAD300ENKSkrSrl27LrkPAPBX/pYZF3LNNddk1AAAOD9/zYyUlBS98MILeuGFF3Tttddm+XEA4M/8NTMkKTk5WadOnbqsxwBn+cYlIkAWxMbG6tixY5luCwgIUNGiRTPdNnXqVMXHx6tTp05KTEzUqFGjdM899+iXX35RyZIlz7vvbdu26YEHHtBNN92kgQMHKjg4WDt27NB3332Xsc2ZM2dUp04d7dixQ507d1a5cuU0e/ZstWnTRjExMXrhhRckScYYNWnSRN9++62ee+45VahQQXPnzlXr1q3Pe9y7775bV111lfr06aP8+fNr1qxZeuihh/T555/r4Ycfzu7LBgB+iczImujoaElSsWLFLvuxAOAryIzzS01NVUxMjJKTk7V161a9+uqrKliwoO64445LPhYAfBWZcX7vvvuuTp48qVdffVVz5sy55PYA4A/IjPNbsWKF8uXLp7S0NJUtW1bdu3fPqAXIEgP4uEmTJhlJ5/0JDg7O2G737t1GksmbN685cOBAxu3r1683kkz37t0zbuvXr5/59z+fd955x0gyR48evWAd7777rpFkPvnkk4zbkpOTTfXq1U2BAgVMXFycMcaYefPmGUlm+PDhGdulpqaamjVrGklm0qRJGbffe++9pnLlyiYxMTHjtvT0dHPXXXeZ66+//rJep0qVKpnatWtf1mMAwNeQGVl3/PhxU6JECVOzZs3LfiwA+AIy4+LWrl2b6TUpX768WblyZZYeCwC+hsy4sEOHDpmCBQuacePGZXqtfvjhh0s+FgB8EZlxYY0bNzbDhg0z8+bNMxMmTMg4Rq9evS75WOAs5lCG3xg9erSWLVuW6eeLL744Z7uHHnpIV111Vcbvd9xxh6pVq6YlS5ZccN+FChWSJM2fP1/p6enn3WbJkiUKDw9Xs2bNMm7LkyePunbtqoSEBK1evTpju9y5c6tjx44Z2+XKlUtdunTJtL8TJ05oxYoVeuKJJxQfH69jx47p2LFjOn78uOrXr68///xTf/3116VfGADAOciMi0tPT1fz5s0VExOj999/P8uPAwBfRGacX8WKFbVs2TLNmzdPvXr1Uv78+ZWQkHDJxwGALyMzztW7d29FRkaqXbt2F90OAPwNmXGuBQsWqFevXmrSpImeeeYZrV69WvXr19fIkSN14MCBiz4WOIup1OE37rjjDlWtWvWS211//fXn3PZ///d/mjVr1gUf8+STT+rjjz9Wu3bt1KdPH91777165JFH9Nhjj2Ws4b13715df/3156zpXaFChYz7z/5ZqlQpFShQINN25cuXz/T7jh07ZIzRa6+9ptdee+28dR05ciRTKAIAsobMuLguXbpo6dKlmjp1qqpUqZKlxwCAryIzzi80NFT16tWTJDVp0kSffvqpmjRpop9++onsAOC3yIzM1q1bp2nTpmn58uXn1AQA/o7MuLSAgAB1795dX375pVatWqUWLVpk+bHwXzTGARfImzev1qxZo5UrV2rx4sVaunSpoqKidM899+irr75Srly5XH7Ms1dy9ezZU/Xr1z/vNtddd53LjwsAyB5vz4wBAwZozJgxGjp0qFq2bOmyGgEA5/L2zPi3Rx55RC1bttTMmTNpjAOAG3hjZvTq1Us1a9ZUuXLltGfPHknKWE/30KFD2rdvn66++mrXFg0A8MrMuJCIiAhJdkQ6kBU0xoH/+PPPP8+57Y8//tA111xz0ccFBgbq3nvv1b333quRI0dq8ODBeuWVV7Ry5UrVq1dPZcuW1c8//6z09PRMV1n9/vvvkqSyZctm/Ll8+XIlJCRkuspq+/btmY4XGRkpyU5fcnYkBgAgZ/lbZowePVr9+/dXt27d1Lt37yveDwD4I3/LjP9KSkpSenq6YmNjXbZPAPBV/pIZ+/bt0969e1WuXLlz7nvwwQcVFhammJiYy94vAPgTf8mMC9m1a5ckqXjx4i7bJ3wbc9QA/zFv3rxMa1ls2LBB69evV8OGDS/4mPNdjXTzzTdLsl8ASdL999+v6OhoRUVFZWyTmpqq999/XwUKFFDt2rUztktNTdXYsWMztktLSztnDdcSJUqoTp06GjdunA4dOnTO8Y8ePZqFZwsAyA5/yoyoqCh17dpVzZs318iRIy+5PQAgM3/JjJiYGKWkpJxz+8cffyxJWZoOEgD8nb9kxvjx4zV37txMP2fXpH3rrbc0ffr0iz4eAOA/mXHixAmlpaVlui0lJUVDhw5VUFCQ6tate9HHA2cxYhx+44svvsi4munf7rrrroyrlSQ7XUeNGjXUsWNHJSUl6d1331XRokXVq1evC+574MCBWrNmjRo1aqSyZcvqyJEjGjNmjMqUKaMaNWpIkjp06KBx48apTZs22rhxo6655hp99tln+u677/Tuu++qYMGCkqTGjRvr7rvvVp8+fbRnzx5VrFhRc+bMOe/IitGjR6tGjRqqXLmy2rdvr8jISB0+fFhr167VgQMHtGXLlou+JmvWrNGaNWsk2eA5deqU3njjDUlSrVq1VKtWrUu8qgDgm8iMzDZs2KBWrVqpaNGiuvfee8/5guq/rwsA+BMyI7NVq1apa9eueuyxx3T99dcrOTlZ33zzjebMmaOqVauy7h8Av0ZmZHbfffedc9vZEeK1a9fmYioAfo3MyGzBggV644039Nhjj6lcuXI6ceKEPv30U23dulWDBw9WeHh4ll9b+DkD+LhJkyYZSRf8mTRpkjHGmN27dxtJZsSIEebtt982ERERJjg42NSsWdNs2bIl0z779etn/v3PZ/ny5aZJkyamdOnSJigoyJQuXdo0a9bM/PHHH5ked/jwYfP000+bYsWKmaCgIFO5cuWM4//b8ePHTcuWLU1oaKgJCwszLVu2NJs2bcpU71k7d+40rVq1MuHh4SZPnjzmqquuMg888ID57LPPLvnanH0e5/vp169fll5fAPAlZEb2XhcA8Cdkxvnt2LHDtGrVykRGRpq8efOakJAQU6lSJdOvXz+TkJCQ9RcYAHwImXH5r9UPP/xw2Y8FAF9AZpzfjz/+aBo3bmyuuuoqExQUZAoUKGBq1KhhZs2alfUXFzDGBBhjTPbb64D327Nnj8qVK6cRI0aoZ8+eTpcDAPBgZAYAIKvIDABAVpEZAICsIjOAK8Ma4wAAAAAAAAAAAAAAn0ZjHAAAAAAAAAAAAADg02iMAwAAAAAAAAAAAAB8mqON8SFDhuj2229XwYIFVaJECT300EPavn17pm0SExPVqVMnFS1aVAUKFNCjjz6qw4cPZ9pm3759atSokfLly6cSJUropZdeUmpqaqZtVq1apVtvvVXBwcG67rrrNHnyZHc/PXiZa665RsYY1uMAPBSZAU9CZgCejcyAJyEzAM9GZsCTkBmAZyMz4EnIDODKONoYX716tTp16qR169Zp2bJlSklJ0X333adTp05lbNO9e3ctXLhQs2fP1urVq3Xw4EE98sgjGfenpaWpUaNGSk5O1vfff68pU6Zo8uTJev311zO22b17txo1aqS6detq8+bN6tatm9q1a6cvv/wyR58vAODKkRkAgKwiMwAAWUVmAACyiswAAB9gPMiRI0eMJLN69WpjjDExMTEmT548Zvbs2Rnb/Pbbb0aSWbt2rTHGmCVLlpjAwEATHR2dsc3YsWNNaGioSUpKMsYY06tXL1OpUqVMx3ryySdN/fr13f2UAABuQmYAALKKzAAAZBWZAQDIKjIDALxPbqca8ucTGxsrSSpSpIgkaePGjUpJSVG9evUytrnhhht09dVXa+3atbrzzju1du1aVa5cWSVLlszYpn79+urYsaO2bdumW265RWvXrs20j7PbdOvW7bx1JCUlKSkpKeP39PR0nThxQkWLFlVAQICrni4AuJwxRvHx8SpdurQCAx2dFMTtyAwAyD5/yQ0yAwCyj8wgMwAgq8gMMgMAsiqnM8NjGuPp6enq1q2b7r77bt14442SpOjoaAUFBalQoUKZti1ZsqSio6Mztvl3iJy9/+x9F9smLi5OZ86cUd68eTPdN2TIEA0YMMBlzw0Actr+/ftVpkwZp8twGzIDAFzLl3ODzAAA1yIzLDIDAC6NzLDIDAC4tJzKDI9pjHfq1Elbt27Vt99+63Qp6tu3r3r06JHxe2xsrK6++mrt379foaGhDlYG+KmkJOmBB6SCBaXPP5e40vGC4uLiFBERoYIFCzpdiluRGQDgGv6QG2QGALgGmZGzyAwA3ozMyFlkBpBNxkhNmkiffSYFBTldjd/J6czwiMZ4586dtWjRIq1ZsybT1QDh4eFKTk5WTExMpqusDh8+rPDw8IxtNmzYkGl/hw8fzrjv7J9nb/v3NqGhoedcXSVJwcHBCg4OPuf20NBQggRwQq9eNpzGj5fCwpyuxiv48jRJZAYAuJ6v5gaZAQCuR2ZYZAYAXBqZYZEZgIdbvVqqUUMqVszpSvxaTmWGowt8GGPUuXNnzZ07VytWrFC5cuUy3X/bbbcpT548Wr58ecZt27dv1759+1S9enVJUvXq1fXLL7/oyJEjGdssW7ZMoaGhqlixYsY2/97H2W3O7gOAB/v2W2ntWqlpUyky0ulq4CAyAwCQVWQGACCryAwAQFaRGYCP6tZNeuopp6tATjEO6tixowkLCzOrVq0yhw4dyvg5ffp0xjbPPfecufrqq82KFSvMjz/+aKpXr26qV6+ecX9qaqq58cYbzX333Wc2b95sli5daooXL2769u2bsc2uXbtMvnz5zEsvvWR+++03M3r0aJMrVy6zdOnSLNUZGxtrJJnY2FjXPXkAlxYfb8zddxtTt64xqalOV+MVfPl8RWYAgOv56jmLzAAA1/PVcxaZAQCu56vnLDID8EHffmtMRITTVfi1nD5nOdoYl3Ten0mTJmVsc+bMGfP888+bwoULm3z58pmHH37YHDp0KNN+9uzZYxo2bGjy5s1rihUrZl588UWTkpKSaZuVK1eam2++2QQFBZnIyMhMx7gUggRwSMeOtim+ZYvTlXgNXz5fkRkA4Hq+es4iMwDA9Xz1nEVmAIDr+eo5i8wAfNCttxozZozTVfi1nD5nBRhjjKtHofuauLg4hYWFKTY29qJrcqSlpSklJSUHK/MNefLkUa5cuZwuA57miy+k0aOlW2+VBg50uhqvkdXzFdyH/waAe/A+68pc6n0W5yxn8foD7kFmXBkyw7Px+gPuQWZcGTLDs/H6A1n0229S3brSwYNS4KVXniYzroynZUZutx/BDxhjFB0drZiYGKdL8VqFChVSeHi4AgICnC4FnuDECWnIEBtGr7zidDUAAAfxPiv7eJ8FwF+QGdlHZgDwF2RG9pEZALxehw5Sx46XbIqTGdnnSZlBY9wFzv6DKFGihPLly+cR/2G9hTFGp0+f1pEjRyRJpUqVcrgieIRu3aSwMKlvXyk42OlqAAAO4n3WleN9FgB/Q2ZcOTIDgL8hM64cmQHAJ2zeLO3cKS1bdslNyYwr54mZQWM8m9LS0jL+QRQtWtTpcrxS3rx5JUlHjhxRiRIlmFbd382aJSUlSTfcIN11l9PVAAAcxPus7ON9FgB/QWZkH5kBwF+QGdlHZgDweh06SJ06SSEhF92MzMg+T8sMGuPZdHY9gXz58jlciXc7+/qlpKQ4/o8CDtq/Xxo71v59yhRnawEAOI73Wa7B+ywA/oDMcA0yA4A/IDNcg8wA4LW++UY6dEh66aVLbkpmuIYnZcalV5NHljB1Qvbw+kFpaXY9j9BQafjwS16pBQDwH7xPyB5ePwD+hHNe9vD6AfAnnPOyh9cPgFcyRurSxS7nGhSU5YdxzsseT3r9GDEOwDMMGyZdfbVUpIh0++1OVwMAAAAAAAAAAHzJokVSbKzUtavTlcAhjBhHjrnmmmv07rvvOl0GPNGGDdKPP0rbt0uvv+50NQAAeBXeYwEAsorMAABkFZkBwOekptrp0196ScqTx+lqfIo3ZQaNcSg6OlovvPCCrrvuOoWEhKhkyZK6++67NXbsWJ0+fdrp8uDr4uOlPn3s399++7KmLwEAwJPxHgsAkFVkBgAgq8gMALhC770nBQZKHTo4XUmOITPOxVTqHiA2MVbxyfEqE1rmnPsOxB1QwaCCCgsJc8uxd+3apbvvvluFChXS4MGDVblyZQUHB+uXX37R+PHjddVVV+nBBx90y7EBSVL37tJdd0nBwdLNNztdDQDAh/AeCwCQVWQGACCryAwA8EInT0qjR0vvvCPlzrnWKJnheRgx7rDYxFg1mN5AtSfX1v7Y/Znu2x+7X7Un11aD6Q0UmxjrluM///zzyp07t3788Uc98cQTqlChgiIjI9WkSRMtXrxYjRs3liTt27dPTZo0UYECBRQaGqonnnhChw8fztjPzp071aRJE5UsWVIFChTQ7bffrq+//totNcOHzJol5c1rp1I/O2ocAAAX4D0WACCryAwAQFaRGQDgpXr1kkqWlP4+T+YEMsMz0Rh3WHxyvI6cOqJdJ3epzpQ6Gf849sfuV50pdbTr5C4dOXVE8cnxLj/28ePH9dVXX6lTp07Knz//ebcJCAhQenq6mjRpohMnTmj16tVatmyZdu3apSeffDJju4SEBN1///1avny5Nm3apAYNGqhx48bat2+fy+uGj9i3T5owQTpwwF6lxZoeAAAX4j0WACCryAwAQFaRGQDghX7/XVq1Sho1SgoIyLHDkhmeianUHVYmtIxWtV6V8Y+gzpQ6mvbwNLWc21K7Tu5SZOFIrWq96rzTLGTXjh07ZIxR+fLlM91erFgxJSYmSpI6deqkevXq6ZdfftHu3bsVEREhSZo6daoqVaqkH374QbfffruqVKmiKlWqZOxj0KBBmjt3rhYsWKDOnTu7vHZ4ubQ0qWNHqUYNKV8+qVIlpysCAPgY3mMBALKKzAAAZBWZAQBeqGtX24O4/fYcPSyZ4ZkYMe4BIsIitKr1KkUWjtSuk7t098S7M/2jiAiLyNF6NmzYoM2bN6tSpUpKSkrSb7/9poiIiIx/FJJUsWJFFSpUSL/99pske8VIz549VaFCBRUqVEgFChTQb7/95rVXjMDNhgyR7r5b+uEHu8Y4AABuwHssAEBWkRkAgKwiMwDAi3zxhbR/v/T2244cnszwPIwY9xARYRGa9vA03T3x7ozbpj08za3/KK677joFBARo+/btmW6PjIyUJOXNmzfL++rZs6eWLVumt956S9ddd53y5s2rxx57TMnJyS6tGT5gzRpp+3bp8GFp4kQpkOtzAADuw3ssAEBWkRkAgKwiMwDACyQmSq+8It1zj3TttY6VQWZ4FjpSHmJ/7H61nNsy020t57bMWHPAHYoWLar//e9/+uCDD3Tq1KkLblehQgXt379f+/f/U8uvv/6qmJgYVaxYUZL03XffqU2bNnr44YdVuXJlhYeHa8+ePW6rHV7q6FGpf3+pdGmpXTupjOunCAEA4N94jwUAyCoyAwCQVWQGAHiBoUOl1FTpjTccLYPM8Cw0xj3A/tj9GWsMRBaO1HfPfJcxrUKdKXXc+o9jzJgxSk1NVdWqVRUVFaXffvtN27dv1yeffKLff/9duXLlUr169VS5cmU1b95cP/30kzZs2KBWrVqpdu3aqlq1qiTp+uuv15w5c7R582Zt2bJFTz31lNLT091WN7xQerr07LNSq1bSkSPSE084XREAwMfxHgsAkFVkBgAgq8gMAPACO3dK8+dLzz8vFS7sWBlkhuehMe6wA3EHMv2jWNV6le6KuCvTmgN1ptTRgbgDbjn+tddeq02bNqlevXrq27evqlSpoqpVq+r9999Xz549NWjQIAUEBGj+/PkqXLiwatWqpXr16ikyMlJRUVEZ+xk5cqQKFy6su+66S40bN1b9+vV16623uqVmeKmRI6UaNaSpU6V333W6GgCAj+M9FgAgq8gMAEBWkRkA4AWMkbp1k/Llk9q3d6wMMsMzBRhjjNNFeLq4uDiFhYUpNjZWoaGhme5LTEzU7t27Va5cOYWEhFz2vmMTY9VgegMdOXVEq1qvyrSmwNkrSUrkL6GlzZcqLCQsu0/FY2X3dYSHW7dOGjXKBlHr1lKtWk5X5LMudr5CzuC/AeA62Xl/wHusf1zsdeSc5Sxef8B1yAzXIDM8F68/4DpkhmuQGZ6L1x9+b84cafhw+5PNXgSZ4RqelBm53X4EXFRYSJiWNl+q+OR4lQnNvN5yRFiEVrdZrYJBBX3+HwV82MmT0ssvS08/Lf36K01xAECO4D0WACCryAwAQFaRGQDg4RISpGHDpHLlHO9FkBmeica4BwgLCbvg//j//ccCeBVjpI4dpRdftCPGFy1yuiIAgB/hPRYAIKvIDABAVpEZAODBBg6UAgNtc9wDkBmehzXGAbjP6NHS7bdLH39sG+NBQU5XBAAAAAAAAAAAfM22bXZZ1/vuk8qWdboaeCga4wDc46efpNWrpdy5pXvvlSpUcLoiAAAAAAAAAADga9LT7cy1aWlS795OVwMPRmMcgOvFxUk9e0qdOknffmv/BAAAAAAAAAAAcLUPP7Qz1nbrJuXL53Q18GCsMe4i6enpTpfg1Xj9fIgxthHet680aJAUFSUFBDhdFQDAi/E+IXt4/QD4E8552cPrB8CfcM7LHl4/AB7jwAFp1iypUCHpscfccgjOednjSa8fjfFsCgoKUmBgoA4ePKjixYsrKChIATQBs8wYo+TkZB09elSBgYEKYg1q7zdmjFS5sjR7tvTKK1KxYk5XBADwUrzPyh7eZwHwJ2RG9pAZAPwJmZE9ZAYAj2KM1L27lCeP9NZbLh+kR2ZkjydmBo3xbAoMDFS5cuV06NAhHTx40OlyvFa+fPl09dVXKzCQ2f292oYNdl3xRx6RYmOle+5xuiIAgBfjfZZr8D4LgD8gM1yDzADgD8gM1yAzAHiEzz6TcueWatSQrrvO5bsnM1zDkzKDxrgLBAUF6eqrr1ZqaqrS0tKcLsfr5MqVS7lz5+YqG293/LjUp4/0zjtS797SwoVOVwQA8AG8z8oe3mcB8CdkRvaQGQD8CZmRPWQGAI9w8qQ0erT9+6RJbjsMmZE9npYZNMZdJCAgQHny5FGePHmcLgXIeenpUvv20tChtjk+dqydugQAABfgfRYAIKvIDABAVpEZAODleveWypSRWrSQQkLceigyw3c4OmZ9zZo1aty4sUqXLq2AgADNmzcv0/0BAQHn/RkxYkTGNtdcc8059w8dOjTTfn7++WfVrFlTISEhioiI0PDhw3Pi6QH+4803pfvvlxYskFq1ksqVc7oi+CAyAwCQVWQGACCryAwAQFaRGYAHWbFCSkiwg/YaNHC6GngRRxvjp06dUpUqVTT67FQH/3Ho0KFMPxMnTlRAQIAeffTRTNsNHDgw03ZdunTJuC8uLk733XefypYtq40bN2rEiBHq37+/xo8f79bnBviNr7+W9uyRIiOl6GipWTOnK4KPIjMAAFlFZgAAsorMAABkFZkBeIgzZ6Q33rDLu/7rwhMgKxydSr1hw4Zq2LDhBe8PDw/P9Pv8+fNVt25dRUZGZrq9YMGC52x71vTp05WcnKyJEycqKChIlSpV0ubNmzVy5Eh16NAh+08C8GcHDtjp0ydOlNq0sSPGATchMwAAWUVmAACyiswAAGQVmQF4iAEDpCpVpIgI6aqrnK4GXsbREeOX4/Dhw1q8eLHatm17zn1Dhw5V0aJFdcstt2jEiBFKTU3NuG/t2rWqVauWgoKCMm6rX7++tm/frpMnT573WElJSYqLi8v0A+A/UlLsuuKjR0svvii9/bZUoIDTVQGSyAwALpSQIPXs6XQVcCMyAwCQVWQGACCryAzATdavl/bulbZulf412wKQVY6OGL8cU6ZMUcGCBfXII49kur1r16669dZbVaRIEX3//ffq27evDh06pJEjR0qSoqOjVe4/6x2XLFky477ChQufc6whQ4ZowIABbnomgI/o3Vtq105atky6+27pllucrgjIQGYAcAlj7IesNm2crgRuRGYAALKKzAAAZBWZAbhBYqL08stS6dLSoEFSnjxOVwQv5DWN8YkTJ6p58+YKCQnJdHuPHj0y/n7TTTcpKChIzz77rIYMGaLg4OArOlbfvn0z7TcuLk4RERFXVjjgiz77TAoIsOuKf/qp/R3wIGQGAJcYOVIqW1aqXdvpSuBGZAYAIKvIDABAVpEZgBv07y/VrSsdOSLdeafT1cBLeUVj/JtvvtH27dsVFRV1yW2rVaum1NRU7dmzR+XLl1d4eLgOHz6caZuzv19oHY/g4OArDiHA5/3xhzRhgvTJJ9Ljj0uzZtkmOeAhyAwALrF8uTR0qLR9u9OVwI3IDABAVpEZAICsIjMAN1i/XvrrL+ngQWnePKergRfzijXGJ0yYoNtuu01VqlS55LabN29WYGCgSpQoIUmqXr261qxZo5SUlIxtli1bpvLly5932hEAF3HqlPT889JHH0ndukkDB0rFijldFZAJmQEg2w4ckJo2lWbPlooUcboauBGZAQDIKjIDAJBVZAbgYmenUC9WzPYlChZ0uiJ4MUcb4wkJCdq8ebM2b94sSdq9e7c2b96sffv2ZWwTFxen2bNnq127duc8fu3atXr33Xe1ZcsW7dq1S9OnT1f37t3VokWLjJB46qmnFBQUpLZt22rbtm2KiorSqFGjMk0tAiALjJE6drQBtHixdOONUo0aTlcFP0JmAMgRZ85INWtKPXpIdeo4XQ2uEJkBAMgqMgMAkFVkBuCQ/v2lBx+0U6g3bux0NfB2xkErV640ks75ad26dcY248aNM3nz5jUxMTHnPH7jxo2mWrVqJiwszISEhJgKFSqYwYMHm8TExEzbbdmyxdSoUcMEBwebq666ygwdOvSy6oyNjTWSTGxs7BU9T8AnvPuuMSNGGLNpkzEPP2xMWprTFeE8fPl8RWYAcLv0dGPuvtuYRx91upIc46vnLDIDAFzPV89ZZAYAuJ6vnrPIDMAB69YZ06qVMffdZ8yhQ05XAzfI6XNWgDHGuLn37vXi4uIUFham2NhYhYaGOl0OkPNWr7bTp48ZIz30kBQVJRUv7nRVOA/OV87jvwHgxZ55RvrpJ2njRilXLqeryRGcs5zF6w/Am3DOchavPwBvwjnLWbz+8BmJiVKjRtI990glS0rnmYkB3i+nz1m53X4EAN7twAG7lvi8eVKnTvbvNMUBAL5m8GBp+XJpyxa/aYoDAAAAAAB4rP79pWbNbG9i4UKnq4GPoDEO4MKSkuxVWB9+KM2cKVWowLriAADfM3WqNHq0tHKlVKiQ09UAAAAAAAD4t/Xrpeho6c8/pZEjpYAApyuCj6AxDuDCunWTOneWTp+WliyRPv/c6YoAAHCtr7+W+vSxF4D93/85XQ0AAAAAAIB/O3NGevllqWlT6ehRvq+BS9EYB3B+H39s1+2oXduuKz5zphQY6HRVAAC4zubNUps29srjWrWcrgYAAAAAAAB9+0odOtgexZIlTlcDH0NjHMC5NmyQvvhCmjVLeuYZqV8/1hUHAPiWP/+UHn5Y6tLFXoEMAAAAAAAAZ339tZScbBvigwdLefI4XRF8DMM/AWR25IidUnbCBGnSJKl8eUbRAQB8y549UuPGUoMGUu/eTlcDAAAAAACAmBjbDK9TRypVSrr9dqcrgg9ixDiAf6SmSm3bSqNGSfv3S4sXs644AMC3/PWX9OCDUsWK0ujRTlcDAAAAAAAASerWzQ7aGzbM9iYAN6AxDuAfffpIzZtLZcva6WVnzGBdcQCA7zh82I4UL1VKmjmTjAMAAAAAAPAEs2dLZcpIUVHSG29IISFOVwQfxbeBAKyoKPvnk09KHTtKAwdKJUo4WxMAAK5y4oRtihcoIM2dKwUFOV0RAAAAAAAADh2SPvpIuuMOqVAhqXp1pyuCD2PEOABp61Zp+nRpzhw7jXrVqtLddztdFQAArhEba5vigYHSggVSvnxOVwQAAAAAAABjpE6dpEGDpL59pUWLnK4IPo7GOODvYmKkF16w06avXy9t2GCb5AAA+ILYWKlJEykpSVqyxF55DAAAAAAAAOeNGyfVrClNnCgNGMBgBrgdU6kD/iw9XWrfXho61P7++uvShx9KAQHO1gUAgCvExEgPPSQlJNhZUVgiBAAAAAAAwDP8+ae0eLFUsaJdU7xmTacrgh9gxDjgz954Q6pfX7r1VumRR6R335VCQ52uCgCA7Dt5Unr0Uen0aemTT6Srr3a6IgAAAAAAAEhSaqrUtav09ttS587SwoVOVwQ/wYhxwF8tXiwdPCi1ayf16yc99phUubLTVQEAkH0nTtgLvk6flj76SLrhBqcrAgAAAAAAwFnDhknNmkkffCC99pqUP7/TFcFPMGIc8Ec7d0rvvy/Nn28b5CdOSC1bOl0VAADZd/y4vdgrMVEaM0a66SanKwIAAAAAAMBZP/wgbdsm3Xmn/b1uXWfrgV+hMQ74m1OnpGeflSZNkg4dkt57zzbIAQDwdseOSU88ISUn23y79VanKwIAAAAAAMBZCQlSnz7S5MlS69b0JpDjaIwD/sQY6fnnpVdekYoXlx58UBo/XgoJcboyAACy5+hR6cknpZQUacQI6fbbna4IAAAAAAAA/9a9u506fdgw6eWXpYIFna4IfobGOOBP3nvPTilbt65tkL/wgnTNNU5XBQBA9hw6JDVvLqWnS2++Kd11l9MVAQAAAAAA4N8++0wqVswOaggIkOrVc7oi+CEa44C/WLPGrt0xbZr9KVxYatTI6aoAAMievXvt1Ft58kivvirVquV0RQAAAAAAAPi3/fuljz6yvYmmTaWFC52uCH6KxjjgD/76Sxo4UJo3T9q6VZo9W5o71+mqAADInj/+kDp0kPLmlbp1k+65x+mKAAAAAAAA8G9paVLHjtIHH9j1xd94Q8qf3+mq4KdojAO+LilJatdOGjPGTjH7wgvSzJlSrlxOVwYAwJX7+WfbDD/bFK9f3+mKAAAAAAAA8F/DhkmPPipt2yaVKMESeHAUjXHA1/XoYa/Guv56u/7qoEE2fAAA8Fbr10uvvCIFB0svvsiaVAAAAACwd68UGmqXTwQAT7Fhg22It21r+xOLFztdEfxcoNMFAHCjyZOlokWlBx+U3n5bqlZNuvtup6sCAODKrVol9esn5c4t9epFUxwAAAAAdu+Wnn5aio93uhIA+Ed8vJ06/f337Uy2I0bYQQ6AgxgxDviqn36SFi6UZs2SVqyQfvnFNsoBAPBWixdLH34oBQRIfftKtWo5XREAAAAAOGvzZqlLF2n6dOnqq52uBgD+0b27HdywZIl0443SLbc4XRFAYxzwSceOST17Sp99Jv31lzR4sDR/vm0kAADgjWbPlqKipORk6fXXmQEFAAAAAJYtk5o2pSkOwPNERUklS0rXXmv7E0yhDg9BYxzwNWlpdr2Od96R8uWTmjWTxo2T8ud3ujIAAK7Mxx/bKdTj46UBA6Q773S6IgAAAABw1tSpUo8edjBMjRpOVwMA/9i7V5o0SVqwQHriCWnUKLskHuAB+D8R8DWvvio9/rh0003Ss89K3brZq7IAAPA2xkhDhkiHDklHjkhvvindfrvTVQEAAACAswYMkMaOldaskSpWdLoaAPhHWpr0/PPS6NHSxIlSnTrSDTc4XRWQgcY44EvmzJFOn5ZatJDGj7dTKDVs6HRVAABcvvR0O/ohLEz6/Xdp6FDpttucrgoAAAAAnJOaKj31lPTjj9KWLXaaYgDwJEOG2FHikrRokR01DniQQCcPvmbNGjVu3FilS5dWQECA5s2bl+n+Nm3aKCAgINNPgwYNMm1z4sQJNW/eXKGhoSpUqJDatm2rhISETNv8/PPPqlmzpkJCQhQREaHhw4e7+6kBOe/33+1UsyNGSOvWSV9/Lb38stNVAS5DZgB+JDlZatNGKltW+u476e23aYrjspAZAICsIjMAeI2TJ+2yUnv3Slu30hR3AJkBXMK6ddL27VLz5lKXLtL770uBjrYhgXM4+n/kqVOnVKVKFY0ePfqC2zRo0ECHDh3K+JkxY0am+5s3b65t27Zp2bJlWrRokdasWaMOHTpk3B8XF6f77rtPZcuW1caNGzVixAj1799f48ePd9vzAnJcXJzUqZM0YYJ9k/zKK9JHHxE68ClkBuAnEhKkJ5+UatWSFi6Uxoyxy4MAl4HMAOAxvv7aTnULj0VmAPAKW7dKt95ql0v87jspXz6nK/JLZAZwEXFxdqDe++9LI0dKDz8slSvndFXAuYyHkGTmzp2b6bbWrVubJk2aXPAxv/76q5Fkfvjhh4zbvvjiCxMQEGD++usvY4wxY8aMMYULFzZJSUkZ2/Tu3duUL18+y7XFxsYaSSY2NjbLjwFyTHq6MU2bGvPdd8YkJxvTqJExW7c6XRUc4i/nKzID8FFHjxpTv74x06cbU7euMXv2OF2Rz/OHcxaZAcAxixcb06SJMadPO12JS/jDOYvMAOCRoqKMKVXKmGHDnK4ky/zhnEVmAP/Rpo0xa9YYs2mTMY8/bvsWQBbk9DnL44eTrlq1SiVKlFD58uXVsWNHHT9+POO+tWvXqlChQqpatWrGbfXq1VNgYKDWr1+fsU2tWrUUFBSUsU39+vW1fft2nTx58rzHTEpKUlxcXKYfwGMNHy7VqCHddZfUu7fUurVUqZLTVQGOIDMAL7Zvnx0p3q6dNHGiNH26nUodcBMyA4BbDR9uR8xERUl58zpdDbKJzADgiNRUqXNn6cUXpSlTpF69nK4IWUBmwC/NmCFFREhVq9pz1ujRUkCA01UB5+XRjfEGDRpo6tSpWr58uYYNG6bVq1erYcOGSktLkyRFR0erRIkSmR6TO3duFSlSRNHR0RnblPzPeitnfz+7zX8NGTJEYWFhGT8RERGufmqAayxbZtfseP55Gz65c0uPP+50VYAjyAzAi/36q11TvHNnuxRIVJRUqpTTVcGHkRkA3KpvX2nUKGnpUik42OlqkE1kBgBHHDwo1a4trVwprV4t/e9/TleELCAz4Jf27JGmTpVef92+D+7RQype3OmqgAvK7XQBF9O0adOMv1euXFk33XSTrr32Wq1atUr33nuv247bt29f9ejRI+P3uLg4wgSeZ88eOwphwQLpl19sY3zOHKerAhxDZgBeat06++GpSxfp44+l2bOl0FCnq4KPIzMAuEV6up395Kef7FqwhQs7XRFcgMwAkOO+/tpeNFy2rL3IqmBBpytCFpEZ8DupqXbQ3ujR0ooVUkqK1KiR01UBF+XRI8b/KzIyUsWKFdOOHTskSeHh4Tpy5EimbVJTU3XixAmFh4dnbHP48OFM25z9/ew2/xUcHKzQ0NBMP4BHOXNG6tBBGj9eSkyUunWTJkywI8YBSCIzAK+wYIE0ZIjNtClTaIrDMWQGgGw7dUqqXl3av1/ato2muA8jMwC4TVqa9Nprtsn0xBPSF1/QFPdyZAZ83uDBUrNmUliYNHSoNGKE0xUBl+RVjfEDBw7o+PHjKvX31JrVq1dXTEyMNm7cmLHNihUrlJ6ermrVqmVss2bNGqWkpGRss2zZMpUvX16F+aAKb2SMHVX34ov2ytF27ezIcaYnATIhMwAPN3q0NH++1LSp9Nlndvr0fPmcrgp+iswAkC1//SVVqWI/n33/vRQS4nRFcCMyA4BbHDki1a8vff659P770sCBUqBXfXWP8yAz4NO+/17asUNq3tzOcjF8ON/rwCs4mq4JCQnavHmzNm/eLEnavXu3Nm/erH379ikhIUEvvfSS1q1bpz179mj58uVq0qSJrrvuOtWvX1+SVKFCBTVo0EDt27fXhg0b9N1336lz585q2rSpSpcuLUl66qmnFBQUpLZt22rbtm2KiorSqFGjMk0tAniVceOkcuXsm+WBA+3UJFWrOl0V4HZkBuAj0tOll16yX/zUrCl9+aX0ySeswQqXIjMA5Jjvv5fuuEN6/HFp1iyaGF6IzADguK++kurVk06ftlOn/31+gechM4C/xcZKr75qL+SZNk268UZ6FPAexkErV640ks75ad26tTl9+rS57777TPHixU2ePHlM2bJlTfv27U10dHSmfRw/ftw0a9bMFChQwISGhpqnn37axMfHZ9pmy5YtpkaNGiY4ONhcddVVZujQoZdVZ2xsrJFkYmNjs/2cgWz5/ntjHn/cmLQ0Y+bPN+a555yuCB7Gl89XZAbgA86cMaZ5c2MmTDDm7beN6dTJZhoc46vnLDIDQI4YOdKY8HBjpk51upIc4avnLDIDgGMSE43p1s2YW24xpn17+7uP8NVzFpkBGGPS041p1cqYb781ZtcuYxo2NCY11emq4MVy+pwVYIwxbu69e724uDiFhYUpNjaW9TngnOho6amnpLlzpYMH7VTq8+ZJQUFOVwYPwvnKefw3AC7gxAmpZUu7HMiaNTa/+vWTAgKcrsyvcc5yFq8/4KUSE6UWLaSNG+3ns5tvdrqiHME5y1m8/oCP+f136dlnpTNn7Gekli2drsilOGc5i9cfbjVlirR3r/TKK1LjxnapvHLlnK4KXiynz1m53X4EANmXkmLXEv/gA7vGeKdO0qef0hQHAHiHXbuk9u3telMffWSn2Orc2emqAAC4fHv3Sg89JOXOLa1fL5Uo4XRFAABvYow0frw0ebKUK5c0YYJUubLTVQFA1vzxhzR7tjR/vjRsmPTEEzTF4XVY/ArwBn36SK1bS+XLS888Iw0dKoWHO10VAACX9sMPUocO0ocfSm+9JdWuTVMcAOCdli2za8CWLy999x1NcQDA5Tl+XHrySWnWLKlSJenLL2mKA/AeSUl2wN6HH0qbN0u//GJ7FoCXYcQ44Olmz7Z/Pv649PLLUpMm0h13OFsTAABZsXChHSE+caL98NSpk9SggdNVAQBweVJT7VSRn30mPf+81KMHS4EAAC7P8uXS66/bv3fqZJdLBABv0ru31LWrVKSI1KqV7VvwnhheiMY44Ml+/12aNMlOTTJrll13iKuwAADeYMwYO1p8zBg728mAAVL16k5XBQDA5TlwwDYvjh+Xpk6V7r7b6YoAAN7k9Gk7E+SOHVJwsDRunHT99U5XBQCXZ+FC+2fjxnYWwN69paJFna0JuEI0xgFPlZBgryCdNk369Vfpk0+kOXOcrgoAgItLT5f69pWCgqR+/exVxO+/b6cKBADAmyxaZDOtWDFpxQqpZEmnKwIAeJN162xTvHBh2wwfPtw2xwHAm/z1l/Tee/a98ZIlUq5cUv36TlcFXDEa44AnMsZO0ffaa7ax0L27HTGem3+yAAAPlpgotW9v1xGvXt2OFJ84UbrmGqcrAwAg65KTpV69pFWrpP/9zzYy+CwGAMiqpCQ7Y9Zvv9lphlu3lh56yOmqAODypaVJzz4rffCBFBMjjRz5z+hxwEvxyQ7wRBMnSjfcINWoIT3yiPTOO3aUAgAAnurECallS6lLFykszF7UNXOmVKKE05UBAJB1O3dKbdtKcXF2LdhHHnG6IgCAN9myRerZU4qMlFJT7TIcERFOVwUAV+bNN6XHH7ezXjz6qPTuu1LevE5XBWQLjXHA0+zcaadMX7DAvpFu0UKqUsXpqgAAuLDdu20T4e23pcOHpcGDpc8+k0JDna4MAICsMcYuX/X++3Z0+IwZUvnyTlcFAPAWqanSiBF2+vSQEOm666SxY6XAQKcrA4Ar88030q5ddlbbd9+V6tWTbrzR6aqAbKMxDniS1FSpc2f7xvmTT+zVV0884XRVAABc2I8/Sr17S5Mm2Q9NixZJs2fbL4MAAPAGJ05InTpJBw9KN90kjRol5c/vdFUAAG/xxx/SCy9IFSvaGUdGjpRuucXpqgDgyp04IfXrJ82fL23eLH3/vV3qFfABNMYBTzJsmNS0qXTkiA2d2bOdrggAgAtbtEgaN86ODv/oI2nfPmn6dClXLqcrAwAga77+2n7pZ4xdDqRZM6crAgB4i7Q0O4py5UopPFxKTpYWL5by5XO6MgC4csZIHTtKw4fbWS969pSioqSAAKcrA1yCxjjgKX78Udq6VXrmGal5czudOo0FAICn+ugj6dtv7Yejvn3tF0Hvv88HJQCAd0hMtPn1++92lpNx4+y0twAAZMW2bdKLL0p33imdPm3X4L3/fqerAoDsGzNGqlZNqlpVeu45+565WDGnqwJchsY44AlOn5Z69ZKmTbON8dGjpUKFnK4KAIBzGSMNGiTFx9ulP9q2lRo2lFq1croyAACyZssWqXt3qUABqUIFacgQKTjY6aoAAN4gJcXO+Lh5s3THHXaQy8yZUokSTlcGANm3ZYu0YoWdyfbzz6WwMLu2OOBDaIwDnqB3b+mll2yjoWNH++UMAACeJi1N6tpVKlfOTjf72GN2Lb369Z2uDACAS0tLs+u+Ll8upadLHTpIDzzgdFUAAG+xaZMd2PLAA9KZM1KpUtKAAcyaBcA3nDol9eghzZgh/fWXnVFp0SKnqwJcjsY44LSlS+2fe/dKV10lPfigs/UAAHA+iYnS00/b6QFr17ZN8eHDpdtuc7oyAAAubedOe3FXyZJ2yapPPpHKlHG6KgCAN0hMtINZ9u6VGjSQvvzSLiN17bVOVwYArtOtm/TKK1LRorZHMXq0FBTkdFWAywU6XQDg144ft02FBx+UVq+2wQMAgKc5c0Zq1kxq2VK6+WapTRu7xjhNcQCAp0tPlz74QOrUyX6xV7asNH8+TXEAQNasWyc1aiRFRtqlECVp4UKa4gB8y9klIe65xy4z9Pjj0vXXO10V4BaMGAecYozUubOdnmTYMGnePCmQa1UAAB7m9GnbFO/SRcqdW+rZU4qKkooXd7oyAAAubvduO0o8MlJKSpL695fuvNPpqgAA3uD0aenVV6WYGKlpU/sZ6J13pEqVnK4MAFxrzx5p8mR70c/atdL27Qzgg0+jCwe4WWxirA7EHTj3jqlTFXt1uFLeeduOuitQIOeLAwDgYhISpCeesNNpHT9uR9zNmUNTHADg2dLTpbFj7Sjx8HA788nChTTFAQBZs3SpXUf8jjvsNOqHDkmLF9MUB+B7UlOl55+306afPm0b4u+/LwUEOF0Z4DaMGAfcKDYxVg2mN9CRU0e0qvUqRYRF2Dt279aZaZP0zYkftKTeNRpSqojCnC0VAIDM4uPtyIjevaWffpJ+/91OrZWbt48AAA+2d6+d5eTGG6XkZOn++6WHH3a6KgCAN4iOtjNklS4tPfusHcgyfLh0661OVwYA7jF4sP3uJzLSLps3aJBUqJDTVQFuxYhxwI3ik+N15NQR7Tq5S3Wm1NH+2P1SWprOPNdOUck/acFVp/VlmUTFJ8c7XSoAAP9ISJCefFLq08eOsIuLsyPvaIoDADyVMbaB0bGjdPPN0m+/SVOn0hQHAFxaero0bpxtCj37rHTsmL04ePFimuIAfNfatdKff0otW0qffGKb43ff7XRVgNvRGAfcqExoGa1qvUqRhSMzmuN7X+msOQk/KC4pXsv/F6lVrVepTGgZp0sFAMA6dco2xV98URo/XipfXnr9dabRAgB4rv37pUcekQ4elIKCpGLFpM8/tyP+AAC4mF9+kRo1stMJd+4s9esntWsnDRsmBQc7XR0AuEdCgvTyy3ba9J07pRkzWFccfoNhP4CbRYRFaFXrVaozpY4K/rpL25d9qAK5pQHPlcs8vToAAE47fdo2xZ97Tnr7bTsVbcOGTlcFAMD5paXZGU2++MJOmT5vnvTuu6wBCwC4tNOn7ZTBe/dKb71lP//s2iUtWiTly+d0dQDgXq+/LvXta893nTrZgRHMEgg/wYhxIAdEhEVo2sPTdCa3FJIiPf2QNPmxT2iKAwA8x+nTdl2pZs2kd96x60zRFAcAeKqtW6UHHpBSUqTixW1jY9EimuIAgEv78kubIXffLT3xhNS1q9Shg22O0xQH4OvWr5dOnpTuu8+OGn/2WalsWaerAnIMl4AAOWB/7H61ndVc734pdXhQOplPajm3JSPGAQCe4cwZ2xD/3/+kSZPsmqxlWOYDAOCBEhOlN9+06yG2aCFNmCC98YZ0111OVwYA8HTR0VKvXlLJktLEidJrr0lXX23XEg8Jcbo6AHC/5GTbDJ81y577EhPtkkSAH2HEOOBm+2P3q87k2nrpkz2afW+4Jvb+LtOa4/tj9ztdIgDAnyUmSk89JVWpIi1fLs2ZQ1McAOCZ1qyxU6ZXqCAVKiRt2GBHidMUBwBcTHq6nSa4TRupe3epalW7jni3bvZiK5riAPzFsGFS+/b2u6B33rFLSQB+hsY44EYH4g6ozpQ6enjxbkWXLaIBwzboroi7tKr1qkzN8QNxB5wuFQDgj5KSpObN7RS00dHS7NlSaKjTVQEAkFlMjNSxozRzptSjh/Txx9KTT0qjRjHlLQDg4s4uvZGcbGcZGTZM+v13ackS6bbbnK4OAHLOb79JW7ZIjz1mp0//4AMuDIJfojEOuFHBoIJqsjOP7ogrqJaTN2VMmx4RFpHRHC+Rv4QKBhV0uFIAgN9JSrIjxZOTpWuukcaNk/LkcboqAAAymzNHevRRO8VjUJC0cKE0f75Ut67TlQEAPNmZM3a64DfftKPFQ0Olp5+2t/XrZzMFAPxFerr04ot2lPibb0qPPy7dcIPTVQGOoDEOuFHYrr807PcI3bVwkyIKXZ3pvoiwCK1us1pLmy9VWEiYQxUCAPxScrJdUzw62v758stSQIDTVQEA8I+//rKjwrdskV5/XRoyRKpf317IVZALiwEAF/HVV1KjRlL16tKIEdILL0gHD9pR4jfd5HR1AJDzPvzQzp6xa5e0Z4/UurXTFQGOye10AYDPOn5c6txZeWbMUJniJc+7SZlQ1nAFAOSwlBR7ZfDevdJ770m1ajldEQAA/0hPt83vBQukwYOlqChp8mTp88+lwoWdrg4A4MkOH5ZeekkqUUKaN0+aPl0aM0YaOVKqUMHp6gDAGfv3S4sWSZMm2cER8+c7XRHgKEdHjK9Zs0aNGzdW6dKlFRAQoHnz5mXcl5KSot69e6ty5crKnz+/SpcurVatWungwYOZ9nHNNdcoICAg08/QoUMzbfPzzz+rZs2aCgkJUUREhIYPH54TTw/+LCXFTs/09ttSyfM3xQFcHjIDcIGUFOmhh+wVwlFRNMXhs8gMwEv9+qsdyZKeLg0YYJsbd95pv8SjKQ43ITMAH2CMNGWK1LKl1L279MwzUtOmNk8WL6YpDpchM+B1jLHnxbfflp5/3l4oxOxL8HOONsZPnTqlKlWqaPTo0efcd/r0af3000967bXX9NNPP2nOnDnavn27HnzwwXO2HThwoA4dOpTx06VLl4z74uLidN9996ls2bLauHGjRowYof79+2v8+PFufW7wcz16SK1aSbfc4nQlgM8gM4BsSk2VGja0U9MuXy6VL+90RYDbkBmAl0lKso3wfv2kUaOkffuk0aOlmTPtBV2AG5EZgJfbs0d65BHp6FFp7lw7ErJfPzv7SKdOUiAricJ1yAx4qtjEWB2IO3DuHVFRiq1QTmcWzJHq1pVuvjnHawM8jaNTqTds2FANGzY8731hYWFatmxZpts++OAD3XHHHdq3b5+uvvqf9ZoLFiyo8PDw8+5n+vTpSk5O1sSJExUUFKRKlSpp8+bNGjlypDp06OC6JwOcNW6cVLSo9NhjTlcC+BQyA8iG1FSpdm17pfD330v58jldEeBWZAbgRb77zq4h/txz0j332D+7dZMaN3a6MvgJMgPwUsbY7+AWL5beecdOo/7ggzZH+vWTAgKcrhA+iMyAJ4pNjFWD6Q105NQRrWq9ShFhEfaO48eV+OEHanvzLj23NEm3r9mpMGdLBTyCV10yFxsbq4CAABUqVCjT7UOHDlXRokV1yy23aMSIEUpNTc24b+3atapVq5aCgoIybqtfv762b9+ukydPnvc4SUlJiouLy/QDZMmqVdLq1faLHQCOIjOAv6WmSlWrSmFh0jff0BQHzoPMABwQG2tH8k2datcQ/+Yb6ZNP7FriNMXhwcgMwAMcOyY9+aR05ozNkXfesX9+9pn0+OM0xeExyAzkhPjkeB05dUS7Tu5SnSl1tD92vyTpVNeOanvjLnWaf0i9Himo+JQEZwsFPISjI8YvR2Jionr37q1mzZopNDQ04/auXbvq1ltvVZEiRfT999+rb9++OnTokEaOHClJio6OVrly5TLtq+Tfaz5HR0er8HnWKRsyZIgGDBjgxmcDn7Rrl/Tmm9K8eUzTBDiMzAD+lpAgValiR4tPnOh0NYBHIjMAByxZIr31lr2gODVVat1aevllqV49pysDLorMADzAihX2+7e33pL27pUefdSOEK9d2+nKgEzIDOSUMqFltKr1KtWZUiejOb6wSGet/etL3f9LnMY8VFrzn/9GZULLOF0q4BG8ojGekpKiJ554QsYYjR07NtN9PXr0yPj7TTfdpKCgID377LMaMmSIgoODr+h4ffv2zbTfuLg4RUREXFnx8A+xsVL79tKkSVL+/E5XA/g1MgP428GD0u23Sy1bSkOHOl0N4JHIDCCHxcRIPXvaWUymT5f695dCQqQFC6QCBZyuDrgoMgNwWFqabYAfOWKnUH/tNenaa+3FViEhTlcHZEJmIKdFhEVkNMcPR+9S9Ds99FMF6ZqChTWy/7p/plcH4PmN8bMhsnfvXq1YsSLT1VXnU61aNaWmpmrPnj0qX768wsPDdfjw4UzbnP39Qut4BAcHX3EIwQ+lpkpt2kiDB0v/WisGQM4jM4C/bdkiNWggde8u9erldDWARyIzgBy2dKk0fLg0aJB04oTUooU0YIBUo4bTlQGXRGYADouJkZ55xq4hfvXVUufO0ogRUuXKTlcGnIPMgFMiwiI07eFpGvHS3Zp5o9ToT6n4lwtoigP/4dHzPZ8NkT///FNff/21ihYtesnHbN68WYGBgSpRooQkqXr16lqzZo1SUlIytlm2bJnKly9/3mlHgMv24otS06ZStWpOVwL4NTID+NvixdL999vpaWmKA+dFZgA5KDnZjhJftMiuJT5unPTdd/Z3muLwAmQG4LDffpMeeURq1syuIZ6UZD/z0BSHByIz4KT9sfvVcm5LfXmt9OQ26dkHpJbzW2esOQ7AcnTEeEJCgnbs2JHx++7du7V582YVKVJEpUqV0mOPPaaffvpJixYtUlpamqKjoyVJRYoUUVBQkNauXav169erbt26KliwoNauXavu3burRYsWGSHx1FNPacCAAWrbtq169+6trVu3atSoUXrnnXccec7wMWPGSEWKSE8+6XQlgM8jM4AseO89+/PGG9LTTztdDeAYMgPwEHv2SM89J7VrJ6Wn29F+Q4dKVas6XRmQgcwAPNiCBdLo0VKVKrYpPmYMszXCUWQGPNX+2P0Za4zPWFlQ4YMGKv/x9zPWHF/VehUjx4GzjINWrlxpJJ3z07p1a7N79+7z3ifJrFy50hhjzMaNG021atVMWFiYCQkJMRUqVDCDBw82iYmJmY6zZcsWU6NGDRMcHGyuuuoqM3To0MuqMzY21kgysbGxrnrq8AVffWVMy5bGpKc7XQmQwZfPV2QGcBEpKcZ07mzMbbcZM2WK09XAi/jqOYvMADzAggXG3HefMd9+a8xjjxkzYIAxSUlOV4Vs8NVzFpkBeKC0NGMGDjTm/vuNqV3bmPnzna4Il8lXz1lkBjzR/tj9JnJUpFF/mW4ti5vYHp2MMcbsi9mXcXvkqEizP3a/w5UC55fT56wAY4zJdnfdx8XFxSksLEyxsbGXXBMEfuK33+y6rXPnSnnzOl0NkIHzlfP4b4AcFxcntW4tpabadcU7dXK6IngRzlnO4vWHTzLGrvv6xx9SxYrSihXS229L5cs7XRmyiXOWs3j94TcSEqQWLaRDh6SaNaX+/aUCBZyuCpeJc5azeP39S2xirBpMb6ACuw5owY//p7yLv5Ry28miz44kL5G/hJY2X6qwkDBniwXOI6fPWY5OpQ54pWPHbNPh009pigMAnLV3r9S2rXT99VKZMjTFAQDOSkqSOna0y03t2yfdeaedBjcw0OnKAADeYOdOqVEjKSxM+ugj6aabnK4IADxeWEiYlj48R8GPPKGQT6dlNMUlKSIsQqvbrFbBoII0xYG/8ekUuBxJSXZU3rvvSuHhTlcDAPBn69bZtVrr1LEjKF5+2emKAAD+7MgR6cEHpfh4O8pv6lS7tjhNcQBAVnz8sXTHHVKrVtLatTTFAeAyhPV+XSGvvC6VLn3OfWVCy9AUB/6FEeNAVhljR+I99xxvzgEAzoqKkmbOlNq3l776SpowQQoIcLoqAIC/+vlnewFxUJDUpYv0wANOVwQA8Bbx8dLDD9vR4j/8IEVGOl0RAHiXqVOlUqWk//3P6UoAr8Cl20BWjRghVaggNW7sdCUAAH9ljDRwoPTtt9Jrr0nTpkljx9IUBwA4Z/p0O+1ttWrS8uU0xQEAWWOMveC3QgU7wnHnTpriAHC5fv1VmjVL6tfP6UoAr8GIcSAr5s2zb9A//NDpSgAA/iox0c5acttt0rPPSs2a2Q8/wcFOVwYA8Efp6VLz5tKaNdLs2dJddzldEQDAW+zeLXXtKv3xhzR8uPTUU05XBADe59QpO1vTJ59IuXI5XQ3gNWiMA5eyaZOdovbzzxmRBwBwxuHD0tNPS507S/fcY6ca/OADqVgxpysDAPij33+X6teXbrzRNjeCgpyuCADgDZKTpZEjpS+/tBf+RkVJN9/sdFUA4J26dpVeftlOow4gy5hKHbiYQ4eknj2lKVP4sgcA4IwtW+zo8BEjpIYN7ajxrl2lihWdrgwA4G9SUqT+/aU6daRXXpEWL+ZzEgAga1askBo0kI4ckfLlk+bPpykOAFdq0iTp6qule+91uhLA6zBiHLiQ06elNm3s2q1FijhdDQDAH82da2ctmTXLjg4fMkS65RbbIAcAICd99529aDg2Vlq6lGYGACBr/vpL6tNHKlpUqlHDzoY1dy4XVgHAldq61S79OmeO05UAXonGOHA+6elS+/ZSr17S//2f09UAAPyNMdLgwdKBA/aDTlCQ/XP/fmn0aKerAwD4k+PHpb59pYMH7Qi/uXOl8HCnqwIAeLqUFOm996Tly6V+/exSUNddJw0YwFKFAHClEhKkF16QPv2UdcWBK8RU6sD59O8v1a7NVCQAgJx35ozUurUUFiaNGWOb4j/9JE2eLI0axZdIAICcYYzNnqZNpdBQO4vWokU0xQEAl7ZqlZ02vXhx6cMPpddes7Mydu7M5xkAuFLGSJ06Sa+/LpUs6XQ1gNeiMQ781/Tp9sqrDh2crgQA4CdiE2N1IO6AHY330ENSy5YZXxod/GOjUnt0s82JPHkcrhQA4Be2bZMeeEA6dszOoBUaKk2ZIuXN63RlAABPduiQvch3zhzp88+l66+Xnn7aLlPI4BMAyJ4JE+x789q1na4E8GpMpQ7829q19s37rFlOVwIA8BOxibFqML2BSv62XzN/KKuQjyZlLONxIPpP7WpSU91b/J/G58ulMIdrBQD4uNOnpUGDpD17pOHDpVdftSPGn3zS6coAAJ4sNdUu+bR0qTRkiHTzzfaCqoUL7RIcoaFOVwgA3u3nn+3sTawrDmQbI8aBs/bssVM7TZrE+hwAgBwTnxyv27/ZpTaL/lL1hn9pf0k7Gm9/zD79+MCtGnj7Gf1YMF7xyfEOVwoA8GmLFtlR4jVqSG+8IXXtatcWpykOALiYb7+106aHhkqLF0uVK0u9ekm//SZFRdEUB4Dsio+XunWTPvpICqSlB2QXI8YBSYqNldq1kz7+mDfsAICck56uMm+N15t5H9Btz67Un3G7VWdKHU17eJo2dHhA28MTtLtqpFa1XqUyoWWcrhYA4Iv275deekm69lrbHP/xR+m55+wSHhERTlcHAPBUhw9LffpI+fNLs2dLhQtLcXHSM89ITZrY5aEAANljjPT889KAAVLx4k5XA/gEGuNASopd/2jwYOmaa5yuBgDgL06dshdl1a2rggMHannsftWZUke7Tu7SqB53666T0ldP2aZ4RBiNCQCAi6WkSO+9J61caadNr1jRNsOXLLHT3hYo4HSFAABPlJoqffihvZjqzTel226zt+/ZYz/fDBokVa/uaIkA4DPGjZMqVZJq1nS6EsBnMO8C/JsxUpcu9irWO+5wuhoAgL/Yt0966CE7Iq9DB0lSRFiEpj08TQHpUtWD0ov3SdMenkZTHADgemvXSg0bSsWK2fVfb7jBjvr79Vdpxgya4gCA8/v+e5sfISH2QqqzTfEtW6S2baUJE2iKA4CrbNokLVtml6cA4DKMGId/e+stO2Xgo486XQkAwF+sXSu99po0frwUGZlx8/7Y/Wo5t6VMoNTrPntby7ktGTEOAHCdY8ekV16xf4+KkooWlU6fts2MevXsnwAA/NfBg9LLL9sLp2bOtPlx1tmZR2bNynw7AODKxcRIL75oz62sKw64FI1x+K/PP5d27pTGjnW6EgCAv5g6VVqwQJozRwoNzbh5/7+mUY8sHKlpD09Ty7kttevkLtWZUofmOAAge9LSpI8+svkzaJBUrZq9PTraLivVu7d0zz3O1ggA8DxJSdK770pr1thp02++OfP9s2fbps3nn0v58jlRIQD4HmPs7IJDh9oZngC4FJeawD9t2GCbE++/LwUEOF0NAMDXpaXZpsO2bXaE3r+a4gfiDmRqiq9qvUp3RdylVa1XKbJwZEZz/EDcAcfKBwB4sfXr7bS3AQHSF1/80xTfulV66ilp1Cia4gCAcy1ZYvOjbFm7nvh/m+JTp0pLl9olOGiKA4DrvPWWVLs2S78CbsKIcfifvXulvn3taIk8eZyuBgDg62Ji7NS0Dz0ktWx5zt0FgwqqRP4SkpRpZHhEWIRWtV6lOlPqqET+EioYVDDnagYAeL+jR+206QEB0qefZh5tcnbkX1SUVLy4czUCADzPjh1Snz5S+fJ2tqsCBc7dZvp0afVqOxsJU/wCgOusWSP98os0ZYrTlQA+i8Y4/EtsrG1OTJgghYU5XQ0AwNf9/rvUqZOd/ur228+7SVhImJY2X6r45HiVCS2T6b6IsAitbrNaBYMKKiyE3AIAZEFamjR+vDRvnp02/b8jTRYvtvfPmSPlz+9IiQAAD5SQYC+a2rFDGjZMuvba828XFSV99ZU0cSJNcQBwpUOHpAEDpPnzmeUWcCPevcB/pKTY9fPefFO65hqnqwEA+LrFi6UePexoigs0xc8KCwk7pyl+VpnQMjTFAQBZs26dnfY2MNBOgfvfpviMGdInn9j1YGmKAwAku5btp59KTZpItWrZdcMv1BT//HM7rfqECVKuXDlbJwD4stRUqX176YMPzj9TBwCXYcQ4/IMxUpcuUosW/6ypBwCAOxhjR4jv3i3NnSsFBztdEQDA1x09Kr38sm1SzJghFS167jbjx0s//CBNmybl5qsAAICkzZvtsht16khffCEFBV142/nzbWN86lRyBABc7dVXbe+iQgWnKwF8Hu9i4B/eflsqV0567DGnKwEA+LJTp6Rnn5XuvFMaN46prwAA7pWWZvNm/nzpjTcuPEPJlCnSpk22OU42AQCOH5dee01KTJQ+/lgqVeri2y9aZEeVf/IJTXEAcLUFC+z3SU2bOl0J4Bd4JwPfN2eO9Oef0ocfOl0JAMCX7d0rtWv3z4gLAADcae1aqV8/e/HvkiUXntL2s8+klSvtWrA0xQHAv6Wm2ouk5s+X+veXqle/9GOWLpUmT7aN8Tx53F0hAPiXnTulMWPseRlAjqAxDt/2ww92dMRnn/ElEADAfVavtiP1PvpIuuYap6sBAPiyI0fstOl58lx42vSzvvjCrhU7fbpddxwA4L/WrJEGDZKefPLiF1T927JltpE+Y8bFp1kHAFy+M2ek556TJkxgGT4gB9EYh+/au1fq08euf8QVrQAAdzBGGjtW+u47ad48KX9+pysCAPiq1FQ7C9bChRefNv2s1avtNOtRUUx7CwD+bM8eO6tVsWLSrFlS4cIZd8Umxio+OV5lQsuc87CjC6NUaPxU5Zn9OQ0bAHCHbt2kF1+Urr7a6UoAv5LlS8YPHjzozjoA14qNldq2teskFSrkdDWA3yEz4BeSkuyVvTExdq09muLAFSEzgCxYsUJq0MBmzRdfXLopvmGDNGyYHSlOMwM+hMwALkNCgvTqq1KPHvbPUaPOaYo3mN5AtSfX1v7Y/ZkeemTxbP3c9xn9r+FRxSoppysHXILMgEebPFkKD7fv8QHkqCw3xitVqqRPP/3UpQdfs2aNGjdurNKlSysgIEDz5s3LdL8xRq+//rpKlSqlvHnzql69evrzzz8zbXPixAk1b95coaGhKlSokNq2bauEhIRM2/z888+qWbOmQkJCFBERoeHDh7v0ecDDpKRIbdrYURTlyjldDeCXyAz4vOho6eGHpcaN7XS2LNcBXDEyA7iIvXulp56SFi+2M2E9/fSlp0T/5RfbAJkxg4u24HPIDCAL0tPtsoJNmkh33y3NmSNVqHDOZvHJ8Tpy6oh2ndylOlPqZDTHDy/9XNt6tdaDTU5rf+pxxSfH5/ATAFyDzIDH2rLFzjr4+utOVwL4pSw3xt988009++yzevzxx3XixAmXHPzUqVOqUqWKRo8efd77hw8frvfee08ffvih1q9fr/z586t+/fpKTEzM2KZ58+batm2bli1bpkWLFmnNmjXq0KFDxv1xcXG67777VLZsWW3cuFEjRoxQ//79NX78eJc8B3gYY6QXXpCaN5fuvNPpagC/RWbAp/34o82Zt9+WHnjA6WoAr0dmAOdx+rTUv7+dXvG112zmhIVd+nF//GEf8+mnWdse8DJkBnAJ338vNWxoZ7VautT+/QLKhJbRqtarFFk4MqM5/vO88fqtR0s1bnJG4SUjtar1qvNOsw54AzIDHik2Vure3c50myuX09UA/slchl27dpm6deuakiVLmgULFlzOQy9Jkpk7d27G7+np6SY8PNyMGDEi47aYmBgTHBxsZsyYYYwx5tdffzWSzA8//JCxzRdffGECAgLMX3/9ZYwxZsyYMaZw4cImKSkpY5vevXub8uXLZ7m22NhYI8nExsZe6dNDTnnrLWOGDHG6CsAxnnS+IjOc/28AN5g2zZiHHzbm5EmnKwFcwlPOWWQGmYG/pacbM2uWMXXrGjN/vv39X2LOxJj9sfvP+9CDW9eZlNo1jfn7/1HA1TzlnEVmkBk4j717jWnRwphOnYw5evSyHrovZp+JHBVp8r4sM+cGmYJ9ZCJHRZp9MfvcVCz8gaecs8gMMsOjpKcb8+STxqxd63QlgEfJ6XNWlkeMS1K5cuW0YsUKvfrqq3rkkUd000036dZbb8304yq7d+9WdHS06tWrl3FbWFiYqlWrprVr10qS1q5dq0KFCqlq1aoZ29SrV0+BgYFav359xja1atVSUFBQxjb169fX9u3bdfLkyfMeOykpSXFxcZl+4AXmzpW2b5d693a6EgAiM+BjUlOlnj3t9LSzZ0uFCjldEeBTyAxANmMefFDasUNassT+/V9LdVxsLdi//tioPQ/X0RMN4hVbhOnT4dvIDOBfTp2S+vWTunSx34d98IFUrNhl7SIiLELTHp6mM0HSI09K8SHStIenKSIswk1FAzmHzIBHGTlSuusuZroFHJb7ch+wd+9ezZkzR4ULF1aTJk2UO/dl7yJLoqOjJUklS5bMdHvJkiUz7ouOjlaJEiUy3Z87d24VKVIk0zbl/rPO9Nl9RkdHq3Dhwucce8iQIRowYIBrnghyxg8/SJMm2XX3WOcV8BhkBnzCyZN2TdcnnrDrvAJwCzIDfuvECbu+YHy8NHasVOb8U9b+dy3YVa1XKSIsQn/t+UV7GtdU2/sSlZIvTvHJ8QoLYRp1+DYyA34vPV2aMUOaONEuodG//xV/H7Y/dr9azm1pf/l7Fy3ntszIGcDbkRnwCN98I23aJE2b5nQlgN+7rBT46KOP9OKLL6pevXratm2bihcv7q66HNW3b1/16NEj4/e4uDhFRPBG0GPt3i316WOb4nnyOF0NgL+RGfAJW7dKL7wgDR8u3Xab09UAPovMgF9KS5PGj7czX73+ulSjxkU3P7sWbJ0pdTKa45/W+1BJjzZR17pnlPJ/rAUL/0BmwO+tX29Hif/vf9IXX0j/GlV6ufbH7s/IlcjCkZr28DS1nNvynIuwAG9FZsAjREfbC5jmzWNQH+ABstwYb9CggTZs2KAPPvhArVq1cmdNkqTw8HBJ0uHDh1WqVKmM2w8fPqybb745Y5sjR45kelxqaqpOnDiR8fjw8HAdPnw40zZnfz+7zX8FBwcrODjYJc8DbnbypNSunb1ClmltAY9BZsAnzJplr+SdOVPy0Q/PgCcgM+CX1qyRBg2ys5F88YWUK1eWHhYRFpHRHD90eJeOPnSf3qwlxVeMpHkBv0BmwK/99Zf0yitSSIg0dar0n1Gnl+tA3IFMTfGzOfLfi7BWt1nNRVfwSmQGPEJqqtShg/Tee1LBgk5XA0BSltcYT0tL088//5wjISLZ9T/Cw8O1fPnyjNvi4uK0fv16Va9eXZJUvXp1xcTEaOPGjRnbrFixQunp6apWrVrGNmvWrFFKSkrGNsuWLVP58uXPO+0IvEhSktSypTRihFS2rNPVAPgXMgNeLTVV6tXLjsSYO5emOOBmZAb8yv79UosWdrarWbOk9u2z3BQ/6+xasMVOSyPultZFsBYs/AeZAb905oy9mKpjR6l7d+nDD7PdFJekgkEFVSJ/iUxNcemfi7AiC0eqRP4SKhhEIwfeicyAR3jlFalZM6lSJacrAXCWcVB8fLzZtGmT2bRpk5FkRo4caTZt2mT27t1rjDFm6NChplChQmb+/Pnm559/Nk2aNDHlypUzZ86cydhHgwYNzC233GLWr19vvv32W3P99debZs2aZdwfExNjSpYsaVq2bGm2bt1qZs6cafLly2fGjRuX5TpjY2ONJBMbG+u6J4/sSUszpmVLYxYvdroSwKP48vmKzECOOHrUmAcfNGb6dKcrAXKEr56zyAx4nDNnjBk0yJjGjY355Zds7WpfzD4TOSrSqL8yfiJHRZp9MftcVCxwfr56ziIz4LHS042ZMcOYunWNmTPH/u5iMWdizP7Y/ee9b3/sfhNzJsblx4R/8NVzFpmBy/LZZ8a88ILTVQAeL6fPWY42xleuXGkknfPTunVrY4wx6enp5rXXXjMlS5Y0wcHB5t577zXbt2/PtI/jx4+bZs2amQIFCpjQ0FDz9NNPm/j4+EzbbNmyxdSoUcMEBwebq666ygwdOvSy6iRIPNDLLxszdqzTVQAex5fPV2QG3O6nn+yXTps3O10JkGN89ZxFZsBjpKcbM2uWzZfPPst2U+PfTfHIUZHmu33fZfqd5jjcyVfPWWQGPNL33xvToIExQ4cak5jodDXAZfPVcxaZgSz77TdjGjY0JjnZ6UoAj5fT56wAY4xx9Sh0XxMXF6ewsDDFxsYqNDTU6XLw0UfSjh3SsGFOVwJ4HM5XzuO/gZeaNk2aM0eaMEEqUsTpaoAcwznLWbz+Pu7HH6XXX5dq1ZK6dbNrwmbDgbgDqj259jlrwe6P3Z9pjVjWgoW7cM5yFq+/n9i9W3rtNSksTOrfn2Wd4LU4ZzmL199h8fHSQw9Jn3wi/Wu9eQDnl9PnrNxuPwLgSl98Ia1eLU2d6nQlAABfkJIivfSSlDev9Nlnl73OKwAA5zh4UHr1VZspEydK4eEu2e3ZtWAlnXct2DpT6rAWLAB4q5gYafBgadcuu554hQpOVwQAuBLGSM8+Kw0YQFMc8FA0xuE9Nm+WPvhA+vxzKTDQ6WoAAN7u8GHpmWekp5+WHnvM6WoAAN7uzBnp7beldetsU+OWW1y6+7CQMC1tvlTxyfHnjAiPCIvQ6jarVTCooMJCwlx6XACAG6WkSOPHS/PmSX36SPfe63RFAIDsGDlSqlZNqlHD6UoAXADdRXiH/fulHj3sSPFsTkEIAIA2bJCeekoaPpymOAAge4yRZs6UGjWSKlaUFi50eVP8rLCQsAtOk14mtAxNcQDwFsZIixZJDRrY2auWLqUpDgDebtUqacsWqWtXpysBcBGMGIfni421o/k+/lgqWtTpagAA3m7CBPvF0+efS4UKOV0NAMCbbdgg9esn1a1rl30KDna6IgCAp9u8WXr9dXsR1fz5UoECTlcEAMiuAwekN96w5/WAAKerAXARNMbh2VJSpFatpDfflCIjna4GAODNkpKk7t2lEiWkqCiW5QAAXLm//pJeecXOZjV5slSypNMVAQA83V9/2Ya4JI0dK111lbP1AABcIzlZat/entvz53e6GgCXQGMcnssYqWNHO1q8WjWnqwEAeLODB6W2bW2uPPig09UAALzV6dPSW29JP/xgR4RUqeJ0RQAAT3fqlDRihLRxozRwoNuW2wAAOKRHD+m556Trr3e6EgBZwFApeK5Bg6TKlaWHHnK6EgCAN/v2W6llS+ndd2mKAwCujDHSp59KDzxgm+ELFtAUBwBcXFqaNGmS1LixdNttNjtoigOAb5k6VQoLk5o0cboSAFnEiHF4pqlTpZMn/5liCgCAy2WMncZqzRpp3jypYEGnKwIAeKN166T+/aX//Y91xAEAWbN8uTR0qG2UfPmllCeP0xUBAFxt82Zpzhzp88+drgTAZaAxDs+zYoW0ZIk0fbpbdp+anq5jZ5J18kyKTiam6HRqmtKNUa6AAOXPk0uFQ4JUOCSPiuYNUq7AALfUAABws8REqXNn6dprpRkzpIArO5+TGQDgx/bvl1591a4TOHWqVKLERTcnMwAA+v13mx3lykmzZ0uFCp13MzIDALzcyZN2CvVZs6Rcudx6KDIDcC0a4/As27ZJw4fbK61cHCgJyanaGXNKe2LOKM0YBUgy/9kmNilVBxOSJEl5AgN0beH8Klcon/Lmdm+4AQBcaN8+qX17qXt3qUGDK9oFmQEAfuzsWrCbNtl1xCtXvujmZAYAQEePSgMGSDEx9nutyMjzbkZmAIAPSE+X2rWz5/tixdx2GDIDcA8a4/Achw5JXbpIM2dK+fK5bLdp6UbbjsVrx8lTmcLjvyHyXynpRtuPJ2j78QRVKFZQ/1ckvwKvcMQhACCHrFwpvfmmNG6cHS1+mcgMAPBjZ9cRnzDBXlzVr99FZxwhMwAASkyU3nvPzn74+uvSXXeddzMyAwB8yKBB0v33S1WrumX3ZAbgXjTG4RkSEqTWre1asJeYovByxCSmaP3BkzqVkibp0uHxX2e3//VYvA4lJOr2UoVUIIh/NgDgkUaNkn78UZo/3057e5nIDADwY2fOSM2bS9WqSUuXSkFBF92czAAA6OuvpcGDpWeflb744oIXU5EZAOBDliyxA/z69XPL7skMwP0CnS4AUGqqbYq/9ppUvrzLdnv0dJJW7Tum03+HSHbFJKZo5d5jik1Kccn+AAAu9NprUlycXQP2CpriZAYA+LnZs6VKlaTevS/ZFCczAAA6etQ2xRcskJ588oJNcTIDAHzIrl12UMa777pl92QGkDNojMNZxkhdu0pPPCHVrOmy3Z44k6zvDpxQurn8q6ouxEhKTTf6Zt9xnUpOddFeAQDZdvq0tHq1bY5fwRRRZAYA+LkZM6R166RXXrnkpmQGAEB//CE1bWpHCxYocMHNyAwA8CGnT9sZQsaPl0JCXL57MgPIOTTG4ay335bKlrVX17pIclq6vv/rpIyrEuRfjOxaHd/9dULp7jgAAODytWp1xVNYkRkA4Oc2bbJfbvXte8kvuMgMAIBSU6Vhw2xu1K59wc3IDADwIcZInTvb2aXKlnX57skMIGfRGIdzFi6Utm+XevVy6W5/ORKnlLR0l11Z9V9GUkJymrYfT3DTEQAAWbZjh3TqlHTvvVf0cDIDAPzcsGHS5MlSRMQlNyUzAAB69VXpzjsv+fmDzAAAH/Lhh9L110v16rll92QGkLNojMM5778vffDBFU17eyFHTydpb9wZt4XIv/1+PEHxSUxBAgCOeuYZ6aOPruihZAYA+LnDh6Vjx7LUFCczAAA6c0b6/nvp8ccv+l0WmQEAPmTdOmnlSjta3A3IDCDn0RiHM8aPl2rUkIKDXbrb348nyHVt9kv78yRXWQGAYzZskK69VipT5ooeTmYAgJ+LipIeekgKvPTHYjIDAKBffpHKl5cKFbroZmQGAPiIo0ell1+2AzKy8JnhSpAZQM6jMY6cl54uzZplp59yoVPJqTp6OjlHrq6S7BQk++LOKCU9PYeOCADIZMECqVmzK3oomQEAfi493ebIo49eclMyAwAgSZo0ya4xexFkBgD4CGOkDh2kd9+VwsLccggyA3AGjXHkvIkT7RdQLr7Kak/cmRy9ukqS0o10IC4xh48KAJAx0rffXvH6TmQGAPi5AwekokWlUqUuuSmZAQBQYqL0889SxYoX3YzMAAAfsXatdM010k03ue0QZAbgDBrjyFmpqdLMmVLTpi7f9dFTSTl2ddVZAZKOnUnO4aMCADRvnlSnzhVfZEVmAICf+/BDqV27LG1KZgAA9OmnUuPGUp48F92MzAAAH2CM9MYb0muvufUwZAbgDBrjyFmrV0vVq0uFC7t0t8YYxSaluHSfWTqupJMECQDkvLlzpQcfvKKHkhkA4OfS0qQVK6Tbb7/kpmQGAEDG2M8fl1h+g8wAAB/x2WfSXXdJRYq47RBkBuCc3E4XAD8zcqQ0fbrLd3sqJU1pOX151d8SUtKUmm6UOzCnJz4BAD914oR05Ih0661X9HAyAwD83MqVNkMKFbrkpmQGAEDHjknp6dL11190MzIDAHzA6dPS2LHSggVuPQyZATiHEePIOUuWSJUrZ+kLqMuVlJru8n1ejuQ0Z48PAH4lKkpq3fqKH05mAICfmz1batUqS5uSGQAAffyx1L79JTcjMwDAB4waJT37rFSggFsPQ2YAzqExjpyRmmpDpUcPt+w+PcdX48jMGGePDwB+JSpKql//ih9OZgCAH0tNlX7/XapWLUubkxkAAC1cKNWqdcnNyAwA8HL79knr10tPPOH2Q5EZgHNojCNnzJ8v3XOPVKKEW3YfGODstB9OHx8A/MbXX0s33ZStdZ6cPmc7fXwA8GuTJkmPPSZl8Vzs9Dnb6eMDgN9buVK68cYsff5w+pzt9PEBwOu9/LL05ptZ/qyQHU6fs50+PuAkGuNwv6QkafRoqWtXtx0iJJez/ysHOXx8APALxkgTJkhdumRrN2QGAPixefOkBg2yvDmZAQB+bs4cqWnTLG1KZgCAF1u5UipeXKpUKUcOR2YAzuH/frjf669L3bpJefO67RD58uRSLoeucioYlFu5ArnCCgDcbt8+KTBQuv76bO2GzAAAP/Xbb1KePJeVI2QGAPix5GTp55/tDIhZQGYAgJc6dkwaNEgaMCDHDklmAM7x+Mb4Nddco4CAgHN+OnXqJEmqU6fOOfc999xzmfaxb98+NWrUSPny5VOJEiX00ksvKTU11Ymn438OHZK2b5cefNCthwkICFChkNxuPcZ5jyupcEieHD8ugPMjM3xcv37Siy9mezdkBgCJzPBLM2ZI//lveClkBgCJzPBbUVHSQw9leXMyA4BEZnid9HSpQwdp5EgpNDTHDktmAM7J+X95l+mHH35QWlpaxu9bt27V//73Pz3++OMZt7Vv314DBw7M+D1fvnwZf09LS1OjRo0UHh6u77//XocOHVKrVq2UJ08eDR48OGeehD974w3plVdy5FAl8gXr+JmUHDnWWUZS8XxBOXpMABdGZviwbdukuDjp1ltdsjsyAwCZ4WfS0+30iK+/ftkPJTMAkBl+asYMadSoy3oImQGAzPAyw4ZJ9etLN9+c44cmMwBneHxjvHjx4pl+Hzp0qK699lrVrl0747Z8+fIpPDz8vI//6quv9Ouvv+rrr79WyZIldfPNN2vQoEHq3bu3+vfvr6AgTgJus2WLbWLcfnuOHK5sWD79djwhR451Vq6AAF1V0H1TxAO4PGSGD3vnHalvX5ftjswAQGb4mffek5o1k3Jf/kdgMgMAmeGHNm+W8uW77GWcyAwAZIYXWbXKLrc0ZYojhyczAGd4/FTq/5acnKxPPvlEzzzzjAL+tf7C9OnTVaxYMd14443q27evTp8+nXHf2rVrVblyZZUsWTLjtvr16ysuLk7btm0773GSkpIUFxeX6QeXKTlZ6tlTevvtHDtkvjy5FJ4/WDm1OkaApLJheZWb9TgAj0Rm+JBt2+xIPxdeaEVmAPg3MsPHnT4tzZsnPfHEFT2czADwb2SGn5g6Vfp72uPLQWYA+Dcyw4MdOGBnux07VnJorW8yA3CGx48Y/7d58+YpJiZGbdq0ybjtqaeeUtmyZVW6dGn9/PPP6t27t7Zv3645c+ZIkqKjozOFiKSM36Ojo897nCFDhmjAgAHueRL+wBipe3f7AaJEiRw99A1FCyj6VFKOHCsgQLq+cP4cORaAy0dm+JBevaQxY1y+WzIDwFlkho9buFCqW1cqVuyKd0FmADiLzPADiYnSTz9d8WAPMgPAWWSGh0pKktq1s03x/M6eQ8kMIOd5VWN8woQJatiwoUqXLp1xW4cOHTL+XrlyZZUqVUr33nuvdu7cqWuvvfaKjtO3b1/16NEj4/e4uDhFRERceeH+5r33pNKlpYceyvFDF8kbpGsL5dPOmNOX3jibKhUrqPxBXvVPCPArZIaPOHRIKlpUKlvW5bsmMwCcRWb4sJQU6cMPpS+/zNZuyAwAZ5EZfuC116QXX7ziEYRkBoCzyAwPdXZQ32Uul+EOZAaQ87xmKvW9e/fq66+/Vrt27S66XbVq1SRJO3bskCSFh4fr8OHDmbY5+/uF1vEIDg5WaGhoph9k0eLFdh2ml192rIRKxQsqb+5At01BEiCpUHBuXcvVVYDHIjN8yKJFUqNGbts9mQGAzPBxo0dLzZtLLliLkcwAQGb4gX37pE2bpAYNsrUbMgMAmeGhJk60M0k1bux0JRnIDCBneU1jfNKkSSpRooQaXeLL8c2bN0uSSpUqJUmqXr26fvnlFx05ciRjm2XLlik0NFQVK1Z0W71+6eef7RdPH37o2LockpQ7MFB3lymiXIEBLg+TAEnBuQJVvUwRBTr4HAFcHJnhQ+bOdeuHFTIDAJnhw6KjpQULpKeecsnuyAwAZIYfWL5catlSypMnW7shMwCQGR7oxx/tAIx+/ZyuJBMyA8hZXtEYT09P16RJk9S6dWvlzv3PVA87d+7UoEGDtHHjRu3Zs0cLFixQq1atVKtWLd10002SpPvuu08VK1ZUy5YttWXLFn355Zd69dVX1alTJwUHBzv1lHxPdLTUrZs0darkAa9raHAe1YxwbZicDZFaVxdV3ty5XLRXAK5GZviQX36RypWT8uVz62HIDMB/kRk+bt06qWlTl+YImQH4LzLDD/zxhxQVJT35pEt2R2YA/ovM8EDHjkm9ekkffyzl8rzzJ5kB5ByvaIx//fXX2rdvn5555plMtwcFBenrr7/WfffdpxtuuEEvvviiHn30US1cuDBjm1y5cmnRokXKlSuXqlevrhYtWqhVq1YaOHBgTj8N33XmjNS6tTRmjJ2GxEMUDglS3bLFFBbsmnUziucLUt1riqkA63AAHo3M8CFDhth1n3IAmQH4JzLDx61fL1Wt6vLdkhmAfyIz/MB330nt2kkhIS7bJZkB+Ccyw8OkpUlt20ojR0pFijhdzQWRGUDOCDDGGKeL8HRxcXEKCwtTbGws63P8V3q6nWLq6aelevWcrua80o3R9uMJ+v14gq7kf/ZcAVLlEqEqF5ZPAUw3Ag/H+cp5/DdwkR9/lCZMkMaOzdHDkhnwN5yznMXr72b33y8tXOi2ESFkBvwN5yxn8frngM6dpR49pMhIl++azIC/4ZzlLF7//+jbV6pUSWrRwulKsoTMgL/J6XMWl4oge15/Xapd22Ob4pIUGBCgCsUKqlyhfNodc1o7Y04pOc1GSoCUKVz+/Xve3IG6vnABlQ3Lqzy5vGJyBQDwDYmJUp8+0qef5vihyQwA8BFHj0qhoW6dJpHMAAAfs2uXXcrJDcgMAHDInDnSqVNe0xSXyAzA3WiM48pNnWqnUe/QwelKsiQkdy5VKFZQ5YsWUExiik7+/XMmNU3pxihXQIDy5cmlwiFBKhySR2HBubmiCgCc0KuX9MILUokSjpVAZgCAl4uKkp54IkcORWYAgA84elQqVEhy8/mZzACAHPT773ZN8XnznK7kipAZgHvQGMeV+eYbOy3hzJlOV3LZAgMCVCRvkIrkDXK6FADAf51d76lxY6crkURmAIBXMkaaP19atChHD0tmAIAXW7LELsGRQ8gMAHCz+HipUyfpk0+kIO8+15IZgGvRGMfl27lTGjhQmjvXrVMTAgD8iDHSkCFSTIw0bJjT1QAAvNmSJVLNmlJwsNOVAAC8xWefSdOnO10FAMAVjLGz3A4YIJUq5XQ1ADwMjXFcnpgYGyqTJ0sFCjhdDQDAFxgj9e4tFS5sm+JM+wQAuFI7dkjvvGMv4gUAICsWLZJuuEEKDXW6EgCAK4wYId11l1SjhtOVAPBANMaRdSkpUuvWtmkREeF0NQAAX5CWJj3/vHTzzVLHjk5XAwDwZgcP2ot4p06VChZ0uhoAgDf46Sdp7Fjp88+drgQA4ArLl0u//ipNmuR0JQA8FI1xZI0xUteutjFetarT1QAAfEFysvT001KjRtJTTzldDQDAm+3dazNl3DipTBmnqwEAeIM//5R69rTTqIeEOF0NACC79u6Vhg6V5s9nNkIAFxTodAHwEu++K5UtKz3yiNOVAAB8wenTUtOm9oemOAAgO779VnrmGTsq5Prrna4GAOANtm2TnntO+uQTqUgRp6sBAGRXYqLUvr29UDZfPqerAeDBGDGOS1u4UNq6Vfr4Y6crAQD4gpgY2wzv1UuqU8fpagAA3soY6f33pXXrpHnzmD4dAJA1CxZIY8ZIM2dKxYs7XQ0AwBW6dpW6d5ciI52uBICHozGOi9u82V5lNWcO048AALLvyBGpeXNpyBCW5gAAXLlTp6Tnn5cqV5amT+ezCgDg0lJTpVdesaMKFyyQgoKcrggA4AoffSRFREgNGzpdCQAvQGMcF3bokNSjhzR7Nh8WAADZt2+fXf/1/felihWdrgYA4K3+/FPq2NE2N+rWdboaAIA3OHjQTrHbqpX05JNOVwMAcJUNG6SvvpKiopyuBICXoDGO8zt9WmrTRho7Vipa1OlqAADe7o8/7Bp+EyZI5co5XQ0AwFstWGA/o0yeLJUp43Q1AABvsHy5nbFq9GipfHmnqwEAuMqRI1KfPna228BAp6sB4CVojONc6elSu3Y2VPjAAADIrk2bpJdeslPdlirldDUAAG+Ulib16yfFxNj1xIODna4IAODp0tOlwYOlXbuk+fOl/PmdrggA4CqpqVLbttKoUVKhQk5XA8CLcBkNzvXyy9K99zItIQAg+777TurbV5o1i6Y4AODKHD8uPfqo9H//J33wAU1xAMClHT8uPfaYVLKknbWKpjgA+Ja+faXmzaXKlZ2uBICXYcQ4MvvoIzvtSNu2TlcCAPB2S5dKY8ZIn30mFSjgdDUAAG+0caOddWTkSOnmm52uBgDgDdavtw2Tt9+WbrnF6WoAAK42Y4adFaRpU6crAeCFaIzjH199Ja1eLU2d6nQlAABvN3u2bYjPmiWFhDhdDQDAG02YIC1ZIn3+uVS4sNPVAAA8nTHS++9L339v15tlal0A8D2bN9vG+Jw5TlcCwEvRGIe1dav0zjvS3Ll2xDgAAFfq44/tl1HTp0u5easBALhMiYlSt252CY7Zs/l8AgC4tLg4qWNH6bbbbMMkIMDpigAArnb8uNSjhx2EwfdNAK4QZw9Ihw5JXbtKUVGM6gMAXDljpGHDpKNHbXOcRgYA4HLt2ye1by+98IJ0//1OVwMA8Aa//GJzY+BAqUYNp6sBALhDaqr09NN2iaVixZyuBoAXozHu706dklq3lsaOlYoXd7oaAIC3Sk+XevWSihaV3nqLERoAgMu3bJk0fLg0bpwUGel0NQAAbzB5sp39cOZMqUQJp6sBALhLnz5Ss2bSzTc7XQkAL0dj3J+lpdmrrF55RSpf3ulqAADeKjVVevZZqVo1qUMHp6sBAHib9HTpjTek/fulBQukvHmdrggA4OlOnZK6dJGuvtquM5srl9MVAQDc5dNP7Z/NmjlbBwCfwByn/uyll6QHH5Rq13a6EgCAtzpzRnrqKalhQ5riAIDLd+yY9OijUpky0kcf0RQHAFza1q32+6wWLaT+/WmKA4Av27zZLgE7dKjTlQDwEYwY91cffCCFhdkPEQAAXInYWKl5c6lbN6lePaerAQB4m/Xrpb597TqBTIkIALgUY6SJE+3sIp9+KpUs6XRFAAB3OnZM6tFDmj1byk0rC4BrcDbxR4sWSRs32g8TAABcicOH7cVVb74p3XGH09UAALyJMfZC3e++s+vChoU5XREAwNMlJEidO0vXXcfU6QDgD1JTpWeekd55Rypa1OlqAPgQplL3N5s2SR9+aH8CApyuBgDgjfbsses6vfceTXEAwOWJj5datpSSk6UZM2iKAwAu7eefpSZNpKefll59laY4APiDPn3s0n1VqjhdCQAfw4hxf3LggNSzp516JDjY6WoAAN5o61bphRekyZOlq692uhoAgDfZtk3q2tWuB1uzptPVAAA8nTHSxx9Lixfbi6lKlHC6IgBATvj0Uzuor2lTpysB4INojPuL+HipTRtp/HipSBGnqwEAeKO1a20zY+ZMqXhxp6sBAHiT6dNtfrAmLAAgK+LjpU6dpBtusFOnBzLpJQD4hU2bpKgo6fPPna4EgI+iMe4PUlOl1q2lgQOla691uhoAgDdaulQaM0b67DOpYEGnqwEAeIvERKlHD6lwYbueeG4+ggIALmHLFql7d2nAAGYYAQB/cuyY9OKLdsZbPjcAcBPOLr7OGDvlbdOm0l13OV0NAMAbzZghzZ8vzZolhYQ4XQ0AwFvs2SM9+6ydPr1RI6erAQB4OmOkceOkr76yowWZpQoA/EdqqvTMM9I770hFizpdDQAf5tHzEPXv318BAQGZfm644YaM+xMTE9WpUycVLVpUBQoU0KOPPqrDhw9n2se+ffvUqFEj5cuXTyVKlNBLL72k1NTUnH4qzhk5UoqIkJ54wulKAMCtyAw3GT1aWrXKToFLUxyAjyAzcsDixbYpPm4cTXEAXo3MyCFxcVLLllJsrJ2liqY4AC9EZmRDnz7SU09JVao4XQkAH+fxI8YrVaqkr7/+OuP33P+aQqN79+5avHixZs+erbCwMHXu3FmPPPKIvvvuO0lSWlqaGjVqpPDwcH3//fc6dOiQWrVqpTx58mjw4ME5/lxy3Jw50h9/SB9+6HQlAJAjyAwXMsYuwZGYaHMkIMDpigDApcgMN0lNlfr1k2Ji7GwjXFQFwAeQGW62aZOdOnfQIOnuu52uBgCyhcy4Ap9+KgUG2llvAcDdjAfr16+fqVKlynnvi4mJMXny5DGzZ8/OuO23334zkszatWuNMcYsWbLEBAYGmujo6Ixtxo4da0JDQ01SUlKW64iNjTWSTGxs7JU9ESesX2/Mgw8ak5zsdCUAcpBXnq9chMxwobQ0Y7p0Meatt5yuBICb+cQ56wqQGW4SHW1M48bGTJvmdCUA3MDnzllZRGa4UXq6MR98YMyjjxpz7JjT1QBwIZ88Z2UBmXEFfvrJmCZNjElNdboSAA7J6XOWR0+lLkl//vmnSpcurcjISDVv3lz79u2TJG3cuFEpKSmqV69exrY33HCDrr76aq1du1aStHbtWlWuXFklS5bM2KZ+/fqKi4vTtm3bLnjMpKQkxcXFZfrxKnv2SC+/LE2dKuXJk6WHGGN0OiVNf8UnaufJU9px4pR2nTyl6IREJaWmubdeAHARMsMFkpOlp5+Wbr3Vjto4DzIDgC8gM1zsm2/s1IdDhkgtWmTcTGYA8AVkhhvExkrNm0tnzkizZklFi5IZAHwCmXEZjh2z3z1NnCjlynVZDyUzAFwpj55KvVq1apo8ebLKly+vQ4cOacCAAapZs6a2bt2q6OhoBQUFqVChQpkeU7JkSUVHR0uSoqOjM4XI2fvP3nchQ4YM0YABA1z7ZHJKTIzUtq0Nk7CwS2+emKIdfwdGcrq54HYhuQNVpmBeXVson/IHefT/NgD8FJnhAqdO2XX9WreWmjQ5524yA4CvIDNcyBjp7belzZulefOkggUlkRkAfAeZ4QY//ij16iW9+aZUvfrfmRFHZgDwemTGZUhNlZ55Rnr3XalIkSw/jM8ZALLLo88IDRs2zPj7TTfdpGrVqqls2bKaNWuW8ubN67bj9u3bVz169Mj4PS4uThEREW47nsskJ9tmxtChUtmyF930xJlk/XwkTicSUxQg6cIRYiWmptsrr06eUnj+YN1UIlQFCBQAHoTMyKYTJ+yIjT59pNq1M99FZgDwMWSGi8TGSs8+K9WoIU2bJgUEkBkAfA6Z4ULGSB98IK1ZI332mU7kLaCf9x4jMwD4DDLjMvTubWeauummLG3O5wwAruLxU6n/W6FChfR///d/2rFjh8LDw5WcnKyYmJhM2xw+fFjh4eGSpPDwcB0+fPic+8/edyHBwcEKDQ3N9OPxjJE6dbLT395++wU3SzdGvx6L16p9x3UiMcU+NKuH+PvPw6eS9PWeo9odc1rGZPXRAJCzyIzLcPCg9MQT0uDBmZriZAYAf0FmXIHNm6WHH5a6dZM6d1a6RGYA8AtkxhWKiZGaNZNSU5UeFaVf0/OQGQB8HplxAZ98IuXObb+LugS+mwLgal7VGE9ISNDOnTtVqlQp3XbbbcqTJ4+WL1+ecf/27du1b98+Va9eXZJUvXp1/fLLLzpy5EjGNsuWLVNoaKgqVqyY4/W71ZAhUsWK0kMPXXCTlLR0rdl3XL8fT8jWoYykdCNtOhyrHw7FKJ0wAeCByIws2rHDXqE7dqx0yy0ZN5MZAPwJmXGZJk6UBgywa8LeeSeZAcCvkBlXYMMG6ZFHpO7dldL1Ba3Zf4LMAOAXyIzz+Okn6fPP7eCMS+BzBgB38Oi5I3r27KnGjRurbNmyOnjwoPr166dcuXKpWbNmCgsLU9u2bdWjRw8VKVJEoaGh6tKli6pXr64777xTknTfffepYsWKatmypYYPH67o6Gi9+uqr6tSpk4KDgx1+di40c+b/s3fncTbVfxzH3zOYDTNjmxnLEJEQWiyJUPlZ06oFiRIlWogklVIhpEVos1UqkS2Vkiwtosheyk7MjDB3bDNm+f7+OLm5zQx3zMw9d3k9H495xLnn3vu5R3Pe997P+X6/1mi/8eNz3SUjK0vf7T0kR1pGgT71vqOpyjLJalwhWkFBQQX62ACQF2TGefj1V2nQIGv624oVnZvJDAD+jsw4TydPSg8/LMXHS7NnS0WKkBkA/B6ZkQ/GSK+9Jq1cKX36qTKiovTdHjIDgP8iM87h4EFp4EDnZ4mz4XMGgMLi1Y3xffv2qXPnzjp06JDKlSunZs2a6aefflK5cuUkSa+88oqCg4N16623Ki0tTW3atNHEiROd9y9SpIgWLlyoPn36qEmTJipevLi6d++u4cOH2/WSCt4PP0izZkkzZ0q5nMiNMVr1V7IcaRluTzOSF/uPpWrjwRTVi4kqhEcHAPeQGXm0YoU128gnn0ilSzs3kxkAAgGZcR62bZP69LEuqGrdWhKZASAwkBnn6cgR6f77pWbNpI8/lpG0at8RMgOAXyMzziIjQ+rZ07pg6ozvoXLC5wwAhSnIsKjCOaWkpCgqKkoOh8O71uc4/eXUnDlSyZK57rYr+YTWJjoKvZzm8WVUNiKk0J8HQO689nwVQHzi32DBAmnaNGukePHiLjeRGUBg8Ylzlh/zmeP/zTfS2LHSO+9Yo8X/QWYAgcVnzll+yqeO/88/S4MHSy+9JDVsKInMAAKNT52z/JBXHv8BA6Qrr3RrXXEyAwgsnj5n+dQa4zjDoUNS795WY+MsTfGTGZnakJRS6OUESVqTkKzMLK6zAACv9uGH1kwjH3+crSlOZgAAsjl40Fr/b+5cl6Y4mQEAyNEXX0gvvmgN4vinKU5mAECAe/99KSTEraY4mQGgsNEY90VpadLdd0vjxrmsCZuT3w8dU6YHJgUwko6nZ2qn40ShPxcA4DwtXiwtWmRdVBWS/YpYMgMA4MIYa6T4bbdJ4eEuN5EZAIBsjJHefluaMEGKjnZuJjMAIICtXWtdLPXii27tTmYAKGw0xn2NMdKDD0p9+0qXXnrWXTOysrTHcaJQ1uHIzY4jx8Xs/ADgpb7+WrrnHqlIkWw3kRkAgGz+/FPav99avukMZAYAIEfDhknXXusyiIPMAIAAlpwsDRwoTZ6c43dR/0VmAPAEGuO+5oMPpGrVpPbtz7nrvpRUZXr4nH4sPVOHU9M9+6QAgHP75hvp8GGpZcscbyYzAADZfPihdNNN2TaTGQCAbDIypOXLpe7dXTaTGQAQwJ58UnrhBal0abd2JzMAeAKNcV+zcWOuTY3/+utYauHWkoMgSfuPev55AQDn8PXXVnMjKCjHm8kMAICLv/+Wli6VWrTIdhOZAQDI5o8/rIEcUVEum8kMAAhgCQlS1apu705mAPAEGuO+ZMcO6bffpCuvdGv3IzZc6WRsel4AwFmsWyclJkrXX5/rLmQGAMDFxInSoEFS2bLZbiIzAAAu0tOlfv2kp57KdhOZAQAB6vvvpQoVpPLl3b4LmQHAE2iM+5I9e6R69dxajyM1I1OnMrM8UFR2yWnprMsBAN4kMVGKj891tDiZAQDIZs8eqVKlbJvJDABANunpUnCwdOGFLpvJDAAIYAkJ0gUXuL07mQHAU2iM+5JRo6Q+fdza9dipzEIuJncZWUanPL0YCAAgd9u2SZdckuvNZAYAIJt9+6TKlbNtJjMAANn8/rtUt262zWQGAASw8eOl3r3d3p3MAOApNMZ9hTFSSkqOozZykmnzFU52Pz8A4AyrV0sNGuR6s93nbLufHwCQg9RUqXTpbJvtPmfb/fwAgBxs2CDVqJFts93nbLufHwACVlaWlJEhRUa6fRe7z9l2Pz8Az6Ex7ivS06Xixd3ePefJcj0nl9l6AQB2SEjIsblxmt2nbDIDALzM7t1SuXI53mT3KZvMAAAvlJgoxcZm22z3KZvMAACbZGTkqZchkRkAPIfGuK84ciRPYVI02N4zud3PDwD4R3q6dPSoVKpUrrvYfc62+/kBAP+xb59UsWKON9l9zrb7+QEAOdi5U6pZM9tmu8/Zdj8/AASsw4elEiXydBe7z9l2Pz8Az6Ex7isyMqSiRd3ePTLU/X0LWljRYBUL5n8tAPAKaWlWU/wsl76SGQAAF7t3S9Wq5XgTmQEAyGbrVunii7NtJjMAIEBlZEjFiuXpLmQGAE/ht91XnDghlSzp9u5Fg4NVvFiRQiwod6XD8hZ6AIBC9MsvUv36Z92FzAAAuFi3TrryyhxvIjMAAC7S062fItmzgcwAgACVliaFheXpLmQGAE+hMe4rduyQqlfP013KhIfYsjZHqbAQG54VAJCj7dulypXPuRuZAQBw2rhRuvDCXG8mMwAATsnJUmRkrjNUkRkAEIB27Trr54nckBkAPIHGuK9ISpJKl87TXSpHhssUUjlnEx8ZbsOzAgBytH9/ruvEnonMAABIkrKypOPHz7omIJkBAHDav/+szQ8yAwACUGKiVKZMnu9GZgDwBBrjviItTQrJ25VL5SJCFF7Uc//EQZJiI0IVYdOUJwCAHGzZIjVocM7dyAwAgCRrPcAiRaTQ0Fx3ITMAAE6//CJddlmuN5MZABCATp3K8xrjEpkBwDNojPuK/fulatXydJegoCDVKJX7SI+CZiRdWDrCY88HAHDD3r1SuXLn3I3MAABIkrZulerUOesuZAYAwGnbNqlKlVxvJjMAIAAdOCBVrZrnu5EZADyBxriv2LRJuvjiPN+tWqkIRYYULfS1OYIkVSgRqrjiYYX8TAAAtzkcUliYVLSoW7uTGQAArVsn1ax5zt3IDACAJCkhQYqNPesuZAYABJgtW6SLLjqvu5IZAAobjXFfcfJknqdSl6TgoCBdUT6q0NfmKBIUpEtjowr5WQAAeXL8uFSypNu7kxkAAB04IJUvf87dyAwAgCRrhqpzXFBFZgBAgDl58qxLM50NmQGgsNEY9wXGWKP+ypQ5r7uXCgvRJeXcb4ycjwbloxVWlLU4AMCrbNokNWyYp7uQGQAQ4LZtk2rXdmtXMgMAAlxGhpSW5tYMVWQGAASQQ4fOOZvI2ZAZAAoTjXFfkJoqReXv6qUapYrrotLFC6ggV5fHRalCSaYcAQCvs2qVVLdunu9GZgBAAPvjD7emUj+NzACAALZ3r1ShghTk3oS3ZAYABIDMTGvmWzezITdkBoDCQmPcF6xfL9Woka+HCAoKUp2yJVWnrHWlVX7X6AiSVCRIalg+WhdEReTz0QAAhWL3bqly5TzfjcwAgACVmiplZUnB7n9MJDMAIIAdOZKn2Q3JDAAIAH/8cV7fRf0XmQGgsJx7riPYb/duqUqVfD9MUFCQapYpoXIRIfr5QLKOp2ee92OVCiumhuWjVTyE/4UAwCsZY30YqVPnvO5OZgBAAAoLkyIi8jy6g8wAgAC1fbtUr16e7kJmAICf++svKT6+QB6KzABQGDgL+IK1a6U77iiwhysdHqLrLiinncnHte3IcZ3MyFKQJHOW+5y+PTKkqGqULq7KkeEKyud0KACAQnTypBQZ6dZ6f2dDZgBAAJk9W7rqqvO+O5kBAAFm//48jRg/E5kBAH5q9WqpRYsCfUgyA0BBojHuC9aulYYPL9CHLBocpBqlS6h6qeI6cDxNicfSdCj1lI6mZbgESnCQFBVaTGXCQ1ShRKjKhIcQIADgC77+WmrSpEAeiswAgABx9Kh00UX5eggyAwACyOefWz/nicwAAD/0009Snz4F/rBkBoCCQmPc2yUlSeHhUmhooTx8UFCQKpQIU4USYZKkzCyjU1lZyjJGRYKCFFIkWMEEBwD4np9/LvArdMkMAPBzM2dKkycXyEORGQDg51JTpfT0fM9QJZEZAOA3UlOl48elUqUK7SnIDAD5RWPc223aJDVq5LGnKxIcpPDgIh57PgBAIVm3TnrhhUJ9CjIDAPyIMdba4hUrFsrDkxkA4GcWL5bat7eyo4CRGQDgo3bskOrW9ehTkhkA8irY7gJwDnPmSLfdZncVAABfcuCA9V+ukAUAuGvmTKlpU7urAAD4iu3bpQsusLsKAIA3mTNH6tjR7ioA4Ky8ujE+cuRINWzYUCVLllRMTIxuuukmbd261WWfli1bKigoyOXngQcecNlnz5496tChgyIiIhQTE6NBgwYpIyPDky/l/BUtKhUrZncVAOD1yIwzLFsm3XGH3VUAgNciM3KwapV05512VwEAXofMyMWcOdLNN9tdBQB4lYDPjFOnpNKl7a4CAM7Kq6dSX758ufr27auGDRsqIyNDTz75pFq3bq0tW7aoePHizv169eql4cOHO/8eERHh/HNmZqY6dOiguLg4/fjjjzpw4IDuvvtuFStWTCNGjPDo6zkvISFSWprdVQCA1yMzzhAWJp08aXcVAOC1yIwcxMRIiYlS9ep2VwIAXoXMyEVUVIGsLw4A/iTgMyM9nUF+ALyeV7+DXbRokcvfp02bppiYGK1Zs0bNmzd3bo+IiFBcXFyOj/H1119ry5Yt+uabbxQbG6tLL71Uzz//vAYPHqxnn31WISEhhfoa8u34cemM0AQA5IzMOEPRolJmpt1VAIDXIjNyUKqUlJxsdxUA4HXIjFywbBMAZBPwmZGaKoWG2l0FAJyVV0+l/l8Oh0OSVPo/03HMmDFDZcuW1SWXXKIhQ4boxIkTzttWrlypunXrKjY21rmtTZs2SklJ0ebNm3N8nrS0NKWkpLj82MbhsK7CBQDkSUBmxmnp6YzeAIA8COjMOI0GBwC4hcwAALgr4DJj/36pQgV7nhsA3OQz35pnZWXp0UcfVdOmTXXJJZc4t3fp0kVVqlRRhQoVtGHDBg0ePFhbt27VnDlzJEkJCQkuISLJ+feEhIQcn2vkyJF67rnnCumV5FFyshQZaXcVAOBTAjYzTktKkuLj7a4CAHxCwGfGaZmZUrBPXTcNAB5HZgAA3BWQmcHstwB8gM80xvv27atNmzbp+++/d9neu3dv55/r1q2r8uXL67rrrtP27dt14YUXntdzDRkyRAMGDHD+PSUlRfF2NBiMsf7L6A0AyJOAzIwz7d8vNWxobw0A4CMCPjNOS0yUrrzS7ioAwKuRGf8wxpqlCgCQq4DLjLQ0LrQF4BN84kzVr18/LVy4UEuXLlWlSpXOum/jxo0lSdu2bZMkxcXFKTEx0WWf03/PbR2P0NBQRUZGuvzYYudOqXJle54bAHxUwGbGmTZvlmrXtrsKAPB6ZMY/jJF++kmqWdPuSgDAa5EZZ1i6VLr8crurAACvFZCZsWaN1KCB558XAPLIqxvjxhj169dPc+fO1bfffquqVaue8z7r1q2TJJUvX16S1KRJE23cuFFJSUnOfRYvXqzIyEjV9vamwWefSW3b2l0FAPiEgM+M0/bts/4bHm5vHQDgxciM//jsM6lZM6Y9BIAckBn/YYz08svSGSMTAQCWgM6MhQul1q3trgIAzsmrp1Lv27evPvzwQ82fP18lS5Z0rqERFRWl8PBwbd++XR9++KHat2+vMmXKaMOGDerfv7+aN2+uevXqSZJat26t2rVrq1u3bho9erQSEhL01FNPqW/fvgoNDbXz5Z3dyZPSggXSokV2VwIAPiGgM+NMzz4rDR1qdxUA4NXIjDOkpUmvvmo1xwEA2ZAZ//HJJ1LTplKZMnZXAgBeJ2AzIzVVWr1aevFFuysBgHMzXkxSjj9Tp041xhizZ88e07x5c1O6dGkTGhpqqlevbgYNGmQcDofL4+zatcu0a9fOhIeHm7Jly5rHHnvMpKenu12Hw+EwkrI9bqF66ilj5s3z3PMB8Au2nK+8REBnxmkrVhjz4IOef14APitQc4PMOMPw4cZ89JF9zw/AZ3jFOcsGZMYZkpONufZaY1JT7asBgE/winOWDQI2M0aONGbmTM88FwC/4+lzVpAxxhRe290/pKSkKCoqSg6HwzPrc2zZIj33nDRzZuE/FwC/4vHzFbKx7d/g2DHphhukOXOk6GjPPS8An0Zu2Mv24//LL9K4cdKMGVJQkOefH4BPsf2cFeBsP/7GSHffLfXpI111leefH4BPsf2cFeA8evx/+00aMkSaO5fPFADOi6czw6unUg9IqanSI49I06bZXQkAwJc88oj0zDM0xQEA7jl0SHr8cWnWLL7AAgCc2zvvSBdfTFMcAPCv9HTr+6ipU/lMAcBn0Bj3NgMGSI89JlWsaHclAABf8eqr0oUXSi1b2l0JAMAXnDol9ehhjRZnjVgAwLl89ZW0YoX03nt2VwIA8CYjRkj33EMvA4BPCba7AJzh44+lqCipbVu7KwEA+IqZM6WtW61pqwAAOJdTp6Ru3aT775cuvdTuagAA3m7FCmn8eOndd6VgvkYEAPzjl1+kP/6Q7rzT7koAIE8YMe4t/vzTuvJ2wQK7KwEA+IqlS6VPP5U+/JApqwAA55aWZjXFu3WTrr/e7moAAN7uq6+kiROti3HDwuyuBgDgLU6elAYPlj75hO+jAPgcGuPeIDVVevBBacoUqSj/JAAAN2zYII0ebTXGyQ4AwLmkpUldu1pTHXboYHc1AABvN2+e9MEHNMUBANk9+aS1HCzLMgHwQXyT7g0ee0zq31+Kj7e7EgCAL9izx8qNmTOliAi7qwEAeLvUVKlLF6lXL6ldO7urAQB4u48+khYutP5brJjd1QAAvMnSpdaI8fbt7a4EAM4LjXG7ffKJVKIEQQIAcM/hw9Zov8mTpbJl7a4GAODtUlOlzp2lBx6Q2rSxuxoAgLd7913pp5+s5f6KFLG7GgCAN3E4pOefZzlYAD6Nxridtm2Tpk2T5s+3uxIAgC84eVK66y7p5ZelCy6wuxoAgLc7edJqivftK/3vf3ZXAwDwdq+9Zn1X9fbbUnCw3dUAALzNgAHS8OHWQD8A8FE0xu2Smir16WOtK860VACAc8nMlHr0sD6EXHqp3dUAALzdiRNWU/zhh6XrrrO7GgCAtxsxwhoJ+PrrUlCQ3dUAALzNvHlSuXJSs2Z2VwIA+UJj3C4DB0qPPsq64gCAczNGeuQR6cYbpVat7K4GAODtjh+X7rzTupjqmmvsrgYA4M2MkYYOlSIipFGjaIoDALJLSpImTJAWLrS7EgDINxrjdpg1y/rA0aGD3ZUAAHzBiBHW1OlduthdCQDA2x0/Lt1xhzRokNSihd3VAAC8WVaW1L+/9Vmjf3+7qwEAeCNjpIceksaOlUJD7a4GAPKNxrinbdtmTZ++YIHdlQAAfMHUqdLff0vjxtldCQDA2x07ZjXFn3hCuvpqu6sBAHizzEzpgQekhg2l3r3trgYA4K2mT5cuu0yqX9/uSgCgQNAY96S0NOnBB6V332VdcQDAuX35pbRkifTee0xpCAA4u6NHrab40KFS06Z2VwMA8Gbp6dK990pt2kh33WV3NQAAb7V7tzRzJlOoA/ArNMY9adAga9qRypXtrgQA4O1+/lmaOFGaPVsKDra7GgCAN0tJsZriTz8tXXWV3dUAALxZaqrUrZvUubN0yy12VwMA8FZZWVK/ftL48VKRInZXAwAFhsa4p8yeba3B0bGj3ZUAALzdtm3SkCH/ZgcAALlxOKym+LPPSldeaXc1AABvdvy41KWL1KeP1Lat3dUAALzZ669LHTpI1avbXQkAFCga456wZYs0eTLrigMAzm3fPmuNv/ffl6Kj7a4GAODN/v7banC88ILUqJHd1QAAvNmhQ9a06YMHSy1b2l0NAMCb/fqr9N131oANAPAzNMYLW1aW9Pjj0tSprCsOADi7rCxr9MY770gVK9pdDQDAm2VlSffcI738slS3rt3VAAC8WVaW1KOHNGKEdNlldlcDAPBmWVnWEk3Tp0tBQXZXAwAFjsZ4YQsOlubNk4pyqAEA5xAcLM2dS2YAAM6NzAAAuIvMAAC4i34GAD8XbHcBAYEQAQC4i8wAALiLzAAAuIvMAAC4i8wA4MdojAMAAAAAAAAAAAAA/BqNcQAAAAAAAAAAAACAX6MxDgAAAAAAAAAAAADwazTGAQAAAAAAAAAAAAB+jcY4AAAAAAAAAAAAAMCv0RgHAAAAAAAAAAAAAPg1GuMAAAAAAAAAAAAAAL9GYxwAAAAAAAAAAAAA4NdojAMAAAAAAAAAAAAA/FpANcYnTJigCy64QGFhYWrcuLFWr15td0kAAC9FZgAA3EVmAADcRWYAANxFZgBAwQuYxvjMmTM1YMAADRs2TGvXrlX9+vXVpk0bJSUl2V0aAMDLkBkAAHeRGQAAd5EZAAB3kRkAUDgCpjE+btw49erVS/fcc49q166tN998UxEREZoyZYrdpQEAvAyZAQBwF5kBAHAXmQEAcBeZAQCFo6jdBXjCqVOntGbNGg0ZMsS5LTg4WK1atdLKlSuz7Z+Wlqa0tDTn3x0OhyQpJSWl8IsFgHw4fZ4yxthcie8iMwAEEnIjf8gMAIGEzMgfMgNAICEz8ofMABBIPJ0ZAdEY//vvv5WZmanY2FiX7bGxsfr999+z7T9y5Eg999xz2bbHx8cXWo0AUJAOHTqkqKgou8vwSWQGgEBEbpwfMgNAICIzzg+ZASAQkRnnh8wAEIg8lRkB0RjPqyFDhmjAgAHOvycnJ6tKlSras2cPQX4OKSkpio+P1969exUZGWl3OV6NY+U+jpX7HA6HKleurNKlS9tdSsAgM84fv9vu41i5j2OVN+SGZ5EZ54/fbfdxrNzHscobMsOzyIzzx++2+zhW7uNY5Q2Z4Vlkxvnjd9t9HCv3cazyxtOZERCN8bJly6pIkSJKTEx02Z6YmKi4uLhs+4eGhio0NDTb9qioKP4ndlNkZCTHyk0cK/dxrNwXHBxsdwk+i8zwPH633cexch/HKm/IjfNDZngev9vu41i5j2OVN2TG+SEzPI/fbfdxrNzHscobMuP8kBmex++2+zhW7uNY5Y2nMiMgkikkJERXXHGFlixZ4tyWlZWlJUuWqEmTJjZWBgDwNmQGAMBdZAYAwF1kBgDAXWQGABSegBgxLkkDBgxQ9+7d1aBBAzVq1Eivvvqqjh8/rnvuucfu0gAAXobMAAC4i8wAALiLzAAAuIvMAIDCETCN8TvuuEMHDx7UM888o4SEBF166aVatGiRYmNjz3nf0NBQDRs2LMfpSOCKY+U+jpX7OFbu41gVDDLDMzhW7uNYuY9jlTccr/wjMzyDY+U+jpX7OFZ5w/HKPzLDMzhW7uNYuY9jlTccr/wjMzyDY+U+jpX7OFZ54+njFWSMMR55JgAAAAAAAAAAAAAAbBAQa4wDAAAAAAAAAAAAAAIXjXEAAAAAAAAAAAAAgF+jMQ4AAAAAAAAAAAAA8Gs0xgEAAAAAAAAAAAAAfo3GuBsmTJigCy64QGFhYWrcuLFWr15td0ke9eyzzyooKMjl5+KLL3benpqaqr59+6pMmTIqUaKEbr31ViUmJro8xp49e9ShQwdFREQoJiZGgwYNUkZGhqdfSoFbsWKFOnbsqAoVKigoKEjz5s1zud0Yo2eeeUbly5dXeHi4WrVqpT///NNln8OHD6tr166KjIxUdHS0evbsqWPHjrnss2HDBl199dUKCwtTfHy8Ro8eXdgvrcCd61j16NEj2/9nbdu2ddknUI7VyJEj1bBhQ5UsWVIxMTG66aabtHXrVpd9Cur3btmyZbr88ssVGhqq6tWra9q0aYX98vwemUFm5IbMcB+Z4T4yw7eRGWRGbsgM95EZ7iMzfBuZQWbkhsxwH5nhPjLDt5EZZEZuyAz3kRnu87nMMDirjz/+2ISEhJgpU6aYzZs3m169epno6GiTmJhod2keM2zYMFOnTh1z4MAB58/Bgwedtz/wwAMmPj7eLFmyxPzyyy/myiuvNFdddZXz9oyMDHPJJZeYVq1amV9//dV88cUXpmzZsmbIkCF2vJwC9cUXX5ihQ4eaOXPmGElm7ty5LrePGjXKREVFmXnz5pn169ebG264wVStWtWcPHnSuU/btm1N/fr1zU8//WS+++47U716ddO5c2fn7Q6Hw8TGxpquXbuaTZs2mY8++siEh4ebt956y1Mvs0Cc61h1797dtG3b1uX/s8OHD7vsEyjHqk2bNmbq1Klm06ZNZt26daZ9+/amcuXK5tixY859CuL3bseOHSYiIsIMGDDAbNmyxYwfP94UKVLELFq0yKOv15+QGWTG2ZAZ7iMz3Edm+C4yg8w4GzLDfWSG+8gM30VmkBlnQ2a4j8xwH5nhu8gMMuNsyAz3kRnu87XMoDF+Do0aNTJ9+/Z1/j0zM9NUqFDBjBw50saqPGvYsGGmfv36Od6WnJxsihUrZmbNmuXc9ttvvxlJZuXKlcYY6wQSHBxsEhISnPtMmjTJREZGmrS0tEKt3ZP+e3LMysoycXFxZsyYMc5tycnJJjQ01Hz00UfGGGO2bNliJJmff/7Zuc+XX35pgoKCzF9//WWMMWbixImmVKlSLsdq8ODBpmbNmoX8igpPbkFy44035nqfQD1WxhiTlJRkJJnly5cbYwru9+7xxx83derUcXmuO+64w7Rp06awX5LfIjPIDHeRGe4jM/KGzPAdZAaZ4S4yw31kRt6QGb6DzCAz3EVmuI/MyBsyw3eQGWSGu8gM95EZeePtmcFU6mdx6tQprVmzRq1atXJuCw4OVqtWrbRy5UobK/O8P//8UxUqVFC1atXUtWtX7dmzR5K0Zs0apaenuxyjiy++WJUrV3Yeo5UrV6pu3bqKjY117tOmTRulpKRo8+bNnn0hHrRz504lJCS4HJuoqCg1btzY5dhER0erQYMGzn1atWql4OBgrVq1yrlP8+bNFRIS4tynTZs22rp1q44cOeKhV+MZy5YtU0xMjGrWrKk+ffro0KFDztsC+Vg5HA5JUunSpSUV3O/dypUrXR7j9D6Bdn4rKGTGv8iMvCMz8o7MyBmZ4RvIjH+RGXlHZuQdmZEzMsM3kBn/IjPyjszIOzIjZ2SGbyAz/kVm5B2ZkXdkRs68PTNojJ/F33//rczMTJd/CEmKjY1VQkKCTVV5XuPGjTVt2jQtWrRIkyZN0s6dO3X11Vfr6NGjSkhIUEhIiKKjo13uc+YxSkhIyPEYnr7NX51+bWf7/ychIUExMTEutxctWlSlS5cOuOPXtm1bvffee1qyZIleeuklLV++XO3atVNmZqakwD1WWVlZevTRR9W0aVNdcsklklRgv3e57ZOSkqKTJ08Wxsvxa2SGhcw4P2RG3pAZOSMzfAeZYSEzzg+ZkTdkRs7IDN9BZljIjPNDZuQNmZEzMsN3kBkWMuP8kBl5Q2bkzBcyo2ieXhECUrt27Zx/rlevnho3bqwqVarok08+UXh4uI2VwZ/ceeedzj/XrVtX9erV04UXXqhly5bpuuuus7Eye/Xt21ebNm3S999/b3cpgFvIDHgCmZEzMgO+hsyAJ5AZOSMz4GvIDHgCmZEzMgO+hsyAJ5AZOfOFzGDE+FmULVtWRYoUUWJiosv2xMRExcXF2VSV/aKjo3XRRRdp27ZtiouL06lTp5ScnOyyz5nHKC4uLsdjePo2f3X6tZ3t/5+4uDglJSW53J6RkaHDhw8H/PGrVq2aypYtq23btkkKzGPVr18/LVy4UEuXLlWlSpWc2wvq9y63fSIjI3mTeB7IjJyRGe4hM/KHzCAzfA2ZkTMywz1kRv6QGWSGryEzckZmuIfMyB8yg8zwNWRGzsgM95AZ+UNm+E5m0Bg/i5CQEF1xxRVasmSJc1tWVpaWLFmiJk2a2FiZvY4dO6bt27erfPnyuuKKK1SsWDGXY7R161bt2bPHeYyaNGmijRs3upwEFi9erMjISNWuXdvj9XtK1apVFRcX53JsUlJStGrVKpdjk5ycrDVr1jj3+fbbb5WVlaXGjRs791mxYoXS09Od+yxevFg1a9ZUqVKlPPRqPG/fvn06dOiQypcvLymwjpUxRv369dPcuXP17bffqmrVqi63F9TvXZMmTVwe4/Q+gXx+yw8yI2dkhnvIjPwhM8gMX0Nm5IzMcA+ZkT9kBpnha8iMnJEZ7iEz8ofMIDN8DZmRMzLDPWRG/pAZPpQZBmf18ccfm9DQUDNt2jSzZcsW07t3bxMdHW0SEhLsLs1jHnvsMbNs2TKzc+dO88MPP5hWrVqZsmXLmqSkJGOMMQ888ICpXLmy+fbbb80vv/ximjRpYpo0aeK8f0ZGhrnkkktM69atzbp168yiRYtMuXLlzJAhQ+x6SQXm6NGj5tdffzW//vqrkWTGjRtnfv31V7N7925jjDGjRo0y0dHRZv78+WbDhg3mxhtvNFWrVjUnT550Pkbbtm3NZZddZlatWmW+//57U6NGDdO5c2fn7cnJySY2NtZ069bNbNq0yXz88ccmIiLCvPXWWx5/vflxtmN19OhRM3DgQLNy5Uqzc+dO880335jLL7/c1KhRw6SmpjofI1COVZ8+fUxUVJRZtmyZOXDggPPnxIkTzn0K4vdux44dJiIiwgwaNMj89ttvZsKECaZIkSJm0aJFHn29/oTMIDPOhsxwH5nhPjLDd5EZZMbZkBnuIzPcR2b4LjKDzDgbMsN9ZIb7yAzfRWaQGWdDZriPzHCfr2UGjXE3jB8/3lSuXNmEhISYRo0amZ9++snukjzqjjvuMOXLlzchISGmYsWK5o477jDbtm1z3n7y5Enz4IMPmlKlSpmIiAhz8803mwMHDrg8xq5du0y7du1MeHi4KVu2rHnsscdMenq6p19KgVu6dKmRlO2ne/fuxhhjsrKyzNNPP21iY2NNaGioue6668zWrVtdHuPQoUOmc+fOpkSJEiYyMtLcc8895ujRoy77rF+/3jRr1syEhoaaihUrmlGjRnnqJRaYsx2rEydOmNatW5ty5cqZYsWKmSpVqphevXple8MWKMcqp+MkyUydOtW5T0H93i1dutRceumlJiQkxFSrVs3lOXB+yAwyIzdkhvvIDPeRGb6NzCAzckNmuI/McB+Z4dvIDDIjN2SG+8gM95EZvo3MIDNyQ2a4j8xwn69lRtA/RQMAAAAAAAAAAAAA4JdYYxwAAAAAAAAAAAAA4NdojAMAAAAAAAAAAAAA/BqNcQAAAAAAAAAAAACAX6MxDgAAAAAAAAAAAADwazTGAQAAAAAAAAAAAAB+jcY4AAAAAAAAAAAAAMCv0RgHAAAAAAAAAAAAAPg1GuMAAAAAAAAAAAAAAL9GYxzwoMzMTF111VW65ZZbXLY7HA7Fx8dr6NChNlUGAPA2ZAYAwF1kBgDAXWQGAMBdZAb8UZAxxthdBBBI/vjjD1166aV655131LVrV0nS3XffrfXr1+vnn39WSEiIzRUCALwFmQEAcBeZAQBwF5kBAHAXmQF/Q2McsMHrr7+uZ599Vps3b9bq1at122236eeff1b9+vXtLg0A4GXIDACAu8gMAIC7yAwAgLvIDPgTGuOADYwxuvbaa1WkSBFt3LhRDz30kJ566im7ywIAeCEyAwDgLjIDAOAuMgMA4C4yA/6Exjhgk99//121atVS3bp1tXbtWhUtWtTukgAAXorMAAC4i8wAALiLzAAAuIvMgL8ItrsAIFBNmTJFERER2rlzp/bt22d3OQAAL0ZmAADcRWYAANxFZgAA3EVmwF8wYhywwY8//qgWLVro66+/1gsvvCBJ+uabbxQUFGRzZQAAb0NmAADcRWYAANxFZgAA3EVmwJ8wYhzwsBMnTqhHjx7q06ePrrnmGk2ePFmrV6/Wm2++aXdpAAAvQ2YAANxFZgAA3EVmAADcRWbA3zBiHPCwRx55RF988YXWr1+viIgISdJbb72lgQMHauPGjbrgggvsLRAA4DXIDACAu8gMAIC7yAwAgLvIDPgbGuOABy1fvlzXXXedli1bpmbNmrnc1qZNG2VkZDAFCQBAEpkBAHAfmQEAcBeZAQBwF5kBf0RjHAAAAAAAAAAAAADg11hjHAAAAAAAAAAAAADg12iMAwAAAAAAAAAAAAD8Go1xAAAAAAAAAAAAAIBfozEOAAAAAAAAAAAAAPBrNMYBAAAAAAAAAAAAAH6NxjgAAAAAAAAAAAAAwK/RGAcAAAAAAAAAAAAA+DUa4wAAAAAAAAAAAAAAv0ZjHAAAAAAAAAAAAADg12iMAwAAAAAAAAAAAAD8Go1xAAAAAAAAAAAAAIBfozEOAAAAAAAAAAAAAPBrNMYBAAAAAAAAAAAAAH6NxjgAAAAAAAAAAAAAwK/RGAcAAAAAAAAAAAAA+DUa4wAAAAAAAAAAAAAAv0ZjHAAAAAAAAAAAAADg12iMA+fh2WefVVBQkEefc9euXQoKCtK0adM8+rwAgPwhMwAA7iIzAADuIjMAADkhH4CzozEOvzdt2jQFBQXl+vPTTz/ZXaKtjh49qscff1xVq1ZVaGioKlasqE6dOunEiRN2lwYAHkdm5GzZsmVnPS4vvvii3SUCgMeRGblLTU3VyJEjVbt2bUVERKhixYq67bbbtHnzZrtLAwBbkBm5O3bsmB599FFVqlRJoaGhqlWrliZNmmR3WQDgEeRD7mbOnKm77rpLNWrUUFBQkFq2bJnrvmlpaRo8eLAqVKig8PBwNW7cWIsXL/ZcsfApRe0uAPCU4cOHq2rVqtm2V69ePc+P9dRTT+mJJ54oiLJs5XA41KJFC+3bt0+9e/dW9erVdfDgQX333XdKS0tTRESE3SUCgC3IDFe1atXS+++/n237+++/r6+//lqtW7e2oSoA8A5kRnZdu3bVggUL1KtXL11++eXav3+/JkyYoCZNmmjjxo2qUqWK3SUCgC3IDFeZmZlq06aNfvnlF/Xt21c1atTQV199pQcffFBHjhzRk08+aXeJAOAR5EN2kyZN0po1a9SwYUMdOnTorPv26NFDs2fP1qOPPqoaNWpo2rRpat++vZYuXapmzZp5qGL4ChrjCBjt2rVTgwYNCuSxihYtqqJFff/XZ8iQIdq9e7fWrl3rEryDBw+2sSoAsB+Z4So2NlZ33XVXtu3PPfecatSooYYNG9pQFQB4BzLD1V9//aU5c+Zo4MCBGjNmjHP71VdfrWuvvVZz5sxR//79bawQAOxDZriaM2eOfvzxR02ePFn33nuvJKlPnz7q1KmTnn/+ed13332KiYmxuUoAKHzkQ3bvv/++KlasqODgYF1yySW57rd69Wp9/PHHGjNmjAYOHChJuvvuu3XJJZfo8ccf148//uipkuEjmEod+MfpdTDGjh2rV155RVWqVFF4eLhatGihTZs2ueyb0zodixcvVrNmzRQdHa0SJUqoZs2a2a5sTUpKUs+ePRUbG6uwsDDVr19f06dPz1ZLcnKyevTooaioKEVHR6t79+5KTk7Ose7ff/9dnTp1UunSpRUWFqYGDRpowYIF53y9ycnJmjp1qnr37q2qVavq1KlTSktLO+f9AACBlxk5Wb16tbZt26auXbue1/0BIFAEWmYcPXpUknVR1ZnKly8vSQoPDz/nYwBAoAq0zPjuu+8kSXfeeafL9jvvvFOpqamaP3/+OR8DAAJBoOWDJMXHxys4+NwtzNmzZ6tIkSLq3bu3c1tYWJh69uyplStXau/evW49HwKH7182ArjJ4XDo77//dtkWFBSkMmXKuGx77733dPToUfXt21epqal67bXXdO2112rjxo3Zvtw5bfPmzbr++utVr149DR8+XKGhodq2bZt++OEH5z4nT55Uy5YttW3bNvXr109Vq1bVrFmz1KNHDyUnJ+uRRx6RJBljdOONN+r777/XAw88oFq1amnu3Lnq3r17js/btGlTVaxYUU888YSKFy+uTz75RDfddJM+/fRT3Xzzzbkej++//16pqamqXr26OnXqpHnz5ikrK0tNmjTRhAkTdOmll7p7aAHA75AZ5zZjxgxJojEOIOCRGa4uvPBCVapUSS+//LJq1qypyy67TPv379fjjz+uqlWrZmt+AEAgITNcpaWlqUiRIgoJCXHZfnppvzVr1qhXr15nOaIA4B/Ih/P366+/6qKLLlJkZKTL9kaNGkmS1q1bp/j4+AJ5LvgJA/i5qVOnGkk5/oSGhjr327lzp5FkwsPDzb59+5zbV61aZSSZ/v37O7cNGzbMnPnr88orrxhJ5uDBg7nW8eqrrxpJ5oMPPnBuO3XqlGnSpIkpUaKESUlJMcYYM2/ePCPJjB492rlfRkaGufrqq40kM3XqVOf26667ztStW9ekpqY6t2VlZZmrrrrK1KhR46zHZdy4cUaSKVOmjGnUqJGZMWOGmThxoomNjTWlSpUy+/fvP+v9AcAfkRnuycjIMLGxsaZRo0Z5uh8A+BMyI3erVq0yF154ocsxueKKK8yBAwfOeV8A8EdkRs5efvllI8l89913LtufeOIJI8lcf/31Z70/APg68sE9derUMS1atMj1tmuvvTbb9s2bNxtJ5s0338zTc8H/MZU6AsaECRO0ePFil58vv/wy23433XSTKlas6Px7o0aN1LhxY33xxRe5PnZ0dLQkaf78+crKyspxny+++EJxcXHq3Lmzc1uxYsX08MMP69ixY1q+fLlzv6JFi6pPnz7O/YoUKaKHHnrI5fEOHz6sb7/9VrfffruOHj2qv//+W3///bcOHTqkNm3a6M8//9Rff/2Va83Hjh2TZF15tmTJEnXp0kV9+vTRvHnzdOTIEU2YMCHX+wKAvyMzzm7JkiVKTExktDgAiMzISalSpXTppZfqiSee0Lx58zR27Fjt2rVLt912m1JTU896XwDwZ2SGqy5duigqKkr33nuvFi9erF27duntt9/WxIkTJVkjGAEgEJAP5+/kyZMKDQ3Ntj0sLMx5O3AmplJHwGjUqJEaNGhwzv1q1KiRbdtFF12kTz75JNf73HHHHXr33Xd133336YknntB1112nW265RZ06dXKug7F7927VqFEj27oYtWrVct5++r/ly5dXiRIlXParWbOmy9+3bdsmY4yefvppPf300znWlZSU5BKUZzq9tl/Hjh1dnuvKK69U1apV9eOPP+b6egHA35EZZzdjxgwVKVJEd9xxh1v7A4A/IzNcORwOXX311Ro0aJAee+wx5/YGDRqoZcuWmjp1qssXaQAQSMgMV3FxcVqwYIG6deum1q1bS5IiIyM1fvx4de/ePdvzA4C/Ih/OX3h4uNLS0rJtP31B7uk+CHAajXGgAISHh2vFihVaunSpPv/8cy1atEgzZ87Utddeq6+//lpFihQp8Oc8fXXXwIED1aZNmxz3qV69eq73r1ChgiTluPZITEyMjhw5UgBVAgD+yxcz40wnT57U3Llz1apVq1zXrwIAFAxfzIxPP/1UiYmJuuGGG1y2t2jRQpGRkfrhhx9ojANAIfDFzJCk5s2ba8eOHdq4caOOHz+u+vXra//+/ZKsZg8AIH98NR/cVb58+RxHnx84cEDSv30Q4DQa48B//Pnnn9m2/fHHH7rgggvOer/g4GBdd911uu666zRu3DiNGDFCQ4cO1dKlS9WqVStVqVJFGzZsUFZWlsuVV7///rskqUqVKs7/LlmyRMeOHXO58mrr1q0uz1etWjVJ1pQmrVq1yvPrvOKKKyQpx9DYv3+/Lr744jw/JgAEmkDJjDMtWLBAR48eZRp1AMijQMmMxMRESVJmZqbLdmOMMjMzlZGRkefHBIBAEyiZcVqRIkV06aWXOv/+zTffSFK+P7sAgL8JtHxwx6WXXqqlS5cqJSVFkZGRzu2rVq1y3g6ciTXGgf+YN2+eS7N49erVWrVqldq1a5frfQ4fPpxt2+kT7ulpPNq3b6+EhATNnDnTuU9GRobGjx+vEiVKqEWLFs79MjIyNGnSJOd+mZmZGj9+vMvjx8TEqGXLlnrrrbecVz+d6eDBg2d9nTVr1lT9+vU1f/58/f33387tX3/9tfbu3av//e9/Z70/ACBwMuNMH374oSIiInTzzTe7fR8AQOBkxunRfR9//LHL9gULFuj48eO67LLLznp/AEDgZEZODh48qJdeekn16tWjMQ4A/xHI+ZCbTp06KTMzU2+//bZzW1pamqZOnarGjRsrPj6+wJ4L/oER4wgYX375pfMKpzNdddVVziuYJGsKj2bNmqlPnz5KS0vTq6++qjJlyujxxx/P9bGHDx+uFStWqEOHDqpSpYqSkpI0ceJEVapUSc2aNZMk9e7dW2+99ZZ69OihNWvW6IILLtDs2bP1ww8/6NVXX1XJkiUlWWt+N23aVE888YR27dql2rVra86cOXI4HNmed8KECWrWrJnq1q2rXr16qVq1akpMTNTKlSu1b98+rV+//qzH5JVXXtH//vc/NWvWTPfff78cDofGjRuniy66iOkNAQQ0MiNnhw8f1pdffqlbb72V9f4A4B9khquOHTuqTp06Gj58uHbv3q0rr7xS27Zt0xtvvKHy5curZ8+ebh9bAPA3ZEZ2LVq0UJMmTVS9enUlJCTo7bff1rFjx7Rw4cJsa90CgL8iH7JbsWKFVqxYIclqpB8/flwvvPCCJGsZjubNm0uSGjdurNtuu01DhgxRUlKSqlevrunTp2vXrl2aPHnyWZ8DAcoAfm7q1KlGUq4/U6dONcYYs3PnTiPJjBkzxrz88ssmPj7ehIaGmquvvtqsX7/e5TGHDRtmzvz1WbJkibnxxhtNhQoVTEhIiKlQoYLp3Lmz+eOPP1zul5iYaO655x5TtmxZExISYurWret8/jMdOnTIdOvWzURGRpqoqCjTrVs38+uvv7rUe9r27dvN3XffbeLi4kyxYsVMxYoVzfXXX29mz57t1vFZvHixufLKK01YWJgpXbq06datmzlw4IBb9wUAf0NmnN2bb75pJJkFCxa4tT8A+DMyI3eHDx82/fv3NxdddJEJDQ01ZcuWNXfeeafZsWOHewcXAPwMmZG7/v37m2rVqpnQ0FBTrlw506VLF7N9+3b3DiwA+DjyIXenX0dOP8OGDXPZ9+TJk2bgwIEmLi7OhIaGmoYNG5pFixad8zkQmIKMMabAuuyAD9u1a5eqVq2qMWPGaODAgXaXAwDwYmQGAMBdZAYAwF1kBgAgJ+QDUHCYjwYAAAAAAAAAAAAA4NdojAMAAAAAAAAAAAAA/BqNcQAAAAAAAAAAAACAX7O1MT5y5Eg1bNhQJUuWVExMjG666SZt3brVZZ/U1FT17dtXZcqUUYkSJXTrrbcqMTHRZZ89e/aoQ4cOioiIUExMjAYNGqSMjAyXfZYtW6bLL79coaGhql69uqZNm1bYLw8+5oILLpAxhjU6AC9FZsCbkBmAdyMz4E3IDMC7kRnwJmQG4N3IDNiFfAAKjq2N8eXLl6tv37766aeftHjxYqWnp6t169Y6fvy4c5/+/fvrs88+06xZs7R8+XLt379ft9xyi/P2zMxMdejQQadOndKPP/6o6dOna9q0aXrmmWec++zcuVMdOnTQNddco3Xr1unRRx/Vfffdp6+++sqjrxcAcP7IDACAu8gMAIC7yAwAgLvIDADwA8aLJCUlGUlm+fLlxhhjkpOTTbFixcysWbOc+/z2229Gklm5cqUxxpgvvvjCBAcHm4SEBOc+kyZNMpGRkSYtLc0YY8zjjz9u6tSp4/Jcd9xxh2nTpk1hvyQAQCEhMwAA7iIzAADuIjMAAO4iMwDA93jVGuMOh0OSVLp0aUnSmjVrlJ6erlatWjn3ufjii1W5cmWtXLlSkrRy5UrVrVtXsbGxzn3atGmjlJQUbd682bnPmY9xep/TjwEA8D1kBgDAXWQGAMBdZAYAwF1kBgD4nqJ2F3BaVlaWHn30UTVt2lSXXHKJJCkhIUEhISGKjo522Tc2NlYJCQnOfc4MkdO3n77tbPukpKTo5MmTCg8Pd7ktLS1NaWlpLrUdPnxYZcqUUVBQUP5fLAAUEmOMjh49qgoVKig42KuufSpQZAYAFIxAyA0yAwAKBpkR7bIvmQEAuSMzol32JTMAIHeezgyvaYz37dtXmzZt0vfff293KRo5cqSee+45u8sAgPO2d+9eVapUye4yCg2ZAQAFy59zg8wAgIJFZngGmQHAH5AZnkFmAPAHnsoMr2iM9+vXTwsXLtSKFStcXnRcXJxOnTql5ORkl6usEhMTFRcX59xn9erVLo+XmJjovO30f09vO3OfyMjIbFdXSdKQIUM0YMAA598dDocqV66svXv3KjIyMn8vFoBvMUbq00e65RapdWu7qzmnlJQUxcfHq2TJknaXUmjIDAAoOP6eG2QGABQcMoPMAAB3kRlkBgC4y9OZYWtj3Bijhx56SHPnztWyZctUtWpVl9uvuOIKFStWTEuWLNGtt94qSdq6dav27NmjJk2aSJKaNGmiF198UUlJSYqJiZEkLV68WJGRkapdu7Zzny+++MLlsRcvXux8jP8KDQ1VaGhotu2RkZEECRBonnpKat5c6tTJ7kryxB+nSSIzAKDw+FtukBkAUHjIDDIDANxFZpAZAOAuj2WGsVGfPn1MVFSUWbZsmTlw4IDz58SJE859HnjgAVO5cmXz7bffml9++cU0adLENGnSxHl7RkaGueSSS0zr1q3NunXrzKJFi0y5cuXMkCFDnPvs2LHDREREmEGDBpnffvvNTJgwwRQpUsQsWrTIrTodDoeRZBwOR8G9eADe7513jHn8cburyBN/Pl+RGQBQ8Pz1nEVmAEDB89dzFpkBAAXPX89ZZAYAFDxPn7NsbYxLyvFn6tSpzn1OnjxpHnzwQVOqVCkTERFhbr75ZnPgwAGXx9m1a5dp166dCQ8PN2XLljWPPfaYSU9Pd9ln6dKl5tJLLzUhISGmWrVqLs9xLgQJEIC++sqYrl2Nycy0u5I88efzFZkBAAXPX89ZZAYAFDx/PWeRGQBQ8Pz1nEVmAEDB8/Q5K8gYYwp6FLq/SUlJUVRUlBwOB1OPAIFg7VprCvU5c6SwMLuryRPOV/bj3wCAL+GcZS+OPwBfwjnLXhx/AL6Ec5a9OP4AfImnz1m2rjHubzIzM5Wenm53GT6nWLFiKlKkiN1lAJbt26VBg6TZs32uKQ4A/oz3WeeH91kAAhGZcX7IDACBiMw4P2QGgEBEZpwfb8sMGuMFwBijhIQEJScn212Kz4qOjlZcXJyCgoLsLgWBLClJ6tVLeustaccO6Yor7K4IAAIe77Pyj/dZAAIFmZF/ZAaAQEFm5B+ZASBQkBn5502ZQWO8AJz+hYiJiVFERIRX/MP6CmOMTpw4oaSkJElS+fLlba4IAevYMalbN+m116Tnn5e6dLG7IgCAeJ+VH7zPAhBoyIzzR2YACDRkxvkjMwAEGjLj/HljZtAYz6fMzEznL0SZMmXsLscnhYeHS5KSkpIUExPjVVMqIECkp1tN8aeflmbOlJo2ldq2tbsqAAh4vM/KP95nAQgUZEb+kRkAAgWZkX9kBoBAQWbkn7dlRrCtz+4HTq8nEBERYXMlvu308WN9BnicMdL990t33y39+aeUlmb9HQBgO95nFQzeZwEIBGRGwSAzAAQCMqNgkBkAAgGZUTC8KTMYMV5AmDohfzh+sM2TT0qNGkmRkdKsWdIHH9hdEQDgP3ifkD8cPwCBhHNe/nD8AAQSznn5w/EDEEg45+WPNx0/RozDYy644AK9+uqrdpcB/Ov116VixaTmzaWxY6XJk6VgTosAAN/CeywAgLvIDACAu8gMAIC7fCkz6ABBCQkJeuSRR1S9enWFhYUpNjZWTZs21aRJk3TixAm7ywMKxyefSJs2SX36SP36SdOnS/+sdQEAQEHgPRYAwF1kBgDAXWQGAMBdZEZ2TKUe4Hbs2KGmTZsqOjpaI0aMUN26dRUaGqqNGzfq7bffVsWKFXXDDTfYXSZQsJYutaZNnzxZuu02acIEKSbG7qoAAH6E91gAAHeRGQAAd5EZAAB3kRk5Y8S4F3CkOrQvZV+Ot+1L2SdHqqPQnvvBBx9U0aJF9csvv+j2229XrVq1VK1aNd144436/PPP1bFjR0nSnj17dOONN6pEiRKKjIzU7bffrsTEROfjbN++XTfeeKNiY2NVokQJNWzYUN98802h1Q2ct/XrpdGjpSlTpN69pSFDpFq17K4KAFAIeI8FAHAXmQEAcBeZAQBwF5nhfWiM28yR6lDbGW3VYloL7XXsdbltr2OvWkxrobYz2hbKL8ehQ4f09ddfq2/fvipevHiO+wQFBSkrK0s33nijDh8+rOXLl2vx4sXasWOH7rjjDud+x44dU/v27bVkyRL9+uuvatu2rTp27Kg9e/YUeN3Aedu1SxowQPrgA+nZZ6X27aWWLW0uCgBQGHiPBQBwF5kBAHAXmQEAcBeZ4Z2YSt1mR08dVdLxJO04skMtp7fUsu7LFB8Vr72OvWo5vaV2HNnh3C8qLKpAn3vbtm0yxqhmzZou28uWLavU1FRJUt++fdWqVStt3LhRO3fuVHx8vCTpvffeU506dfTzzz+rYcOGql+/vurXr+98jOeff15z587VggUL1K9fvwKtGzgviYnSvfdKU6dKM2dKJUpId99td1UAgELCeywAgLvIDACAu8gMAIC7yAzvxIhxm1WKrKRl3ZepWqlqzl+OH/f+6PylqFaqmpZ1X6ZKkZU8VtPq1au1bt061alTR2lpafrtt98UHx/v/KWQpNq1ays6Olq//fabJOuKkYEDB6pWrVqKjo5WiRIl9Ntvv/nsFSPwMw6HdNdd1lrimzZJP/9sjRgHAPgt3mMBANxFZgAA3EVmAADcRWZ4J0aMe4H4qHgt677M+cvQdEpTSXL+UsRHxZ/9Ac5T9erVFRQUpK1bt7psr1atmiQpPDzc7ccaOHCgFi9erLFjx6p69eoKDw9Xp06ddOrUqQKtGcizkyelLl2kF1+U0tKkSZOkTz+VgoLsrgwAUMh4jwUAcBeZAQBwF5kBAHAXmeF9GDHuJeKj4vX+ze+7bHv/5vcL7ZdCksqUKaP//e9/euONN3T8+PFc96tVq5b27t2rvXv/XQNhy5YtSk5OVu3atSVJP/zwg3r06KGbb75ZdevWVVxcnHbt2lVotQNuSU+3pkvv31+KjZUee0x67z0pNNTuygAAHsJ7LACAu8gMAIC7yAwAgLvIDO9CY9xL7HXsVbe53Vy2dZvbTXsde3O5R8GYOHGiMjIy1KBBA82cOVO//fabtm7dqg8++EC///67ihQpolatWqlu3brq2rWr1q5dq9WrV+vuu+9WixYt1KBBA0lSjRo1NGfOHK1bt07r169Xly5dlJWVVai1A2eVlSX17i3dead0+eXW+uKTJ0ulS9tdGQDAg3iPBQBwF5kBAHAXmQEAcBeZ4V1ojHuBvY69LmsK/HDvDy5rDhTmL8eFF16oX3/9Va1atdKQIUNUv359NWjQQOPHj9fAgQP1/PPPKygoSPPnz1epUqXUvHlztWrVStWqVdPMmTOdjzNu3DiVKlVKV111lTp27Kg2bdro8ssvL7S6gbMyRho4ULrqKqlDB6lbN2nsWOmCC+yuDADgQbzHAgC4i8wAALiLzAAAuIvM8D5BxhhjdxHeLiUlRVFRUXI4HIqMjHS5LTU1VTt37lTVqlUVFhaW58fel7JPLaa1cP5SnF5T4L+/LMt7LFelyEoF9Iq8T36PI+DixRelokWlQYOku+6SevSQWre2uyqPONv5Cp7BvwFQcPLz/oD3WP8623HknGUvjj9QcMiMgkFmeC+OP1BwyIyCQWZ4L44/UHDIjILhTZnBiHGblQwpqZjiMS6/FJK15sCy7stUrVQ1xRSPUcmQkjZXCviISZOk5GSrKf7YY1KbNgHTFAcA/Iv3WAAAd5EZAAB3kRkAAHeRGd6pqN0FBLqosCgt6rpIR08dzXZFSHxUvJb3WK6SISUVFRZlU4WAD5k5U1q7Vnr7bWncOKlsWal7d7urAgDYgPdYAAB3kRkAAHeRGQAAd5EZ3onGuBeICovK9X98f58+ASgwX30lzZkjzZhhNci3b5cmTLC7KgCAjXiPBQBwF5kBAHAXmQEAcBeZ4X1ojAPwfStXSm+8Ic2aJX3/vdUg//BDKSjI7soAAAAAAAAAAADgBWiMA/BtmzZJzzwjffqp9Oef0qhRVmO8KKc3AAAAAAAAAAAAWOgcFRBjjN0l+DSOH87L9u3Sww9LH38spaRIjzxiTaMeEWF3ZQCAAsT7hPzh+AEIJJzz8ofjByCQcM7LH44fgEDCOS9/vOn4BdtdgK8rVqyYJOnEiRM2V+LbTh+/08cTOKd9+6T77pOmT5dCQqQePaR33pHKlbO7MgBAAeF9VsHgfRaAQEBmFAwyA0AgIDMKBpkBIBCQGQXDmzKDEeP5VKRIEUVHRyspKUmSFBERoSDWNXabMUYnTpxQUlKSoqOjVaRIEbtLgi9ISpLuvlt6910pJkbq1EkaOVK68EK7KwMAFCDeZ+UP77MABBIyI3/IDACBhMzIHzIDQCAhM/LHGzODxngBiIuLkyTnLwbyLjo62nkcgbM6ckTq2lUaP16qWlXq3l3q00dq2NDuygAAhYD3WfnH+ywAgYLMyD8yA0CgIDPyj8wAECjIjPzzpsygMV4AgoKCVL58ecXExCg9Pd3ucnxOsWLFvOIqEfiAY8ekzp2lUaOk2rWlgQOlFi2k9u3trgwAUEh4n5U/vM8CEEjIjPw5a2acPGl9/gIAP0Fm5A+fMwAEEjIjf7wuM4yNli9fbq6//npTvnx5I8nMnTvX5XZJOf6MHj3auU+VKlWy3T5y5EiXx1m/fr1p1qyZCQ0NNZUqVTIvvfRSnup0OBxGknE4HOf9WgHk08mTxnTsaMyKFdbfX3rJmBdesLcmL+TP5ysyAwAKnr+es8gMAChAW7YYc911xjF7tl+es8gMACh4/nrOIjMAoOB5+pxl64jx48ePq379+rr33nt1yy23ZLv9wIEDLn//8ssv1bNnT916660u24cPH65evXo5/16yZEnnn1NSUtS6dWu1atVKb775pjZu3Kh7771X0dHR6t27dwG/IgCFIj1d6tZN6tdPuvpqado06a+/pFdftbsyeBCZAQBwF5kBAAXAGOmtt6Qvv5Tee08qUcLuigoFmQEAcBeZAQC+z9bGeLt27dSuXbtcb//vfPPz58/XNddco2rVqrlsL1myZK5z08+YMUOnTp3SlClTFBISojp16mjdunUaN24cQQL4gsxM6d57pS5dpNatpc8+k5YskaZPl4KC7K4OHkRmAADcRWYAQD4dPCg9+KDUpIk0d64UHCylpNhdVaEgMwAA7iIzAMD3BdtdgLsSExP1+eefq2fPntluGzVqlMqUKaPLLrtMY8aMUUZGhvO2lStXqnnz5goJCXFua9OmjbZu3aojR454pHYA58kY68uYNm2km2+WfvhBevdd6yfYZ05fsAGZAQBwF5kBAP/x1VdS587S0KHSgAF89joDmQEAcBeZAQDeydYR43kxffp0lSxZMtsUJQ8//LAuv/xylS5dWj/++KOGDBmiAwcOaNy4cZKkhIQEVa1a1eU+sbGxzttKlSqV7bnS0tKUlpbm/HuKn14VDXg1Y6THHpMuvVS66y5p82bpueekTz+VQkPtrg5ejswAALiLzACAf6SmSkOGWEtZLVggRUTYXZHXITMAAO4iMwDAO/lMY3zKlCnq2rWrwsLCXLYPGDDA+ed69eopJCRE999/v0aOHKnQ82yejRw5Us8991y+6gWQT88+K5UvL/XpI+3ZIz38sPTxx9IZa+4AuSEzAADuIjMAQNKmTdKjj1o/119vdzVei8wAALiLzAAA7+QT82F999132rp1q+67775z7tu4cWNlZGRo165dkqx1PRITE132Of333NbxGDJkiBwOh/Nn7969+XsBAPJm7FgpK0saNEj6+2/pnnukyZOlcuXsrgw+gMwAALiLzAAQ8IyRxo+XnnlG+uADmuJnQWYAANxFZgCA9/KJxvjkyZN1xRVXqH79+ufcd926dQoODlZMTIwkqUmTJlqxYoXS09Od+yxevFg1a9bMcdoRSQoNDVVkZKTLDwAPeest6a+/pOHDpWPHrGnUX3lFuuACuyuDjyAzAADuIjMABLTERKlTJ6s5/umnUi5ftsNCZgAA3EVmAID3srUxfuzYMa1bt07r1q2TJO3cuVPr1q3Tnj17nPukpKRo1qxZOV5dtXLlSr366qtav369duzYoRkzZqh///666667nCHRpUsXhYSEqGfPntq8ebNmzpyp1157zWXKEgBeYsYM6ZdfpJdftta169ZNGjpUqlfP7srgBcgMAIC7yAwAOIfPP5e6drWWsHr4YSkoyO6KbENmAADcRWYAgB8wNlq6dKmRlO2ne/fuzn3eeustEx4ebpKTk7Pdf82aNaZx48YmKirKhIWFmVq1apkRI0aY1NRUl/3Wr19vmjVrZkJDQ03FihXNqFGj8lSnw+EwkozD4Tiv1wnADQsXGtO5szEZGcZkZhpz993GzJtnd1U+x5/PV2QGABQ8fz1nkRkAkIsTJ4zp18+YRx4x5uTJPN3VX89ZZAYAFDx/PWeRGQBQ8Dx9zgoyxphC7r37vJSUFEVFRcnhcDANCVAYVq6URo6UZs2SQkKkxx6TateW3FiHB644X9mPfwMAvoRzlr04/gA8av16acAAaeBAqV27PN+dc5a9OP4AfAnnLHtx/AH4Ek+fs4oW+jMAwNls2SINGybNni2FhkovvSSVLUtTHAAAAAAKQlaW9Npr0g8/SB99JP2zhikAAAAABBpb1xgHEOD27pX69ZPef1+KjJSmTpUOHJCGDLG7MgAAAADwfQcOSLfcIoWFWTN00RQHAAAAEMAYMQ7AHocOSd27S+++K8XGSgsWSEuXStOmSUFBdlcHAAAAAL5t/nzpjTes0eK1a9tdDQAAAADYjsY4AM87flzq2lUaN06qVu3fhvjHH0vBTGQBAAAAAOft+HFrHfHixaWFC60lqwAAAAAATKUOwMPS06Vu3aQnnpAuvVT65RdpzBjpgw+kkBC7qwMAAAAA37V2rXTDDdb06WPH0hQHAAAAgDMwYhyA52RlSb17S3fdJbVsKf32m7We+KxZUkSE3dUBAAAAgG/KypJefllas0aaOVMqW9banpTEuuIAAAAA8A9GjAPwnMGDpSuvtEYv7N4t9etnjRSPjra7MgAAAADwTQcOSDffLEVFSR999G9TfOZMqXt36dgxe+sDAAAAAC/BiHEAnjF2rFSypHT//VJionTPPdLUqVJsrN2VAQAAAIBv+uILadw4afx4qVYta9uJE1L//lKZMtKCBVKxYvbWCAAAAABegsY4gML33nvWCPHXX5eSk62p1CdMkKpUsbsyAAAAAPA9aWnWslQZGdLChVJYmLV940bp0Uet21q1srVEAAAAAPA2NMYBFK7PP5e++spqjp88KXXpIo0c+e9oBgAAAACA+7Ztkx58UOrbV7rxRmubMdKbb0qLF1vTqbOuOAAAAABkQ2McQOFZuVJ66y1p1iwpM9MaKf7441KDBnZXBgAAAAC+57PPrNm3Jk+W4uOtbUeOWI3yK66QZs+WgoPtrREAAAAAvBSNcQCFY8sWadgw64uZokWlHj2sn5YtbS4MAAAAAHxMZqb1+erIEWn+fCk01Nr+ww/S009Lo0ZJjRrZWyMAAAAAeDka4wAK3t69Ur9+1hR+JUtaoxdat5ZuuMHuygAAAADAt/z9t3TffdJNN1kXG0tWo3zkSOmPP6R586TISBsLBAAAAADfwPxaAArWoUNS9+7Su+9KsbHS0KHWeuLdutldGQAAAAD4lrVrpdtus0aLn26K799vNckrVpSmT6cpDgAAAABuYsQ4gIJz/LjUtav0yitStWrSmDHWFH8PP2x3ZQAAAADgWz75RPrgA2t5qjJlrG2ffy69+qo0frx08cW2lgcAAAAAvobGOICCkZ4u3X23NGSIVL++NWJ8/35p3Di7KwMAAAAA35GVJT37rJScLH36qVSsmJSWJj35pPW567PPpLAwu6sEAAAAAJ/DVOoA8i8rS+rd2xot3qKFNaLhxx+ll1+WgoLsrg4AAAAAfMOxY1KXLlKlStLrr1tN8T//lDp2lJo3t7bRFAcAAACA88KIcQD59/jjUpMm0i23SF99ZU359+GHUjDX3gAAAACAW3btku67T3r6aeuCY2OkadOsUeOTJ0vx8XZXCAAAAAA+jcY4gPwZO1aKirJGjK9YIU2cKM2cKRXl9AIAAAAAbvnuO+m556R33pGqVpWOHJEeekiqXVuaP18qUsTuCgEAAADA59G5AnD+3ntP2rNHeu016eefpVGjpFmzmNoPAAAAANz17rvS4sXS3LlSyZJWk3zYMGnkSKlxY7urAwAAAAC/QWMcwPn5/HNr2vT33pM2bZKGDrWmUC9e3O7KAAAAAMD7ZWRIjz1mNcM/+kjKypKeeUbat0+aN0+KjLS7QgAAAADwKywADCDvfvxReustacoUaft26dFHrTXFo6PtrgwAAAAAvN/hw9Ktt0pXXSW98IK0e7fUsaN08cXW5yya4gAAAABQ4BgxDiBvtmyRnn1W+vRTKSFBeuAB6YMPpLJl7a4MAAAAALzfb79JDz4ojR0rXXGFNGOG9Zlq4kRrfXEAAAAAQKGgMQ7AfXv3Sv36SR9/LB07Jt1zjzWaoUIFuysDAAAAAO/3xRfS669bM24VLy716CFdcIH02WdSUb6iAQAAAIDCxFTqANxz6JD1pc3kyVJwsHTXXdKkSdaXOAAAAACA3BljjRCfPdtaP3z3bunGG6X77rNm5KIpDgAAAPivrCxp6VK7q4BojANwx4kTViN83DipdGmpSxfplVekmjXtrgwAAAAAvFtqqtSzp/Xnt9+2GuRvvCHNnSs1a2ZvbQAAAAAK39Ch0s6ddlcB0RgHcC6ZmdZI8UGDpOrVrab4Cy9I9erZXRkAAAAAeLcDB6SbbpI6dZJuv90aJR4fL73/vhQdbXd1AAAAAArb889LQUHSvffaXQnEGuMAzsYYqX9/6YYbpKZNpTvukB5/XGrUyO7KAAAAAMC7rVljXWA8YYK0caPUq5f15+rV7a4MAAAAgCc8/7z0zjvStm12V4J/2DpifMWKFerYsaMqVKigoKAgzZs3z+X2Hj16KCgoyOWnbdu2LvscPnxYXbt2VWRkpKKjo9WzZ08dO3bMZZ8NGzbo6quvVlhYmOLj4zV69OjCfmmAf3jlFSk21mqId+sm3X+/1KKF3VUhQJEZAAB3kRkAbDdzpjR8uDRtmrUk1bp10sKFNMW9EJkBAHAXmQEgTyZOtH7WrJFCQuyuBv+wtTF+/Phx1a9fXxMmTMh1n7Zt2+rAgQPOn48++sjl9q5du2rz5s1avHixFi5cqBUrVqh3797O21NSUtS6dWtVqVJFa9as0ZgxY/Tss8/q7bffLrTXBfiFWbOkrVulwYOl++6zmuPt2tldFQIYmQEAcBeZAcA2WVnS009LP/4oPfmktSxVly7SiBFSsWJ2V4cckBkAAHeRGQDcNmWK9Oyz0qpVUrlydleDM9g6lXq7du3U7hyNttDQUMXFxeV422+//aZFixbp559/VoMGDSRJ48ePV/v27TV27FhVqFBBM2bM0KlTpzRlyhSFhISoTp06WrduncaNG+cSOADO8OOP0ocfWqMc+vWTrrtOuvVWu6tCgCMzAADuIjMA2OL4calnT+maa6TDh60ZuGbNksqUsbsynAWZAQBwF5kBwC3vvisNHSp9/71UubLd1eA/bB0x7o5ly5YpJiZGNWvWVJ8+fXTo0CHnbStXrlR0dLQzRCSpVatWCg4O1qpVq5z7NG/eXCFnTFPQpk0bbd26VUeOHPHcCwF8xV9/SU89ZU3598QTUr160t13210V4BYyAwDgLjIDQIE6cEC6+WbpppukBQukuDjpo49oivsJMgMA4C4yAwhwEyda/ZUlS6SLLrK7GuTA1hHj59K2bVvdcsstqlq1qrZv364nn3xS7dq108qVK1WkSBElJCQoJibG5T5FixZV6dKllZCQIElKSEhQ1apVXfaJjY113laqVKlsz5uWlqa0tDTn31NSUgr6pQHeKS3NGuEwaZK1Dl758tKDD9pdFeAWMgMA4C4yA0CB2rhRevhha+mp6dOlN96QLrzQ7qpQQMgMAIC7yAwgwL3yijRmjPTtt1Lt2nZXg1x4dWP8zjvvdP65bt26qlevni688EItW7ZM1113XaE978iRI/Xcc88V2uMDXuuRR6S+fa0RDsZIgwbZXRHgNjIDAOAuMgNAgfn6a2n0aGtkeFqa9NlnUlGv/qoFeURmAADcRWYAAezFF63R4kuXSjVr2l0NzsLrp1I/U7Vq1VS2bFlt27ZNkhQXF6ekpCSXfTIyMnT48GHnOh5xcXFKTEx02ef033Nb62PIkCFyOBzOn7179xb0SwG8z9tvWyPEd+2SEhMl3kzBx5EZAAB3kRkAzsu770ovvSSlp0uPPy49/TRN8QBAZgAA3EVmAAHAGGv2qHfekZYtoynuA3yqMb5v3z4dOnRI5cuXlyQ1adJEycnJWrNmjXOfb7/9VllZWWrcuLFznxUrVig9Pd25z+LFi1WzZs0cpx2RpNDQUEVGRrr8AH7tp5+kxYul2Fjpjz+s6T6CguyuCsgXMgMA4C4yA0CejRxpLUF14YXS559LjRrZXRE8hMwAALiLzAD8XEaG1LmztGiRNX16jRp2VwQ32NoYP3bsmNatW6d169ZJknbu3Kl169Zpz549OnbsmAYNGqSffvpJu3bt0pIlS3TjjTeqevXqatOmjSSpVq1aatu2rXr16qXVq1frhx9+UL9+/XTnnXeqQoUKkqQuXbooJCREPXv21ObNmzVz5ky99tprGjBggF0vG/AuCQnS0KHStddKv/4qvfYaTXF4JTIDAOAuMgNAoTFGeuABa0TI009bM2+VKGF3VcgHMgMA4C4yA4DTiRNShw7Spk3W9OnVqtldEdxlbLR06VIjKdtP9+7dzYkTJ0zr1q1NuXLlTLFixUyVKlVMr169TEJCgstjHDp0yHTu3NmUKFHCREZGmnvuucccPXrUZZ/169ebZs2amdDQUFOxYkUzatSoPNXpcDiMJONwOPL9mgGvkpZmTPv2xowda0yPHsZkZNhdEfLJn89XZAYAFDx/PWeRGQAKRUaGMS1aGFOnjjEHDthdjcf56zmLzACAguev5ywyA4AxxphDh4y58kpjrrjCmL//trsan+fpc1aQMcYUcu/d56WkpCgqKkoOh4NpSOBfHnpIioiQ9u2Tpk9nPTw/wPnKfvwbAPAlnLPsxfEHfMiePVLz5lKTJtKHHwbkLFucs+zF8QfgSzhn2YvjDxSi3bulG2+UoqOlzz6TSpa0uyKf5+lzlk+tMQ6gAE2dKh08KO3aJU2bRlMcAAAAAHLy8cfSlVdKffpIH30UkE1xAAAAIOCtWmVNnx4fL331FU1xH0UnDPBjjlSHjp46qkqRlVxv+Plnpb09ScExsSr2yWypWDF7CgQAAAAAb5WSYs2ytXq19Oqr0u23210RAAAAADvMni298ILUoIH07rsMNPRhjBgH/JQj1aG2M9qqxbQW2uvY++8NSUk6dd89+tGxSa3aJclhUu0rEgAAAAC80Q8/SB07Stu3S6+8QlMcAAAACETGSCNHWj9t2lgz8dIU92n86wF+6uipo0o6nqQdR3ao5fSWWtZ9meIj4pTWsb3WHduu6+9MVdypv3X01FFFhUXZXS4AAAAA2C89XRo+3GqIh4ZKTz1lrS0OAAAAILCcOmUtp7Rhg3Tffdaf4fMYMQ74qUqRlbSs+zJVK1XN2Rw/2Km9tu/bqFZ3pCoutpqWdV+WfZp1AAAAAPBzjlSH9qXsc934xx/S9dfrcNkSSk9KkF58kaY4AAAAEIiOHJFuuMFqig8bRlPcj9AYB/xYfFS8szneacEOHVv+ja7qdkoxcVZTPD4q3u4SAQAAAMCjsi07ZYw0caI0cKASnnhIv094VvdcfUiOuhfZXSoAAAAAT/vjD6lDB+nwYWnSJOn66+2uCAWIxjjg5+Kj4jW79nNq/6fUqJfkCJfev/l9muIAAAAAAtKZy07d8Voznbi+rZServ3PP66d99+mHm1StbLUMR09ddTuUgEAAAB40qJFUo8eUlCQNHOm1KCB3RWhgNEYB/zcgTXLdfLBXrrtdunvEta2bnO7WSMjAAAAACDAnF526uGdMXp2+h7dVH+L1taP0e7b/qc7O6QqswbLTgEAAAABxRhpzBjplVekkiWlBQukqlXtrgqFgMY44Mf2r/9euzq31a03pKpkfDX9cO8PLmuO0xwHAAAAEHAOH1Z8vyc1PLazHupXVd8H7VPSfV10yw2pKlqVZacAAACAgHLypDVK/JdfpDJlpPnzrf/CL9EYB/zUgc2rtPu2/+m261MVUdn6cueq+Kuca46fbo7vS9lnd6kAAMAOp05JH35odxUA4FlffSV16iQ99JCiXnpVUzt9oJMhUoeuUkJJlp0CAAAAAsq+fdKNN0rHj0s1akgffCCFhdldFQoRjXHAH+3fr3J9B2l0z4sVeoHriIf4qHhnczymeIxKhpS0uVgAAOBRhw9LL70ktW9vNccBIBAcPy716yd9/rm0cKHUqJH2Ovaq29xukqSsf74dYdkpAAAAIED8+KN0111ScLB0003SCy9Yf4ZfK2p3AQAKWGKi1K2bir4zWdPiY3T01NFsa+PFR8VreY/lKhlSUlFhUTYVCgAAPOr336XXX5f++kvq1UsaNEg6elR65BG7KwOAwrVqlTRkiPTEE1Lr1pKkvY69ajm9pXYc2aFqparp/ZvfV7e53ZwzazGdOgAAAODHJk+W5syx/jxsmNSkib31wGNojAP+JClJ6tJFeuMNqUYNRUm5Nr7/2ywHAAB+yBjpm2+kN9+USpeWHn5YqlvX2r5wodUoBwB/lZ4uDR8u7d4tffqpVKqUJGlfyj6XpvjpJviy7suc21tOb6nlPZbzuQkAAADwJ2lp0oAB0okTUmqqNH26VKWK3VXBg5gTAPAXBw9aTfHx46VateyuBgAA2OnkSendd62RkT/9ZDXG33nHeo8wY4a1fdMmacoUuysFgMKxZYvUoYN1MdB77zmb4pJUMqSkYorHuDTFJZadAgAAAPza6fXEQ0OtGfTmzaMpHoAYMQ74g0OHrKb4q69KtWvbXQ0AALDLgQPShAnS6tXWOlkLF1of+FJTpUmTpNmzpVtukRYskMLDpZQUuysGgIKVlWXNhrF8uTRtmlShQrZdosKitKjrIpadAgAAAALFt99KI0ZINWpY64jPnCkVKWJ3VbABjXHA1x0+LHXuLL38snTJJXZXAwAA7LBmjdUISk2V+vaVnn9eCgqSkpOlceOkJUuk7t2lRYukYsXsrhYACseePdY5sH17a73AoKBcd40Ki2LZKQAAAMDfGSONGSNt2GDNInXFFdJ999ldFWxEYxzwZUeOSHfeKY0eLdWrZ3c1AADAkzIzpfnzrenQa9SQhg2TqlWzbtu3z5pJ5vffpT59pMGDrSuiAcAfGSO9/7700UfWRUI1athdEQAAAAC7paRIvXtby8olJEhPPSW1bGl3VbAZjXHAVyUnW03xUaOkSy+1uxoAAOApDofVDP/8c+n666UPP5QiI63bNm+2ZpE5flx69FGpSRNbSwWAQnfwoPTQQ9Za4p99JhXlaw4AAAAg4G3eLD38sLWm+GefSZMns544JNEYB3xTSorVFH/xRenyy+2uBgAAeML27dL48dK2bVLPntJXX1nrYRkjffed9NprUunS1ujwmjXtrhYACt/ChdbsGGPGSJddZnc1AAAAALzBzJnWjFItW0q//GLNthcRYXdV8BI0xgFfc/SodMcd0vDhUoMGdlcDAAAKkzHSihXSxIlSeLg1KvKKK6zbsrKkefOkt96S6te3mubly9taLgB4xNGj0sCBUsmSVnM8LMzuigAAAADYLT3dGixgjFSmjPU5Yfp0KSjI7srgRWiMA77kdFP8mWekRo3srgYAABSW9PR/r3Bu2FB65RWpQgXrtrS0f9fSbdfO2u/0VOoA4O+WLbMuEn7mGdYHBAAAAGDZs0e6/35r6vQ5c6THHpPatLG7KnghGuOArzh+3Jo+fehQ1gsFAMBfnTplXc388cfSbbdZI8LDw63bHA7pzTelxYulrl2lL7+UQkJsLRcAPObECenJJ62Lg+bPt0aLAwAAAMDnn1tLLPXsKb3zjjWzXvXqdlcFL0VjHPAFJ05YTfEnnpCaNrW7GgAAUNBOnZImT5Zmz5buuktatEgqVsy67a+/rPXDN2+WHnhAGjRICg62t14A8KSVK60LhB9/XGrb1u5qAADAuWzaZHcFAAJBerr01FPSsWPSDTdYgwvmzeMiWpwVjXHA251uig8cKF19td3VAACAgrZokTRmjNS9u/TVV1LRf96ib9hgXfF87Jj0yCPS6NG2lgkAHpeWJg0bJiUmSp9+KpUqZXdFAADgXKZNkz74wO4qAPi7ffuk3r2lLl2k77+XjhyRPvyQgQQ4JxrjgDc7eVLq3Fl69FGpRQu7qwEAAAVp505p8GBreq/586USJSRjrEb5pElSxYrWbDEXXWR3pQDgeWvXWiPEH3rIWicQAAB4t2PHrNwODrZmxAKAwrJokfTyy9LTT0svvij17WuNGAfcQGMc8FapqdbVTg89JF17rd3VAACAgnLihPTSS9b0giNHWo3vtDRpyhTr6uaWLa1p1cuWtbtSAPC89HRpxAhp61bp4485FwIA4At+/VV67DGpfn0pIUH66COpUiW7qwLgbzIyrBmlDh+W+veXhg+3BhbUqGF3ZfAhzCkAeKO0NKlrV6lPH6lVK7urAQAABcEYac4c6frrpQYNrPXES5eWnn9e6tDB2mfhQmt9LBpBAALR5s3WObJmTWnGDM6FAAB4O2Ok8eOl556TypWTYmOti31Z3xdAQdu/35pJqnZtqUoVaeZMa/Y9muLII0aMA97mdFP8vvuk1q3trgYAABSE33+3pkW/4grpiy+kPXusqb7277cuhHvqKSkoyO4qAcAemZnWVIirV1vrkpYvb3dFAADgXA4ftj7LVKsmpaRYozhZChJAYVi82Jp5b+RIaexYqXlz63MD36PgPNg6YnzFihXq2LGjKlSooKCgIM2bN895W3p6ugYPHqy6deuqePHiqlChgu6++27t37/f5TEuuOACBQUFufyMGjXKZZ8NGzbo6quvVlhYmOLj4zV69GhPvDwg706dkrp1k3r0kNq1s7sawKuQGQB80tGj0qBB1giKV1+1Prx16yaNGiU98IA0b57Upg0f5goYmQH4kD/+sEaJlysnzZpFUxweR2YAwHn4/nupUyepXj1p40ZrppcAaIqTGYCHZWZaF9188onVFB88WHr0UWugAd+j4DzZ2hg/fvy46tevrwkTJmS77cSJE1q7dq2efvpprV27VnPmzNHWrVt1ww03ZNt3+PDhOnDggPPnoYcect6WkpKi1q1bq0qVKlqzZo3GjBmjZ599Vm+//XahvjYgz9LTpe7dpbvusr4YAuCCzADgU4yxvhy66Sbp2mulG26Q7r9f+vpr6bXXrPXE69Wzu0q/RWYAPiAry5p6ddAg6a23pHvu4cst2ILMAIA8yMyUXnhBevNNqXp16dgx62LfALmwjcwAPCgxUbr5ZmtWimuusdYT/+gjqUkTuyuDj7N1KvV27dqpXS6jYqOiorR48WKXbW+88YYaNWqkPXv2qHLlys7tJUuWVFxcXI6PM2PGDJ06dUpTpkxRSEiI6tSpo3Xr1mncuHHq3bt3wb0YID9OnZLuvlvq3Nn64hxANmQGAJ+xbp00dKj1Ya11a2uar1tusdYXL17c7uoCApkBeLldu6SHHpLat7e+TKchDhuRGQDgpv37rYt9mzeXkpKsLO/Y0e6qPIrMADxkxQpr5r2xY6WpU6XwcGnuXKkoq0Mj/2wdMZ5XDodDQUFBio6Odtk+atQolSlTRpdddpnGjBmjjIwM520rV65U8+bNFRIS4tzWpk0bbd26VUeOHPFU6UDuTjfFu3SRbrzR7moAv0FmAPC4w4etL4deeEGqWFH68UfpoousUeJ9+9IU92JkBuAhxkjvvmudE1991VqXlKY4fAyZASAgLVhgfX/ZoYO11u+bbwZcU/x8kBlAHhkjvfKK9M470qRJ0pNPSi1bWuuL0xRHAfGZ/5NSU1M1ePBgde7cWZGRkc7tDz/8sC6//HKVLl1aP/74o4YMGaIDBw5o3LhxkqSEhARVrVrV5bFiY2Odt5UqVSrbc6WlpSktLc3595SUlMJ4ScC/a4rfdRdvJoECRGYA8KjMTGnyZOntt6XISKlSJem++6RGjeyuDG4gMwAP+esv6+KhZs2sL9eLFLG7IiDPyAwAAef4cWngQCksTGrQQFq/3srxsDC7K/N6ZAaQR6dOSQ8/LFWtKvXsaV1EO3GiVLOm3ZXBz/hEYzw9PV233367jDGaNGmSy20DBgxw/rlevXoKCQnR/fffr5EjRyo0NPS8nm/kyJF67rnn8lUzcE6nTlkN8W7daIoDBYjMAOBR330nPfiglJFhrX3Vt681Whw+gcwAPMAYacYM6f33rVHitWrZXRFwXsgMAAFnzRrp8celXr2sLL/9dut7TJwTmQHk0aFDUo8e0j33SL//Lk2ZIs2fL5UoYXdl8ENeP5X66RDZvXu3Fi9e7HJ1VU4aN26sjIwM7dq1S5IUFxenxMREl31O/z23dTyGDBkih8Ph/Nm7d2/+XwhwprQ0qWtXawoimuJAgSEzAHjMpk3S5ZdLnTtL994rrV0rjRhBU9yHkBmAByQlWefJ3bulzz+nKQ6fRWYACCiZmdLIkdbavvffby2DMmYMTXE3kRlAHu3aJd12m9S/v/TBB1LZstL06TTFUWi8ujF+OkT+/PNPffPNNypTpsw577Nu3ToFBwcrJiZGktSkSROtWLFC6enpzn0WL16smjVr5jjtiCSFhoYqMjLS5QcoMGlp1kjxHj2k66+3uxrAb5AZADzi55+lK6+UWrWypkvfu9f68BYebndlyAMyA/CA2bOti4GHDJGGDmVNQPgsMgNAQNm9W7rhBqsxFR8vLV8uffaZdPHFdlfmE8gMII+2bLEGG/TtK73wgvTMM1Lv3lJQkN2VwY/Z+sn02LFj2rZtm/PvO3fu1Lp161S6dGmVL19enTp10tq1a7Vw4UJlZmYqISFBklS6dGmFhIRo5cqVWrVqla655hqVLFlSK1euVP/+/XXXXXc5Q6JLly567rnn1LNnTw0ePFibNm3Sa6+9pldeecWW14wAd3qk+L33Su3b210N4FPIDAC2ycy01tEbPVrat0/q108aNEgK9uprTAMamQHY6OBB64KhCy+0RomHhNhdEXBWZAYA/OOjj6xRmk88Ib30kvX95W232V2VVyEzgAK0erV1AW3z5tKnn0pz50pRUXZXhUBgbLR06VIjKdtP9+7dzc6dO3O8TZJZunSpMcaYNWvWmMaNG5uoqCgTFhZmatWqZUaMGGFSU1Ndnmf9+vWmWbNmJjQ01FSsWNGMGjUqT3U6HA4jyTgcjoJ66QhEqanG3HKLMZ9/bncl8GP+fL4iMwB4XHKyMS+/bEzTpsZccYUxDzxgzJEjdldVoPz1nEVmADaZNcuYVq2MWbvW7kpQCPz1nEVmAAh4ycnGdO9uzNNPG/PJJ8a0bm3M9u35ekh/PWeRGUABWbzYmP/9z5ibbjJm/HhjsrLsrgg28vQ5K8gYY/LdXfdzKSkpioqKksPhYBoSnJ/UVGuk+H33Se3a2V0N/BjnK/vxbwD4gT//lF5/XdqxQ4qLkw4csEZM1K1rd2UFjnOWvTj+8BtnjhIfOpRR4n6Kc5a9OP4ACsX331tTFz/1lDRvnpXhI0bkO8s5Z9mL4w+vNmeO9MYb1ux8L78sNWhgd0WwmafPWSzyBRS2003xXr2ktm3trgYAAOTEGGnJEunNN6XISGsqr99/l66+Wrr7bqZNB4DczJ4tvfWWtdzEZZfZXQ0AAHBHWprVED94UBo71po+/ZFHpA4d7K4MgD+bPNn67FC5svTOO9I/SwgAnsQ3fEBhSk2VunSRevemKQ4AgDc6ftz6UPa//0k//CA9/7xUrJi0apU0c6bUowdNcQDIycGD1gXAGzdaa4nTFAcAwDesWye1by81aSJdd5309NPSlCk0xQEUrhEjrNn4br9dmjWLpjhsw4hxoLCcboo/8IDUurXd1QAAgDNt2yZNnCj99pt0111WU+fzz6WHHpJeeEG68kq7KwQA7zVrlnVR0ZgxNMQBAPAVGRlWU2rjRmuk5ogRUvny0vz5UlHaBAAKiTHS/fdLX38tffihdNVVdleEAEfiAYUhNVXq3Fl68EFrBBoAALBfVpb1Qeydd6zp0vv2tday+vtv6b77pCpVrOZ4aKjdlQKAdzp4UHr0Ual6demLL1hLHAAAX/HHH9LDD1uDeFq1sj7/DBsmtWhhd2UA/FlmpjVoMClJWrNGKlPG7ooAGuNAgTt50mqK9+tnvdEEAAD2cjik6dOtkRDXXitNmiTFxFi3zZljjRwfNcpqkgMAcsYocQAAfE9WlvV5Z/Fi6c03pY8+khYtkj79lGmMARSugwet2fguv9w6B7FMHbwE/ycCBenkSenOO2mKAwDgDX77zRoV3rWrVKGC9NVX0tChVlP85EmpTx9p9WprlDhNcQDI2em1xDdtskaJ0xQHAMA37N0r3Xyz9efXX7eWjYqNlWbMoCkOoHAtWyZdcol0zz3WBbY0xeFFGDEOFJQTJ6yR4g8/LF13nd3VAAAQmDIzpYULpSlTpLg4qzFer57rPn/8YW1/7DGpbVt76gQAX8AocQAAfI8x0nvvWaPDX39d2rBB6t1beuMNqUYNu6sD4M+ysqTnn5fefVcaP166/Xa7KwKyoTEOFITTTfFHHrGmaAUAAJ51+LA0ebI1LWC7dtLUqVLp0tn3mznTmlZ96lSpUiXP1wkAvoC1xAEA8E1JSdbI8Pr1rc8+gwZZn4s++4w8B1C4EhKke++1Zqt4/32pZUu7KwJyRGMcyK8TJ6zp0/v3l665xu5qAAAILOvXWyMfDh6UevaUBgyQihTJvl9qqnVbVJS11nixYp6vFQB8waxZ0ttvS6NHM0ocAABfMmeOtZ742LHWTFq33CI99RTfVwIofF99Jb34opSebg1EYLk6eDEa40B+HD9ujRQfMIAroAAA8JT0dGnuXGvkd7Vq1pToF1+c+/7btlnriffvL7Vv77k6AcCXJCVZo8Rr1JA+/5xRZQAA+IqkJOuzTrVq1sjw8eOltWulTz6RypSxuzoA/iw93boAZ/9+ax3xd9+V6tSxuyrgrGiMA+fr6FFrpPjgwVLz5nZXAwCA/ztwwPqQtWyZdNNN1pp5kZFnv88nn1hXK0+ZIsXHe6JKAPA9jBIHAMD3GGNNlz55spXhMTHWd5XXX299VgoKsrtCAP5s505rEEK7dtLPP1vfu1SrZndVwDnRGAfOR0qK9UZz6FCpaVO7qwEAwH8ZI61YIb31ljUdYK9e0tChcpw6qqOnUlRJ2Rvj+1L2qWRWMUU99bwUESEtWMDU6QCQE0aJAwDgmw4csDL8kkukL76wlosaPFh6/fWzz6YFAAVh1izropyHH5bGjZM++ECqUMHuqgC30BgH8io52WqKP/ec1Lix3dUAAOCfUlKk99+X5s2TrrrKGgFRqZIkyZHqUNsZbZV0PEnLui9TfNS/I8H3Ovbq7pebatSnKbrk+bdU/JY7bHoBAODlGCUOAIDvycyU3nnHWlpq7FipcmWpd2+pYkVp4UIucgNQuE6csJaVjY62plB//nlr5gqWbYAPCba7AMCnHD4s3X679MILNMUBACgMGzZYU3F17WpNBfjFF9bFaP80xSXp6KmjSjqepB1Hdqjl9Jba69gryWqKj+rfQI9/tFeP3RalI62Y1QUAsklIkDp3ljZtskaJ0xQHAMA3/PKL1L69NavWF19Ihw5JN99szar1wgs0xQEUrs2bpRtusM47rVpJo0ZZF9vSFIePYcQ44K6//7a+QGJEBQAABSstTZozx5p6q1o1qV8/qU6dXHevFFlJy7ovU8vpLZ3N8Q/aT9YfPW9U1fQUPfpgVX1z73JViqyU62MAQMAxxjrPfvCBNGaMVK+e3RUBAAB3HDwoPfusdOqUleMlS1rTph87Zi0bVaKE3RUC8GfGSBMnSosXW+eglSulGTOspnh4uN3VAXlGYxxwR1KS1KWLtV4GXyABAFAwdu+2pvH9+Wfp1lut6bfc/FInPire2Rw323foaJtr9GkjaXOTatmmVweAgLd3r7UOaaNG1ijxonwVAACA10tJsb6L/OUXaehQqUkTa4atLl2sdX07drS7QgD+7uBBa1a/pk3/HdDwzTfSRx9JxYrZXR1wXvg0DJxLQoI1nev48VLt2nZXAwCAb8vKsq4yfvdd68ri+++3pv0LCsrzQ8VHxev9m9/X8CFN1esGaU+09MPN79MUB4DTjPl3HdJXXpEuvtjuigAAwLmkpkqTJklffin17y8NG2Z9jho92mqSz5hhLTsFAIXp66+ll16SXn5ZuvRS6Y03pC1bpKlTpSJF7K4OOG80xoGz2b9fuusua6oQvkQCAOD8HTpkfXhatEi67jppwoR8f5mz17FX3eZ2044a/27rNrcbI8YBQJJ27LBGibdqJS1cyJdXAAB4u4wMafp06cMPrXXDFy2SgoOlXbukvn3/nWXrPC4qBgC3paVJTz4pnTwpffaZFBEhvfqqtG+f9V0O5yD4OBrjQG727pW6d5feekuqUePc+wMAgOxWr7ZGOyQnS/fea414KIDmzF7HXuca49VKVdP7N79vNcn/WXOc5jiAgJWVZY3m+OYb6wusatXsrggAAJxNVpb06afSm29Kd9xhNcSLFbNmfpk2TfrkEyvbyXQAhe2336ylGvr2lW66ydo2YYK0Z481cpymOPwAjXEgJ7t2WV/ev/subzoBAMirEyeskQwzZ0p160pPPSVdeGGBPfy+lH0uTfHTTfDTa46fbo4v77FclSIrFdjzAoDX27rVugDpppuk+fP54goAAG9mjDVV8bhx0v/+Z83wEh5u3XbwoPTQQ1L9+taITWZ+AVCYjJHeflv6/HNr5ooKFaztb78t/f679PrrfLaA36AxDvzXjh3SffdZ071WqWJ3NQAA+I7Nm62ZVrZts0Y6zJ377xc7BahkSEnFFLemYT9zZPiZzfGY4jEqGVKywJ8bALxSRoY1gmPVKus8HM+MGQAAeLUVK6w1wy+/3BoRHhX1722ff241y8eOlS67zL4aAQSGv/+2Rog3aCDNm2ct4SBJU6ZIv/7K9OnwOzTGgTP9+ad0//3WVVF8mQQAwLmlpkqzZ0sff2xdUHb//VK9eoX6lFFhUVrUdZGOnjqabUR4fFS8lvdYrpIhJRUVFpXLIwCAH9m4URo4ULrrLmsaVr60AgDAe33/vdUQv/hia1BOuXL/3uZwWJkeGek6ehwACsuSJdKLL1oX4lx++b/bP/tMWr7cOk+dbpQDfoLGOHDa779LDz4ovf++VLGi3dUAAODdtm61ptTavFnq1MlqjJco4bGnjwqLyrXxzfTpAALCqVPSiBHW55jp06W4OLsrAgAAuVm5UnrpJal6dWvpxpgY19u/+cbK9eeek66+2p4aAQSOU6esZe9SUqwmePHi/962Zo31fc+nn9IUh1+iMQ5I1pf6Dz8sffghXygBAJCbU6es6dFnzJDKl5d695auuMLuqgAg8Pzyi/TEE9YsHcOGMUocAABvtWqVNGqUVLWq9Oab2b93PHbMynRJWrDAoxcbAwhQW7dKDz0kPfCAdMstrrft2SM9/rg1M2BIiD31AYWMxjiwYYM0YID00UfZr9YEAADSjh3W1cJr10o33yx98IE1vR8AwLNOnrRGkh04YM3UUbas3RUBAICc/PyzNHKktVTjxInWhcX/9d131gVuTz4ptWrl+RoBBBZjpMmTpfnzrfXDK/1ntj2HQ7rnHmtWi1Kl7KkR8AAa4whsv/5qXQHFl0oAALhKT7em03rvPal0aWt0+MiRjEoEALv88IP0zDPSo49KHTvaXQ0AAMjJmjXW56by5aXx43NervHkSWnoUGu0+Lx5XHQMoPAdPCj17WvN+jdvnlSkiOvt6enS3Xdb56+qVW0pEfAUGuMIXL/8Yl2ROXOm9YU/AACQdu+2rg7+6Ser8TJ1KlcKA4Cdjh+3vjw/ccJa5y862u6KAADAf/36q9VQKldOeuUVa6R4Tlatsr6PHDBA6tDBszUCCEyffy6NGyeNHStddln2242RHnxQ6tFDatTI4+UBnkZjHIFp1SprqqJPPuGLJQAAMjOlL76Qpk2TiheXevWShg9ndDgA2O3bb6UXXpCGDJH+9z+7qwEAAP+1YYM0YoR1MfHYsVLlyjnvl5ZmLYfy11/SrFkM0gFQ+I4ft2bLDQ21muNhYTnvN2KEVLu2tXQeEACC3d1x//79Bf7kK1asUMeOHVWhQgUFBQVp3rx5LrcbY/TMM8+ofPnyCg8PV6tWrfTnn3+67HP48GF17dpVkZGRio6OVs+ePXXs2DGXfTZs2KCrr75aYWFhio+P1+jRowv8tcCH/PCD9OyzNMWBQkRmAD7ir7+sBnibNtIff0hvvWVNnX711TTF4TFkBpADh0N64AFrmsMFC2iKA/8gMwB4jY0bpc6dpQkTpJdekiZNyr0pvm6dNTq8QQNp+nSa4h5CZiCgrV5tzQJ4883WaPHcmuIffiglJlrLNQEBwu3GeJ06dfThhx8W6JMfP35c9evX14QJE3K8ffTo0Xr99df15ptvatWqVSpevLjatGmj1NRU5z5du3bV5s2btXjxYi1cuFArVqxQ7969nbenpKSodevWqlKlitasWaMxY8bo2Wef1dtvv12grwU+Ytky6wqoTz5h/R6gEJEZgBfLypIWLZJuv9264rd78QAAWmlJREFUcrhZM2nxYumxx6SyZe2uDgGIzAD+44svpFtukbp0kV5/XSpRwu6KAK9BZgCw3ebNUteuVkaPGGFdXFylSs77pqdLzz9vNc4//NDKd3gMmYGAlJFhzU7x+uvS7NlyNGuofSn7ctw16cvZSp/9ibX8A4MjEEiMmyZMmGBKlChhOnXqZA4dOuTu3dwmycydO9f596ysLBMXF2fGjBnj3JacnGxCQ0PNRx99ZIwxZsuWLUaS+fnnn537fPnllyYoKMj89ddfxhhjJk6caEqVKmXS0tKc+wwePNjUrFnT7docDoeRZBwOx/m+PHiDRYuMueEGY44ds7sSoNB4y/mKzLD/3wDIZt8+Y4YPN+a664wZMcKYhAS7K4IX8IZzFplBZuAfBw8a0727MQMHGnP8uN3VANl4wzmLzCAzANts2WLMXXcZc++9xmzbdu79N20ypnVrY2bMMCYrq/Dr8zLecM4iM8iMgPPHH8a0aWPMBx8Yk5Vlkk8mmyvfvdJUe62a2ZO8x2XX/T8vNStqhpuWExqa5JPJNhUMWDx9znJ7xPiDDz6oDRs26NChQ6pdu7Y+++yzgurN52jnzp1KSEhQq1atnNuioqLUuHFjrVy5UpK0cuVKRUdHq0GDBs59WrVqpeDgYK1atcq5T/PmzRUSEuLcp02bNtq6dauOHDlSqK8BXmTBAumdd6SZM621UwEUKjID8BIZGVYGduokDR4sXXWV9PXX1lq1sbF2VwdIIjMAGWONIuvcWerXTxozRoqIsLsqwCuRGQA87o8/pO7dpdGjpWeekSZPli68MPf909OtkeTDhklTp1ozwDAS0xZkBgKGMdLbb0sDBlj/7dpVCgrS0VNHlXQ8STuO7FDL6S2117FXkvTX9nXa1aWdbr/+pPZkHNLRU0ftrR/wsKJ52blq1ar69ttv9cYbb+iWW25RrVq1VLSo60OsXbv2/+3dd3gU5frG8TsdAiShJaE3FURRrJxY0J8gReweKwewi4IFEDlYQNED2NuxSxFFUBREEVGK4KEIgnQUpUgASajpPfv+/hiJBJIwSXZ3tnw/15VLszs78+yQzJ3ZZ+Z93VJYSkqKJCnhqA9tExISSp5LSUlRfHx8qefDw8NVr169Usu0atXqmHUcfq5u3brHbDs/P1/5+fkl32dkZFTz3cBR06ZJ06dbHzYd8QcFAM8iMwAHbd9ufWCzfLnUs6f09tsMkw6fRmYgaCUnS4MGWXOOzp4tRUQ4XRHg88gMAF7x++9Wg9vlkh59VGrb9vivWbdOevhhqW9f62JkGuKOIzMQ8FJTrYtrO3WSZs6UQv++F7ZpTFMt7LdQF39wcUlzfHLP95V/VS/dd2meopu31sJ+C9U0pqlj5QNOqFRjXJJ27Nih6dOnq27durrqqquOCZJAMGbMGD311FNOlwF3+PBDad48678B+LMK+DoyA/CiggLrJGjyZKluXemOO6z57PgwBn6CzEBQcbmkt96SvvlGevFFex+2AyhBZgDwmJ07rfOonBzpscekk08+/msKCqwm+q+/SpMmSYmJnq8TtpEZCFhffSW9+qr00kvSaaeVuUiz2GZ/N8cPbtO2ay/R+LOlvLZWU7xZbDPv1gz4gEqlwHvvvachQ4aoa9eu2rhxoxo2bOipupT41x8QqampatSoUcnjqamp6tixY8kye/fuLfW6oqIiHTx4sOT1iYmJSk1NLbXM4e8Ty/kjZfjw4Ro8eHDJ9xkZGWrWjAOE33n3XWnlSmn8eCkszOlqgKBDZgBesnmzNV3I+vXSVVdJEydKcXFOVwVUCpmBoPLrr9Ywh1deaU13EWp7hjMAIjMAeMjevdKYMdZoLk88If31O35cP/8sPfKIdPfd1vDpXJjsU8gMBKSsLGt0ilq1pFmzpBo1Kly8WWwzfXjNhzp//Pl64hJpWz1pyTUf0hRH0LJ9Bt6jRw8NGzZM//3vfzV9+nSPhohkDXOSmJio+fPnlzyWkZGh5cuXKykpSZKUlJSktLQ0rVq1qmSZBQsWyOVyqVOnTiXL/PDDDyosLCxZZu7cuWrbtm2Zw45IUlRUlGJiYkp9wc+89prVIHj7bZrigAPIDMDDcnOt0VAuv1x6+WXpppukOXOk++6jKQ6/Q2YgaBQUSM88Y33Y/u67Uv/+NMWBSiIzALhddrbV0L7tNum666TPP7fXFM/Plx5/3LpTc8oU6YYbaIr7GDIDAenHH60LbG+4wRp56jhNcUnamb5TfWb0kWQ1xSWpz4w+JXOOA0HH2NS1a1ezc+dOu4vbkpmZaVavXm1Wr15tJJmXXnrJrF692uzYscMYY8zYsWNNXFycmTlzplm3bp256qqrTKtWrUxubm7JOnr06GHOOOMMs3z5crN48WJz4oknmptvvrnk+bS0NJOQkGD69OljNmzYYKZOnWqio6PNO++8Y7vO9PR0I8mkp6e7783Dc8aONWboUGNcLqcrAbzOV45XZIbz/wYIUGvXGjNwoDE9ehjz/vvGZGY6XRH8nC8cs8gMMiMorFhhTNeuxkyZwnkK/JYvHLPIDDIDcBuXy5ipU435v/8z5ssvK5fPy5cbc8klxnz+uefq83O+cMwiM8iMgFJQYMyIEcb07WvMwYO2X5aclmxav9ra6EmZ1q+2NkuSl5T6Pjkt2YNFA/Z4+5hluzHuCd9//72RdMxXv379jDHGuFwu88QTT5iEhAQTFRVlunTpYjZv3lxqHQcOHDA333yzqV27tomJiTG33XabyTzqQ+K1a9eaCy64wERFRZkmTZqYsWPHVqpOgsRPuFzGjBxpBQQfNiFIBfLxisxA0MrMNOa996xm+AMPGLNundMVIYAE6jGLzIDPyMoyZtAgY2691Zj9+52uBqiWQD1mkRlAEFqzxphevYwZM8aYvDz7r8vJsW7G6dePXD+OQD1mkRlwxPr1xlx6qXUxTyXsTN9ZZhP86Gb5znT3XkACVJa3j1khxhjj7rvQA01GRoZiY2OVnp7OMCS+yhjp3/+W6ta1/gsEKY5XzuPfAG5hjLRypfT++9Kff1pDpV97rVSzptOVIcBwzHIW+z/AzZsnjR5tzTvao4fT1QDVxjHLWex/wA0KCqRRo6Q//pCee05q3Nj+a5cutaZDeegh6YorPFVhwOCY5Sz2f4AoLrama1i+XHr9demIuevtSM9LV4/JPbQ3e68W9ltYak7xnek7dfEHFyu+Vrzm9J6j2Bqxbi4esM/bx6xwj28B8DSXSxo0SGrdWnrwQaerAQCg6tLSpMmTpS+/lE4/XRoyRDrpJKerAgBUxqFDVjO8dm3reF67ttMVAQCAdeus86t77pGeecb+63JyrLnEMzKkzz6zbsoBAE/bulV64AHpuuukadOkkJBKryK2Rqzm9J6jzIJMNY1pWuq5ZrHNtOjWRaoTWYemOIIOjXH4t+JiacAA6cwzpbvvdroaAAAqzxjpf/+TJkywGuO9e0tffSVFRjpdGQCgMoyRPv9cevtt6emnpaQkpysCAABFRdLzz0tr1kgffSQlJNh/7Q8/SE8+KQ0dKvXs6akKAeBvxljnE7NnS2+8IbVsWa3VxdaILbfxfXSzHAgWNMbhv4qKpLvukv7v/6S+fZ2uBgCAytm9W5o0SVqwQDr/fGnkyGqf8AAAHPLnn9LgwVK7dtLXX0tRUU5XBAAAfv3VGvq8d29r6kW7d1xmZUmPPmoNvT5jhhTL3ZQAvGDXLmngQOmSS6SZM6XQUKcrAgISjXH4p4IC6dZbpauvlm64welqAACwp6BAmjXLGi69Zk2pXz9p2DBOdgDAX7lc0vvvS9OnSy++KJ1yitMVAQAAl0t67TVp4UIrp5tW4q7IOXOs+ccffVTq2tVjJQJACWOsES0mT5ZefVVq29bpioCARmMc/icvT+rTR/rXv6SrrnK6GgAAjm/jRmn8eGnDBumKK6T33pPq1XO6KgBAdWzZIg0aZH1o/vXXUliY0xUBAIDt2615eS+/3Lrb2+5d4vv3Sw8/LNWvb01tVauWZ+sEAEnau9c6ZnXoYN1IEU7LDvA0fsvgX3JypFtukfr3l3r0cLoaAADKl5EhTZ0qffGF1KaNdPvt0hlnOF0VAKC6ioqkl16Sli2TXn+daTAAAPCC9Lx0ZRZkljkn7q6MXaoTUVuxH35qDT/8+utS69b2VmyMdd42bpw0Zox0zjlurhwAyjFjhjWP+AsvSB07Ol0NEDQYtxP+IzPTGjb9wQdpigMAfJMx0qJF0m23SX37SjEx1vC6r79OUxwAAsGaNdJll0mNGlnHd5riAAB4XHpeunpM7qGLJl6knek7Sz23M32nbnjlfK0+v7Vy0/Zbd3vbbYrv3Cldf720das0ezZNcQDekZZmfW60YoU18hRNccCruGMc/iEtTbr5ZumJJ6TzznO6GgAAStu9W/rgA+n776ULLpBGjqRZAgCBJDdXGjVK2rXLmv8vPt7pigAACBqZBZnam71X2w5t08UfXKyF/RaqWWwz7UxL1sv3n62Ry/Zp9A1NNb5/XzUNtXEfmMslvfOO1UR/8UXp5JM9/h4AQJI0d640dqz0zDNSUpLT1QBBicY4fN+BA1ZTfPRo6eyz3bZalzFKzy9SXlGxXMYoNCRE0RFhiokMV4jd+YcAAEGhzMwwxYr59huFTJkiRUdL/fpJ//63ZOeDGACA/1i0yGqKP/SQdMUVx12c8wwAgF1khj1NY5pqYb+FuviDi0ua41MvfE2pd9yo6Lhs3T+glebfvqjMYdaPsXmzNHiwNQLMrFmcvwHwjuxs6ZFHpLAw6csvpVq1Kr0KMgNwDxrj8G2pqVLv3tYcfqedVu3V5RUVa3tajvZk5Ss9v1CmjGVCQ6S4qAg1qVNDLWOjFRHGH8gAEIzKy4w6v29Wy8+mKHTrb/rjkkuVOWKsEpo1sjKDD1UAIHCkp1sXPIWEWMOmx8aWuyjnGQAAu8iMqmkW26ykOZ68f5t23XK5nrpIymzfuuQO8goVFkrPPy+tWiW99ZbUvLlX6gYALVkijRghPfqo1KVLpV5KZgDuR2Mcvmv3bqlPH+mNN6o9pFFOYZE27s/Uroy8MsPjSC4jHcwr1MG8Qm3cn6mWsdE6uX4dRYUTKAAQDMrKjPDMDDX7eqYazZ+j7OYt9cd1Nym9fYeS1+zdl0lmAEAg+fJL6dVXrakxOncudzHOMwAAdpEZ1dcstpk+vOZDnT/+fF17o6QQack1Hx6/Kb5ypTR8uDWn7/Dh1kVvAOBpeXnW+cSBA8e90PZoZAbgOTTG4Zv++EO6/Xbp3XelE06o8mqMMUrOyNWa1Ay5jDluiBzNZaTtaTnalZmrsxPjlFi7RpVrAQD4tmMywxg1+OlHtZj+iSIy07Xrsqv043/HyRVVdhaQGQAQAPbutYZXbdFC+vprqUbZx3LOMwAAdpEZ7rMzfaf6zOhjffNXb7vPjD7l3zGek2M1pfbtkz7+WGrY0HvFAghu69ZZ5xUPPCBdeaXtl5EZgOfRGIfv+f136Z57pAkTrA+kqshljFbuSdOuzLxqlWMkFRQbLd19SCfWraVTG9Zhzg4ACDBHZkaN1D1qMWOaGqxYogNnnqtN9z+s3CY25qoTmQEAfm3xYuvD8xdflDp2LHcxzjMAAHaRGe6zM31nyRzjreu21ofXfKg+M/qUzDl+THN8wQLpmWekhx+25hMHAG/55hvpnXekKVMqdUEOmQF4B41x+JZNm6T775c++khq3LjKq3EZo+W7D2lPdr4bi5N+P5StYpfR6QkxhAkABAiXMVqxPVWaNUudZs1Qcc2a2nH19dp890CpGnOGkxkA4EdcLunpp6WpUyv88IrzDACAXWSG++zK2FWqKX64CX54zvHDzfFFty5SU1dt6ZFHpKgoaeZMqU4dh6sHEFQKCqQ335Ree63STXEyA/AOGuPwHb/9ZjXFp0yR4uOrtao1qeluD5HDtqXnqEZEmNrVr+2R9QMAvOjAAaU+M1atfvpJey7ppp+ffl6FcXXdtnoyAwD8xO23S9dff9wPrzjPAADYRWa4T53IOoqvZX1WeOSd4Uc2x+OjG6ruV/Ol8ZOkUaOk8893sGIAQWvAAOm226RWrSr1MjID8B4a4/AdQ4ZIH35Y7ab4nqw8/ZGe66aiyrZpf6YSa0UprkaER7cDAPCgoiLlDLxff1x4qfbcM0jy0JWzZAYA+LgffpAOHJDuvLPCxTjPAADYRWa4V2yNWM3pPUeZBZlqGlN6mqtmsc20uMvHavD4aEV0/EOaPdu6WxwAvG3FCmnLFunaayv1MjID8K6qjw8KuNPixVLNmtUaPl2SCl0urU5Jd1NR5QuRtColTS5jPL4tAIAHuFxydemibe1O155LunusKS6RGQDg01wuaehQawj1CnCeAQCwi8zwjNgascc0xVVcLL3yihoNe1oRY56VRo6kKQ7AOcOGSTNmVOolZAbgfTTG4RtGjZLGj6/2an47mK28YpcbCqqYkZSeX6QdHr6SCwDgIS++qH0XXKzfburr0aa4RGYAgE+bPVu68EKpVq0KF+M8AwBgF5nhJatXS5ddZs0h/uWXUrt2TlcEIJgtXWoNnx4XV6mXkRmA9zGUOpy3YoXUvLlUu3pzXLiM0fZD2W4qyp6th7LVKi7aq9sEAFTTr7/KzJypnyZ8Jrm8d6UsmQEAPmjWLOmWWypchPMMAIBdZIYXZGdbd4bv3StNmiQlJDhdEQBIM2dKV11VqZeQGYAzuGMczvv8c+mGG6q9mj1Z+SrwYoNDkjIKinQor9Cr2wQAVIPLJT39tPaPeIrMAIBgV1go/fKL1LlzhYtxngEAsIvM8LBvvpGuuELq2pWmOADfUVBgTRVbycY4mQE4g8Y4nFVUJK1aJXXrVu1V7crMlWcHwz1WiKTdmQw/AgB+4/vvpehobTvlDDIDAILdN99Il1563MU4zwAA2EVmeEhqqtSnj7RggfTVV1KPHk5XBAB/+/FH6eKLK/0yMgNwBkOpw1mffFLpK6nKcyi3UN69vsqam+NgLldYAYBfyM6WnnxS+uwzHcoqIjMAINh9/LE0fPhxF+M8AwBgF5nhZi6XNH68NG2a9OyzUseOTlcEAMd66y3psccq/TIyA3AGd4zDWVOnSldeWe3VFBa7lFNU7IaCKi8tv1DGeDvCAACV4nJJ11wjPf64Chs0JDMAINjt2yft3i2ddlqFi3GeAQCwi8xws19+sYZNz86WZs+mKQ7AN6WmSikp0imnVOplZAbgHO4Yh3N+/VWKiZFatKj2qjILitxQUNUUuYzyi12qER7mWA0AgONwuaw5n7p3V2ZugWNlkBkA4CN++km66CIppOLBCznPAADYRWa4SX6+NGaMtHGjdRdm8+ZOVwQA5Vu40Jom9jjnFUcjMwDncMc4nPP667aGLrSj2OErnIpdXGEFAD5t61bphBMkkRkAAEmTJ0t33nncxcgMAIBdZIYb/PCDdNllUocO0qef0hQH4PvGjZMeeqjSLyMzAOdwxzicsWePlJxc6SFGylO567Hcr5IXhAEAvG3zZqllS0lkBgAEvfR0adcuqVmz4y7q9CGbzAAA/+H0IduvM+PQIenf/5bCw6Xp06XYWKcrAoDjW7dOSkiQatas9EudPmT7dWYA1URjHM545RWpf3+3HYHDQp0d/CAshMEXAMCn7d8vNWggicwAgKD31VfSpZdKYccfOpDMAADYRWZUgTHS1KnWHZejRknnned0RQBg37hx0r/+VaWXkhmAc/jph/elplpXU/Xq5bZV1ol0bj6MiNAQRYZxiRUA+LRt26T27SWRGQAQ9GbNkq6+2taiZAYAwC4yo5K2bJGuuUbavl2aPZumOAD/sn+/tGaN1L17lV5OZgDO8fnGeMuWLRUSEnLM14ABAyRJF1988THP9e/fv9Q6kpOT1atXL0VHRys+Pl5Dhw5VUVGRE28HkvTyy26bW/yw8NBQ1YpwJkzq1ohQCGOPAD6BzEC51q615qkTmQHAQmYEqbw8ad8+6dRTbS1OZgCQyAzYQ2bYlJ9v3R3+yCPSSy9Jjz4qRUY6XRXgNmRGkPjiC+nWW6v8cjIDcI7PD6X+008/qbi4uOT7DRs26NJLL9X1119f8thdd92lUaNGlXwfHR1d8v/FxcXq1auXEhMTtXTpUu3Zs0d9+/ZVRESERo8e7Z03gb9t22bN8+qBfV+vRoRyCotl3L7m8oXIChIAvoHMQLmysqSoqJJvyQwAZEaQ+ugj6Yh/YzvIDABkBuwiM45jwQLpP/+R7r1XeuIJJrlFQCIzgsRHH1kjUVUDmQE4w+cb4w0bNiz1/dixY9WmTRtddNFFJY9FR0crMTGxzNd/99132rRpk+bNm6eEhAR17NhRTz/9tIYNG6Ynn3xSkVyR6F2PPmr94euBOTSaxtTUzsw8t6+3Iuav7QLwDWQGypSfLxUVSUecaJIZAMiMIDV9uvTf/1bqJWQGADIDdpEZ5UhNlYYNk+LipBkzpJgYpysCPIbMCALvvSdddplUu3a1VkNmAM7w+aHUj1RQUKCPPvpIt99+e6mhHiZPnqwGDRro1FNP1fDhw5WTk1Py3LJly9ShQwclJCSUPNa9e3dlZGRo48aNXq0/6G3dKtWpI515pkdWn1grSlFh3v2RrlsjQrFRXGEF+CIyAyWys6WjTkzJDABHIjOCxMaN1ughrVtX6mVkBoAjkRmoCJlxFJdLeucdqW9f6YEHpFdeoSmOoEJmBKCsLOtu8aOGv68KMgNwhs/fMX6kL774Qmlpabr1iLkbbrnlFrVo0UKNGzfWunXrNGzYMG3evFnTp0+XJKWkpJQKEUkl36ekpJS5nfz8fOXn55d8n5GR4eZ3EqTGj7f+EPaQkJAQtakbrU37szy2jaO1iYs+/kIAHEFmoMTq1VL79qUeIjMAHInMCBKffCLddVelX0ZmADgSmYGKkBlHWLtW+ve/pSuukGbPlsKcmUsXcBKZEYA+/VS64Qa3XORDZgDO8KvG+Lhx49SzZ081bty45LG777675P87dOigRo0aqUuXLtq6davatGlTpe2MGTNGTz31VLXrxRFycqSVK6VnnvHoZk6sW1t/pOUqp6j4+AtXw+G5OJox7Ajgs8gMlNi1Szri5+AwMgPAYWRGECgulhYtkkaOrNLLyQwAh5EZOJ6gz4ysLOnJJ63h08ePlxo1croiwDFkRoDJy5M+/FD69lu3rTLoMwNwgN8Mpb5jxw7NmzdPd955Z4XLderUSZK0ZcsWSVJiYqJSU1NLLXP4+/Lm8Rg+fLjS09NLvnbu3Fnd8vHee1K/ftIRQ8Z4QlhoiM5qFOvRbRx2VmJcqSFwAPgOMgOlpKRIR11tLZEZACxkRpB44QXp1lurfLcamQFAIjNgT1BnxhdfSFdeKXXrZjWPaIojiJEZAWjIEGn4cMmN87wHdWYADvGbxviECRMUHx+vXr16VbjcmjVrJEmN/vrDKykpSevXr9fevXtLlpk7d65iYmLU/qhhVQ+LiopSTExMqS9Uw8GD1pBJN93klc01jI7SiXVreXQbp8XHqE6UXw24AAQVMgOlbN4snXlmmU+RGQDIjCCQni5984103XXVWg2ZAYDMgF1Blxk7dkjXX29NY/X111ZjHAhyZEaAcbmkrVs9cnwLuswAHOYXvwkul0sTJkxQv379FB7+d8lbt27Vxx9/rMsuu0z169fXunXrNGjQIHXu3FmnnXaaJKlbt25q3769+vTpo+eee04pKSl6/PHHNWDAAEVFRTn1loLLww9bQxaGeu86jFMb1lF+cbGSM/Lcvu629WurjYeDCkDVkRk4xrZtUjlXVUtkBhDMyIwgsXy51KOHW+YBJDOA4EVmoLKCIjMKC6WXX5YWL5aef15q29bpigCfQGYEoBUrpHPP9djqgyIzAB/hF3eMz5s3T8nJybr99ttLPR4ZGal58+apW7duateunYYMGaLrrrtOX331VckyYWFhmjVrlsLCwpSUlKR//etf6tu3r0aNGuXttxGcJk6UWrSQzjvPq5sNCQnRWYlxah0X7db1tm9QR+3r13brOgG4F5mBUoyxhrgKL/9aQDIDCF5kRhA4eFAaM0Y6zhCWdpEZQPAiM1BZAZ8ZS5ZIPXtKTZpIM2fSFAeOQGYEoE8/la65xmOrD/jMAHxIiDHGOF2Er8vIyFBsbKzS09MZhqQy5s2T3n5b+uSTKs/l5w5/ZuVp1Z40FbmMqvrDXjM8VOc2qqv60e6bPwTwBI5XzuPfwMfMn28Nn/vCC7YWJzMQbDhmOYv97wVz50obNkiDBrl91WQGgg3HLGex//1bQGXG7t3SiBHWBcijR0t16zpbD3wSxyxnsf/dLDtb+uc/rc+XvCCgMgOwwdvHLL8YSh1+6IcfpNdec7wpLkmNa9dQ/VYNtflgtran5ai4EteCRISGqE3dWjqpXi2Fe3EoeACAm2RlWXcw2ERmAECA+fln6R//8MiqyQwAgF0BkRnbt0v//a+0ZYv01FNSx47O1QIA3jRxovSvf3ltcwGRGYAPozEO91u2THr2WWt4kZo1na5GkhQVHqbT4mN0coPaSk7PVUpWng7mFarQdWyoRIWFql6NCDWuU0NN69RUWGiIAxUDANzi2WelWbMq9RIyAwACyKpV0r33emz1ZAYAwC6/zAyXS1q6VHr9dSkqShowQOrUyZlaAMAJ6enSjBnSt996dbN+mRmAn6AxDvdatcq6avTTT6VatZyu5hgRoaFqU7eW2tStJWOMcoqKlVfkkssYhYaEKDoiTDXDnb3DHQDgJgcOSPHxUr16VXo5mQEAfq6gwBr20AtDsZEZAAC7qpUZubnWnYvz5knnny/17m2d84S4uQmyZ8/f2znvPOmllyo1EhcABIwRI6Thwx0bFZfzDMD9aIzDfdats0Li00+98uFTdYWEhKhWRLhqRThdCQDAI+67Txo50i2rIjMAwA8tWiRddJHXN0tmAADssp0Zhw5Jb70lLVgg9esnvfyytGKFdMcdUmGh1Lmz1K2bdPbZVWuSZ2dLa9dKX38trVwpJSRIffpIQ4ZYc4kDQDCaOVMyRurSxelKJHGeAbgLjXG4xy+/SIMHW3OKx8U5XQ0AANbJS7t2TlcBAHDKxInS6NFOVwEAQNXt3m01wX/5xbrwd/jwvxvfzZtL//yntczSpdLHH0vDhlkjOCYmSieeKJ10klS7tnVuFBNjDY3+55/SwYPWCFsbN0p791rDpJ9zjnTlldITT0g1ajj7vgHAabNnS5MmWcdWAAGFxjiq7/ffrTmGpkyR6td3uhoAACzR0VJmplSzptOVAAC8bckS6w63Fi2crgQAgMr79VfpxRetuW0fesgazrw8TZpI119vfUlWw3vvXmnnTquhnpUlhYdb/w0JkZo2lWJjpdatpbvusm5wcWiIYADwSePGWSN0TJ5sXTgEIKDQGEf1bN8u3XOP9OGH1jBLAAD4grQ0606IBg2crgQA4G3btllTacyY4XQlAABUzo8/WneI165tDWPevn3l11G/vvV18snW8OoAAHuMsUbNKCiw+h2hoU5XBMADaIyj6nbtkm6/3RqisEkTp6sBAOBvo0ZJjz7KSQwABJvkZOnOO61zlDp1nK4GAIDjM0aaM0d6801r+PMXXpCaNXO6KgAILvn51g2AnTpJ997rdDUAPIjGOKpmzx6pb1/p/fcZnhAA4FvWrZNSU6WLL3a6EgCAN/32m9S/vzX0YfPmTlcDAEDFioqkTz+VJkyQLrpI+uADqV49p6sCgOBz6JDV67jnHunyy52uBoCH0RhH5e3bJ/3rX9Jbb0lt2jhdDQAAf8vLkwYPlj7+2OlKAADeNHOm9Pbb1pCHjGYFAPBlOTnS+PHWlB/XXmtlWHS001UBQHD64w/pjjuk556TzjrL6WoAeAGNcVTOwYPSLbdIr74qtW3rdDUAAJT28MPSgw9K8fFOVwIA8IbiYmsewMxMq7EQGel0RQAAlO3gQemNN6QffrCmJvz2Wymcj2YBwDErV0rDhlkXKzEqLhA0+OsL9h06JN10k3X11KmnOl0NAAClvfWW1RC/4gqnKwEAeMP+/dZ84tddJ/Xp43Q1AACUbedO6aWXpC1bpAEDpMcfl0JCnK4KAILbrFnSO+9In30m1a3rdDUAvIjGOOxJT7ea4mPGSGec4XQ1AACU9umn0vLl1vx8AIDAt3Kl9MgjVqOhY0enqwEA4Fjr1kkvvyzl5kqDBkmdOjldEQBAsm6sWL7caopHRTldDQAvozGO48vIkG68UXr6aebZAAD4nqlTrSt9J07kzgsACHTGSP/9rzUM7eefc3cHAMC3GCMtWGBlVWKiNHy4dNJJTlcFAJAkl8saOr1GDevGCj5DAoISjXFULCvLulN85Ejp3HOdrgYAgL8ZY41ksmuX1RRnfj4ACGzp6dK990pnn22NFMIHWQAAX1FUJE2bZjVa/vEPa3je+HinqwIAHJaXJ91xh3TJJdZ/AQQtPkFG+bKzrab4o49KSUlOVwMAwN/275cGDrTy6Y03aI4AQKD7+Wfp4Yel//yHcxMAgO/IypLGjZO+/FK65hppxgypVi2nqwIAHOnAAalPH+nBB6Xu3Z2uBoDDaIyjbDk50s03S0OHShdc4HQ1AABYjLGGTh83TnruOenMM52uCADgScZYcwAuWGDdiVe/vtMVAQAgpaRIr78urVwp3Xmn9N13UliY01UBAI62dat0113Syy9Lp5/udDUAfECo0wXAB+XlSbfcIj30kHTRRU5XAwCAZelSqWdPac8e6euvaYoDQKDLyLDu7MjOtoZOpykOAHDar79Kd99tjV7VrZs0Z450/fU0xQHAF/34o3TPPdKkSTTFAZTgjnGUlp9vNcXvu8+abwMAAKdt3y49/rjVEPnoI6lBA6crAgB42po10pAh0qhR0vnnO10NACCYGSMtWSK99poUEyMNGiSdcorTVQEAKvL551ZD/PPPpdhYp6sB4ENojONvBQVS797W0CLdujldDQAg2B06JI0ebTXGn3lGatfO6YoAAJ5mjPTuu9K330qffMLFUAAA5xQXS198Ib3/vnWn4SuvSI0bO10VAKAixkjPPy9t2yZ99pkUEeF0RQB8DI1xWAoLrWEK+/WzhqkFAMApeXnWfH3z5knDh0sXX+x0RQAAb8jMlAYMkNq3tz7ECmXmLwCAA3JzpYkTrSzq1cuazqNOHaerAgAcT2GhNdXFiSdKb70lhYQ4XREAH0RjHFZg9Osn3XyzdMUVTlcDAAhWxcXS5MnWh1D33CN98w1NEQAIFuvWSQ89JD31lHThhU5XAwAIRvv3S2+8IS1ebH1ONmcOdxoCgL9IS5P69pVuvVW69lqnqwHgw2iMB7uiIun2262wuPpqp6sBAAQjY6TvvpNeeMG6QGvOHCky0umqAADeYIw0bpz09dfW0OkNGzpdEQAg2GzdKr38srRzp3Wn4YgR3GUIAP5k+3bpzjulsWOlc85xuhoAPo7GeDArLpbuuEO6/HLpn/90uhoAQDD6+Wfr7sAOHaTPP5diYpyuCADgLVlZfw91+PnnjBICAPCuFSusecMjIqRBg6SOHZ2uCABQWT/+KD3+uDR+vNSihdPVAPADNMaDlcsl3X231K2bdOONTlcDAAg227dLI0dKNWtKb74pNWnidEUAAG/asEF68EHrrryLLnK6GgBAsCgulmbNkt57TzrpJOvuwubNna4KAFAV06ZJH39sXWQbG+t0NQD8BI3xYORySf37S507S717O10NACCYHDgg/ec/1jCFTz0ltW/vdEUAAG+bMEH64gvrQ6yEBKerAQAEg+xsaeJEafp06bLLpMmTaaIAgL8yxrqwaedOqzkeTpsLgH0cMYKNMdKAAVKnTlK/fk5XAwAIFrm50quvSgsXSo89Jl14odMVAQC8LTtbuv9+qWVLqzERFuZ0RQCAQPfnn9J//2tN4XTrrdKcOdbQ6QAA/1RQYPU3Tj5ZeuMNKSTE6YoA+Bka48HEGOmBB6w5k+64w+lqAADBoLhYmjRJ+vBD6b77pGHDOGkBgGC0aZN1LvLoo9IllzhdDQAg0K1da80ffviirP/8h/MQAPB3hw5JfftavY2rr3a6GgB+KtTpAiry5JNPKiQkpNRXu3btSp7Py8vTgAEDVL9+fdWuXVvXXXedUlNTS60jOTlZvXr1UnR0tOLj4zV06FAVFRV5+604zxhp8GCpXTvpnnucrgYA3I7M8DHGSLNnS927W3eLf/ut9M9/8mEUAJ9AZnjZpEnS8OHSRx/RFAfgd8gMP+JyWecgV11lzSH+6KPSp59ao1VxHgLAC8gMD9q2zfpc6cknaYoDqBafv2P8lFNO0bx580q+Dz9ivohBgwbp66+/1rRp0xQbG6uBAwfq2muv1ZIlSyRJxcXF6tWrlxITE7V06VLt2bNHffv2VUREhEaPHu319+IYY6RHHrGGLBwwwOlqAMBjyAwf8dNP0qhR0plnSjNmSHXqOF0RAByDzPCCnBzpwQelJk0YOh2AXyMzfFxurnXx1SefSF26SBMmSPXqOV0VgCBFZnjA0qXSiBHSxIlSs2ZOVwPA3xkfNnLkSHP66aeX+VxaWpqJiIgw06ZNK3nsl19+MZLMsmXLjDHGzJ4924SGhpqUlJSSZd566y0TExNj8vPzbdeRnp5uJJn09PSqvREnuVzG/PvfxrzwgtOVAPACvz5eVROZ4QO2bDHmlluM6d/fmD//dLoaADYE6zGLzPCCTZuM6dLFmLlzna4EgJsE9DGrAmSGD0tNNWbkSGMuvdSYDz4wphL7E4BnBesxi8zwgI8/Nuaaa4zJyHC6EgAe4u1jlk8PpS5Jv//+uxo3bqzWrVurd+/eSk5OliStWrVKhYWF6tq1a8my7dq1U/PmzbVs2TJJ0rJly9ShQwclJCSULNO9e3dlZGRo48aN3n0jThk5UoqNlYYMcboSAPA4MsMh+/b9PW/sE09Ib70lNWrkdFUAUCEyw4M++sgaserDD6Uj9iMA+Csyw8ds2iTdfbd0773WMOnffmvNORsZ6XRlAEBmuIsx0jPPWHeLf/opoxECcBufHkq9U6dOmjhxotq2bas9e/boqaee0oUXXqgNGzYoJSVFkZGRiouLK/WahIQEpaSkSJJSUlJKhcjh5w8/V578/Hzl5+eXfJ+RkeGmd+Rlo0ZZJwX//neFi2UXFCk1J1+H8gp1MLdQeUXFchkpNESKjghT/ZqRqlsjQgm1olQjnOEPAfgmMsM7jsyMtANpajLubdVd9ZN+v/dB5f8jycqM9BwyA4BPIzM8JDdXeughKT5emjFD2S4pNS2b8wwAfo3M8I7jfjZVI0KNli9Wg4njFBYfLw0aJJ18stNlA0ApZIabFBRI/ftLp58uPfaYFBJS6mn6GQCqw6cb4z179iz5/9NOO02dOnVSixYt9Omnn6pmzZoe2+6YMWP01FNPeWz9XjF6tFRcbM29UQZjjPblFGjLoWylZFuhGSLJHLFMsZHS84uUkV+kbX893yymhtrUra26NSI8/Q4AoFLIDM85OjNCiorUYsYn6jDrC23tc7s23f2AdZJCZgDwE2SGB2zeLA0cKPPww9p3wcXakpLBeQaAgEBmeI6dz6ZMfr5iP5+pxl99rv1nnquVQ0cpoVVTKzMcqRoAykdmuMHBg9YoIHffLV15ZcnD9DMAuIvPD6V+pLi4OJ100knasmWLEhMTVVBQoLS0tFLLpKamKjExUZKUmJio1NTUY54//Fx5hg8frvT09JKvnTt3uveNeNrzz0vZ2dKTT5b5dF5RsZbuPqTFuw4qNfvvK8lMmUv//biRtDMjT9/v2K/VKWkqcrncWTUAuBWZ4R6lMiMrT40WfKvz77xFIcUuLRk3RXu69ix15S6ZAcAfkRnVNGWKNGSI8saN19JTzuE8A0BAIzPc43ifTUUeOqi2b7+q8+7po9CiQi196wP9OnCI8us3IDMA+A0yo5K2bJH++U9rJNwjmuL0MwC4k181xrOysrR161Y1atRIZ511liIiIjR//vyS5zdv3qzk5GQlJSVJkpKSkrR+/Xrt3bu3ZJm5c+cqJiZG7du3L3c7UVFRiomJKfXlN15+Wdq/35p/46ghRiRpT1ae5m7fp71/BUh54VGew8tvT8/VvD/261BeYfXqBQAPITOq78jMqLtmlZL691XsLxu17I0J2n5TH5nwigeeITMA+Asyo4ry8qT77pM2bNCej6Zqbn4E5xkAAh6ZUX0VfTZV649tOn3UcJ352GClte+gxeOm6o/re8sVVaNkGTIDgL8gMyph8WLp3nulSZOkM88seZh+BgB38+mh1B9++GFdccUVatGihf7880+NHDlSYWFhuvnmmxUbG6s77rhDgwcPVr169RQTE6P7779fSUlJ+sc//iFJ6tatm9q3b68+ffroueeeU0pKih5//HENGDBAUVFRDr87D3j9dWnXLumFF8psiien52hlSrrbNpdbWKwfkvfr/Kb11SA60m3rBYCqIDPc63Bm1N6+VR1ff0EFcXW16j8vKb9Bwyqtj8wA4EvIDDf4/XerKT5kiJKTOnOeASBgkRnuVeZnU8ao/srlajN5goqio7Wl313KaFt+A+hIZAYAX0JmVNHkydKMGdZX7dolD9PPAOAJPt0Y37Vrl26++WYdOHBADRs21AUXXKAff/xRDRtaH8q//PLLCg0N1XXXXaf8/Hx1795db775Zsnrw8LCNGvWLN17771KSkpSrVq11K9fP40aNcqpt+Q5b79tDTXyyitlNsX/zMpza4hI1tVWxUZasuugLmpeX3HM0wHAQWSG+/yZlaf1G7bo9DdfVmR6mjY9MFTZLVtXa51kBgBfQmZU0yefSB98IE2YoD/jGmjl7kNuXT2ZAcCXkBnuc/RnUyGFhWoy5yu1+GKaDnXoqHWPjlJefEKl1klmAPAlZEYlGWMNm37woHWOERZW8hT9DACeEmKMqezoE0EnIyNDsbGxSk9P981hSN5/X1qzxrpjvIymeE5hseZu36tiD/1Lh0iqER6qS1s1VHioX43ODwQcnz9eBQF//zfIScvQjhFPq+661frl3od0qONZbl0/mQH4Fn8/Zvk7v9v/eXnSkCFSnTrS008rR6GcZwBBxO+OWQHG3/f/kZ9NhWdlqtUnHyrhfwu1u3svJV91vYqjo6u1fjID8C3+fszyd36x//PzpXvukc46S7r//lJP0c8Agou3j1n81vu7iROllSul114rsylujNHqlHS5PHj5g5GUW+TSL/uzPLcRAIDHmWXLVNCrl9JOPkVL357k9qa4RGYAgN/avVu66irpssuksWNlwsM5zwAA2HLkZ1PNvpquTg/cpayWrbV4/FRtv7lftZviEpkBAH4lJ0e64Qbpn/88pilOPwOAp/n0UOo4ji+/lBYulMaPl8q5sml3Zp5Sc/K9Us7vh7LVLKYmQ5AAgD/KzVX+EyO0bPQryk1s7PHNkRkA4Gf69JHeeUc68URJnGcAAOw7nBlxG9ep9eQJWvTxzHI/x6ouMgMA/MDbb1uN8csvP+YpzjMAeBp3jPuzF16Q3n23wpOJzQe9d9VTiKTfD3GVFQD4pRtu0Oab+nmlKS6RGQDgV778UmrevKQpLnGeAQCwb/PBLEUd2K/TRo/Q8tfe91hTXCIzAMDnpaRIX38tde5c5tOcZwDwNBrj/mrJEqlpUykystxF0vIKlZ5f5LWSjKTdGXkqKHZ5bZsAADdYtkwFUTW09cIuXtskmQEAfsIYafRo64Lcv3CeAQCw63BmnPLif7T57vuVF5/g0e2RGQDg4yZNkh54QGrW7JinOM8A4A00xv3V2LHSm29WuMiO9BwdO+u4Z7kk7crI9fJWAQDVMm6cknvfSmYAAI713XfSeeeVuiCX8wwAgF070nNUd/1aRR04oNSLvHMhLpkBAD5q+3bp22+lCy4o82nOMwB4A41xf7Rpk1RUJNWpU+Fi+3IKZLxU0mEhkg7kFnh5qwCAKjt4UNqxQ390PJfMAAAca948qUvpRgbnGQAAu/Zl5+uEie/otzvv89o2yQwA8FErVkj//KdUv36ZT3OeAcAbaIz7oy++kB58UAoLK3cRlzHKLPDesCOHGUkH8wq9vl0AQBU995xcgwaRGQCAY+XkSMuWSb16lTzEeQYAwC6XMdLGjQopKtKBc/7hte2SGQDgo/73P+nss8t8ivMMAN5CY9wf/fab1KJFhYtk5hd5/eqqw7ILi1XscmrrAADb9u+Xli1TZqfzyAwAwLEeeUR6/PFSD3GeAQCwKzOvUKe8OFrrHxnh9W2TGQDgg3bvlhITy3yK8wwA3kJj3B9lZ0u1alW4SIHL5aViylbo8PYBADY8/rh0990qqFnT0TLIDADwUfv2Se3bl3qI8wwAgF1m/jzlJjZSbpOmjmyfzAAAH1JcLKWkSE3LzgTOMwB4C41xf7Rvn9SkSYWLOH2Bk9PbBwAcR1aWtG2b1Lu348dsp7cPAChHGecdTh+znd4+AMC+6GfHamvfOx3bPpkBAD4kK0uKj5dCQsp82uljttPbB+A9NMb9TXGx9d8K5heXpLCy88VrQh3ePgDgOEJCpFDrzwAyAwBwDJdLyssryYrDyAwAgC3PPaeizp2V2eZEx0ogMwDAhyxdKnXsWO7TnGcA8JZwpwtAJe3ebV1ZdRw1IypunHtSaIgUFcY1FwDg06KipJwcSWQGAKAMGRlSTMwxd3SQGQAAW2bNkvlmjrQr3ZHNkxkA4GO2bJFaty73ac4zAHgLv+3+JivruPOLS1J0eJjCHbrMKTYqQiHlDIkCAPARhYXSX3OLkxkAgGPk55d53kFmAABsqVVL0cZFZgAALNu2SSedVO7TnGcA8BYa4/7mjz+kdu2Ou1hISIjioiI8X8/R25VUt4b3twsAqKSiIslYEyiRGQCAY/z6q9S+/TEPkxkAAFvi4xXy559kBgDAsmGDdPbZ5T7NeQYAb6Ex7m/Wri3zA6qyJNaK8nAxxzKS4h3YLgCgkkJDS80bS2YAAErZvl1q0qTMp8gMAMBxRUdLublkBgBAKiiQcnOl8Ipn9iUzAHgDjXF/8+efUkKCrUWbx9aUtwcAiQoLdSTAAACVFBlZMse4RGYAAI5SwRROZAYA4LjCw6WCAjIDACDt2CE1ayYdZ7hyMgOAN9AY9zcHD0p169patEZ4mBrVruG1MAmR1CouWqHMxwEAvi8vT6pdu+RbMgMAUMqff0otW5b5FJkBADiuli2lPXvIDACA1Rhv0+a4i5EZALyBxri/SU2VWrWyvfjJDWoffyE3CQ8NUZu4su8qAQD4GJfL+joCmQEAKLFpk9SuXblPkxkAgAolJkrJyZLIDAAIeitWSBdeaGtRMgOAp9EY9yculzUXR1iY7ZfERkWobX3vhEnHhFhFhfMjBQB+ITJSKi4u9RCZAQAokZMjRZU/pCCZAQCoUESEVFQkicwAgKC3YoV06qm2FiUzAHgav/X+ZO9eqWHD487FcbS29WqrdmSYx4YgCZGUWCtKTevU8NAWAABuFxpa8kHVkcgMAICMkTIzpTp1KlyMzAAAlKtWLesiq7+QGQAQpIyR0tKkuDjbLyEzAHgSjXF/cuiQVL9+pV8WFhqiC5rWV2RYqNvDJERSTFS4zmkcpxDm4gAA/5GXJ0VHH/MwmQEAkMtlZcRxjtVkBgCgXK1bW3PK/oXMAIAgVVhonVfUsj9kOZkBwJNojPuTVauk886r0kujI8J0UfP6inJzmMREhevCZvUVEcqPEgD4FWPKfYrMAIAgl5wsJSTYWpTMAACUKS5O2rev1ENkBgAEofXrpTPOqPTLyAwAnsJvvz/58UfptNOq/PLakeG6pGUDJdQqf67AymgdG62LmjdQZBg/RgDgd6KjSw1teDQyAwCC2KFDUt26thcnMwAAxwgJsUYgOQqZAQBBZsmSKjXGJTIDgGdwBPAnyclS06bVWkWN8DAlNamrMxNjFRFauWutDi9dMzxU5zetp46JsQqv5DoAAD6iqEiqUfFcSmQGAASpmjWlSt5BQWYAAEpp2NCavqkMZAYABJFff5Xatavyy8kMAO4W7nQBsMnlkrKzpUaNqr2qkJAQtYyNVrM6NbUrM1e/H8xWRkGR9VwZyx8ebLd+zQidULe2GtWOYv4NAPB3OTlSZORxFyMzACAIffWVdP75lX4ZmQEAKLFhg9SqVblPkxkAECR27JDOPLNaqyAzALgTjXF/sXy5dO65bl1lWGiIWsRGq0VstPKKinUor1CH8gqVV1Qsl5HCQkIUHRGmujUiFFcjgiFGACCQbNkitW9ve3EyAwCCyJ490uWXV/nlZAYAQC1bSnv3HncxMgMAAti+fVJhoRQW5pbVkRkA3IHGuL+YN09KSvLY6muEh6lR7TA1ql3xsLoAgABx4IA1z3gVkBkAEMCys6XVq62GhhuQGQAQpCZOlK67rlIvITMAIMCsWSN17+6RVZMZAKqKS2b8xbJlUufOTlcBAAgU06ZJffo4XQUAwNekp0utW1f54ikAACRJ9epJRUVOVwEAcNLChdIppzhdBQCU4tON8TFjxuicc85RnTp1FB8fr6uvvlqbN28utczFF1+skJCQUl/9+/cvtUxycrJ69eql6OhoxcfHa+jQoSrypz/OCwul8HApLs7pSgDAZ5EZlWCMNZxVTIzTlQCAI8iMCkyZUqX5xQEgUJEZVTRpknTllU5XAQBeRWYc5aefPHbHOABUlU8Ppb5o0SINGDBA55xzjoqKivToo4+qW7du2rRpk2rVqlWy3F133aVRo0aVfB99xN0NxcXF6tWrlxITE7V06VLt2bNHffv2VUREhEaPHu3V91NlERFWEwMAUC4yoxK2bJEaNrTu4gCAIERmVGDRImn6dKerAACfQWZUUUKClJfndBUA4FVkxlFq1JBCQpyuAgBK8enG+Jw5c0p9P3HiRMXHx2vVqlXqfMSw4tHR0UpMTCxzHd999502bdqkefPmKSEhQR07dtTTTz+tYcOG6cknn1RkZKRH34PbECAAUCEyoxLatLHuGAeAIEVmVCAx0cqIRo2crgQAfAKZUUW1akk5OU5XAQBeRWYAgO/z6aHUj5aeni5JqnfUHW6TJ09WgwYNdOqpp2r48OHKOeIP72XLlqlDhw5KSEgoeax79+7KyMjQxo0bvVM4AMDryIwKhIYy3x8AHIHMOEJiopSS4nQVAOCzyAwbCgqkHTuk1q2drgQAHBX0mcEouAB8kE/fMX4kl8ulhx56SOeff75OPfXUksdvueUWtWjRQo0bN9a6des0bNgwbd68WdP/Gv4vJSWlVIhIKvk+pZwPfPLz85Wfn1/yfUZGhrvfTuXk5kpcCQYAtgV1ZtixYwd3AgLAX8iMo9SpI2VmOl0FAPgkMsOmmTOlyy93ugoAcBSZIUbBBeCT/KYxPmDAAG3YsEGLFy8u9fjdd99d8v8dOnRQo0aN1KVLF23dulVt2rSp0rbGjBmjp556qlr1utXOnVLz5k5XAQB+I6gzw4433pCO2BcAEMzIjKNkZUlRUU5XAQA+icywadIk6cMPna4CABxFZog7xgH4JL8YSn3gwIGaNWuWvv/+ezVt2rTCZTt16iRJ2rJliyQpMTFRqamppZY5/H1583gMHz5c6enpJV87d+6s7luoni1bGH4KAGwK+sw4nvR0acMG6bzznK4EABxHZhylsFD63/+ks85yuhIA8Dlkhk1Ll0onnSTFxTldCQA4hsyQlJ0t1azpdBUAcAyfbowbYzRw4EDNmDFDCxYsUKtWrY77mjVr1kiSGv01RGxSUpLWr1+vvXv3liwzd+5cxcTEqH379mWuIyoqSjExMaW+HLV4sfSPfzhbAwD4ODLDphdekO6/3+kqAMBRZEY5Jk6UeveWwv1mYDEA8DgyoxKMkcaOlYYOdboSAHAEmXGE6dOl//s/p6sAgGP49CceAwYM0Mcff6yZM2eqTp06JXNoxMbGqmbNmtq6das+/vhjXXbZZapfv77WrVunQYMGqXPnzjrttNMkSd26dVP79u3Vp08fPffcc0pJSdHjjz+uAQMGKMofhghMT5d++kl65hmnKwEAn0Zm2LBzp7R+vTRqlNOVAICjyIwyZGVJU6ZI337rdCUA4FPIjEqYMcMadaScOxoBINCRGX8pKJAmTJC++cbpSgDgWMaHSSrza8KECcYYY5KTk03nzp1NvXr1TFRUlDnhhBPM0KFDTXp6eqn1/PHHH6Znz56mZs2apkGDBmbIkCGmsLDQdh3p6elG0jHr9YpBg4xZsMD72wXglxw9XjmMzDgOl8uY664zZuNGpysB4EN89pjlYWRGGR5+2Jg5c5yuAoAP86ljlheRGTalpxvzf/9nTG6u05UA8AE+f8zyEDLjL2PGGDN5sjPbBuB3vH3MCjHGGM+13QNDRkaGYmNjlZ6e7t1hSNassYa8/egj720TgF9z7HiFEj77bzBunHTwIMMaAijFZ49ZQcJn9v+PP0pvvilNmuRcDQB8ns8cs4KUT+9/Y6S+faX+/aXzz3e6GgA+wKePWUHA0f2/ZYv12dP06VJIiHe3DcAvefuY5dNDqQe1nBxpyBBp6lSnKwEA+LtVq6SvvpI+/9zpSgAAvubQIenRR60PrgAAqIp33pHataMpDgDBLj9fGjDAygWa4gB8FI1xX2SMdP/90rBhUsOGTlcDAPBnu3dbV+p+9pkUFuZ0NQAAX1JQIN12m/Tcc1JcnNPVAAD80TffSEuWSB984HQlAACnDRliNcZbtnS6EgAoF41xXzR2rNShg9Stm9OVAAD8WUqK1KeP9O67Ur16TlcDAPAlxcXSXXdJt98unX2209UAAPzR999bU3F89pkUGup0NQAAJz33nJSQIF15pdOVAECF+KvV10ybJu3aJT34oNOVAAD82fbt0i23WB9UnXCC09UAAHxJbq6VEd268cEVAKBqZs+WXn1V+uQTKSrK6WoAAE4aN07680/p8cedrgQAjos7xn3Jjz9KH39sNceZgwMAUFVLlkgjRkgTJkgtWjhdDQDAlxw8KPXuLQ0eLF16qdPVAAD8jTHSCy9ImzZJU6dKNWo4XREAwEnTp0v/+580fjw9DQB+gca4r/jjD+uKqunTpXD+WQAAVVBYKD39tLRjh5UnsbFOVwQA8CU7dlhzir/4onTGGU5XAwDwN8nJ0qBBUufONEAAANKCBdaNflOmMKUGAL9BB9YXpKdLd95pDTkSE+N0NQAAf/Tbb9L990u33iqNGuV0NQAAX7NmjXWX+PjxUsuWTlcDAPAnxcXSf/8rzZ1rXVzVtq3TFQEAnPbTT1YmTJsmRUQ4XQ0A2EZj3GlFRVYT4z//YbhbAEDlGSO9+640a5b0/vtSs2ZOVwQA8DXz51vD3k6bJtWv73Q1AAB/snat9Mgj0vXXS19+yR2BAADp11+lRx+1zi+io52uBgAqhca4k4yRHnhAuuUWqVMnp6sBAPib1FRpwAApKUmaOZMPqQAAx/r4Y+mLL6TPP+dDKwCAfbm51khUycnSBx9IiYlOVwQA8AXJydJ991nnGXFxTlcDAJXGJ+hOeuUV686+6693uhIAgL/56iupd29pxAhpyBCa4gCA0oyx7hJfvNj60IqmOADArnnzpF69pPPOkyZPpikOALDs2yfddps1JSzZAMBPcce4U778Utq0yRr+FgAAu7KypIcflmrXtoZPr1HD6YoAAL7G5bLmE2/YUHrjDSkkxOmKAAD+4MABaehQqU4da0SqOnWcrggA4CsyM60bNF59VWrVyulqAKDKaIw74eefpffes4Yz5EMqAIBdy5dLw4dLjz0mdenidDUAAF+UlyfdcYd06aXSrbc6XQ0AwB8YY40uMmGCNHq0dO65TlcEAPAleXlWU/ypp6RTT3W6GgCoFhrj3rZrl3Wn32efSZGRTlcDAPAHRUXSf/4jbdliXVRVt67TFQEAfFFamvSvf0kDBkg9ezpdDQDAH2zfbo0ycu650jffSBERTlcEAPAlRUXWBbf33SclJTldDQBUG41xb8rKsubgePddqV49p6sBAPiDLVuk+++3Gh0jRjDSCACgbDt3WucaY8dKZ5/tdDUAAF9XVGQNh7tokfTii9KJJzpdEQDA1xgj3XuvdPXVUo8eTlcDAG5BY9xbioutD6pGjJBOOMHpagAAvs4Y6f33pS+/lN55R2re3OmKAAC+asMG6YEHrOma2rRxuhoAgK/7+Wfp3/+WbrnFmkuci28BAGUZNkw680zpppucrgQA3IbGuDcYIw0dKl11lXThhU5XAwDwdamp1l3i55wjffGFFBbmdEUAAF+1YIH07LPS1KlSfLzT1QAAfFlWlvTkk9LevdJHH5EbAIDyPfusVKeOdcc4AAQQGuPeMGaMVL++NQwuAADlMUaaMkWaOFF6/nnp9NOdrggA4Mveflv63/+kGTOk6GinqwEA+CpjpGnTrNwYNkzq3t3pigAAvuyFF6QDB6zmOAAEGBrjnuZySS1aWMNTAQBQEWOk/Hzp66+liAinqwEA+DKXS6pZU/rwQyk01OlqAAC+zBjrbvFvvpGiopyuBgDgy1wuKTFRGjKEqTYABCQa454WGir17u10FQAAfxAaKt12m9NVAAD8QWio1K+f01UAAPxBaKh0++1OVwEA8AehoYx8CyCgcWsBAAAAAAAAAAAAACCg0RgHAAAAAAAAAAAAAAQ0GuMAAAAAAAAAAAAAgIBGYxwAAAAAAAAAAAAAENBojAMAAAAAAAAAAAAAAhqNcQAAAAAAAAAAAABAQKMxDgAAAAAAAAAAAAAIaDTGAQAAAAAAAAAAAAABjcY4AAAAAAAAAAAAACCgBVVj/I033lDLli1Vo0YNderUSStWrHC6JACAjyIzAAB2kRkAALvIDACAXWQGALhf0DTGP/nkEw0ePFgjR47Uzz//rNNPP13du3fX3r17nS4NAOBjyAwAgF1kBgDALjIDAGAXmQEAnhE0jfGXXnpJd911l2677Ta1b99eb7/9tqKjozV+/HinSwMA+BgyAwBgF5kBALCLzAAA2EVmAIBnBEVjvKCgQKtWrVLXrl1LHgsNDVXXrl21bNkyBysDAPgaMgMAYBeZAQCwi8wAANhFZgCA54Q7XYA37N+/X8XFxUpISCj1eEJCgn799ddjls/Pz1d+fn7J9+np6ZKkjIwMzxYKANV0+DhljHG4Ev9FZgAIJuRG9ZAZAIIJmVE9ZAaAYEJmVA+ZASCYeDszgqIxXlljxozRU089dczjzZo1c6AaAKi8AwcOKDY21ukyggKZASAQkBveQWYACARkhneQGQACAZnhHWQGgEDgrcwIisZ4gwYNFBYWptTU1FKPp6amKjEx8Zjlhw8frsGDB5d8n5aWphYtWig5OZkgP46MjAw1a9ZMO3fuVExMjNPl+DT2lX3sK/vS09PVvHlz1atXz+lS/BaZ4T38btvHvrKPfVU55Eb1kBnew++2fewr+9hXlUNmVA+Z4T38btvHvrKPfVU5ZEb1kBnew++2fewr+9hXlePtzAiKxnhkZKTOOusszZ8/X1dffbUkyeVyaf78+Ro4cOAxy0dFRSkqKuqYx2NjY/khtikmJoZ9ZRP7yj72lX2hoaFOl+C3yAzv43fbPvaVfeyryiE3qobM8D5+t+1jX9nHvqocMqNqyAzv43fbPvaVfeyryiEzqobM8D5+t+1jX9nHvqocb2VGUDTGJWnw4MHq16+fzj77bJ177rl65ZVXlJ2drdtuu83p0gAAPobMAADYRWYAAOwiMwAAdpEZAOAZQdMYv/HGG7Vv3z6NGDFCKSkp6tixo+bMmaOEhASnSwMA+BgyAwBgF5kBALCLzAAA2EVmAIBnBE1jXJIGDhxY5lAjxxMVFaWRI0eWORwJSmNf2ce+so99ZR/7yn3IDM9jX9nHvrKPfVU57C/3IDM8j31lH/vKPvZV5bC/3IPM8Dz2lX3sK/vYV5XD/nIPMsPz2Ff2sa/sY19Vjrf3V4gxxnhlSwAAAAAAAAAAAAAAOMA7M5kDAAAAAAAAAAAAAOAQGuMAAAAAAAAAAAAAgIBGYxwAAAAAAAAAAAAAENBojNvwxhtvqGXLlqpRo4Y6deqkFStWOF2SVz355JMKCQkp9dWuXbuS5/Py8jRgwADVr19ftWvX1nXXXafU1NRS60hOTlavXr0UHR2t+Ph4DR06VEVFRd5+K273ww8/6IorrlDjxo0VEhKiL774otTzxhiNGDFCjRo1Us2aNdW1a1f9/vvvpZY5ePCgevfurZiYGMXFxemOO+5QVlZWqWXWrVunCy+8UDVq1FCzZs303HPPefqtud3x9tWtt956zM9Zjx49Si0TLPtqzJgxOuecc1SnTh3Fx8fr6quv1ubNm0st467fu4ULF+rMM89UVFSUTjjhBE2cONHTby/gkRlkRnnIDPvIDPvIDP9GZpAZ5SEz7CMz7CMz/BuZQWaUh8ywj8ywj8zwb2QGmVEeMsM+MsM+v8sMgwpNnTrVREZGmvHjx5uNGzeau+66y8TFxZnU1FSnS/OakSNHmlNOOcXs2bOn5Gvfvn0lz/fv3980a9bMzJ8/36xcudL84x//MOedd17J80VFRebUU081Xbt2NatXrzazZ882DRo0MMOHD3fi7bjV7NmzzWOPPWamT59uJJkZM2aUen7s2LEmNjbWfPHFF2bt2rXmyiuvNK1atTK5ubkly/To0cOcfvrp5scffzT/+9//zAknnGBuvvnmkufT09NNQkKC6d27t9mwYYOZMmWKqVmzpnnnnXe89Tbd4nj7ql+/fqZHjx6lfs4OHjxYaplg2Vfdu3c3EyZMMBs2bDBr1qwxl112mWnevLnJysoqWcYdv3fbtm0z0dHRZvDgwWbTpk3m9ddfN2FhYWbOnDlefb+BhMwgMypCZthHZthHZvgvMoPMqAiZYR+ZYR+Z4b/IDDKjImSGfWSGfWSG/yIzyIyKkBn2kRn2+Vtm0Bg/jnPPPdcMGDCg5Pvi4mLTuHFjM2bMGAer8q6RI0ea008/vczn0tLSTEREhJk2bVrJY7/88ouRZJYtW2aMsQ4goaGhJiUlpWSZt956y8TExJj8/HyP1u5NRx8cXS6XSUxMNM8//3zJY2lpaSYqKspMmTLFGGPMpk2bjCTz008/lSzzzTffmJCQELN7925jjDFvvvmmqVu3bql9NWzYMNO2bVsPvyPPKS9IrrrqqnJfE6z7yhhj9u7daySZRYsWGWPc93v3yCOPmFNOOaXUtm688UbTvXt3T7+lgEVmkBl2kRn2kRmVQ2b4DzKDzLCLzLCPzKgcMsN/kBlkhl1khn1kRuWQGf6DzCAz7CIz7CMzKsfXM4Oh1CtQUFCgVatWqWvXriWPhYaGqmvXrlq2bJmDlXnf77//rsaNG6t169bq3bu3kpOTJUmrVq1SYWFhqX3Url07NW/evGQfLVu2TB06dFBCQkLJMt27d1dGRoY2btzo3TfiRdu3b1dKSkqpfRMbG6tOnTqV2jdxcXE6++yzS5bp2rWrQkNDtXz58pJlOnfurMjIyJJlunfvrs2bN+vQoUNeejfesXDhQsXHx6tt27a69957deDAgZLngnlfpaenS5Lq1asnyX2/d8uWLSu1jsPLBNvxzV3IjL+RGZVHZlQemVE2MsM/kBl/IzMqj8yoPDKjbGSGfyAz/kZmVB6ZUXlkRtnIDP9AZvyNzKg8MqPyyIyy+Xpm0BivwP79+1VcXFzqH0KSEhISlJKS4lBV3tepUydNnDhRc+bM0VtvvaXt27frwgsvVGZmplJSUhQZGam4uLhSrzlyH6WkpJS5Dw8/F6gOv7eKfn5SUlIUHx9f6vnw8HDVq1cv6PZfjx49NGnSJM2fP1/PPvusFi1apJ49e6q4uFhS8O4rl8ulhx56SOeff75OPfVUSXLb7115y2RkZCg3N9cTbyegkRkWMqNqyIzKITPKRmb4DzLDQmZUDZlROWRG2cgM/0FmWMiMqiEzKofMKBuZ4T/IDAuZUTVkRuWQGWXzh8wIr9Q7QlDq2bNnyf+fdtpp6tSpk1q0aKFPP/1UNWvWdLAyBJKbbrqp5P87dOig0047TW3atNHChQvVpUsXBytz1oABA7RhwwYtXrzY6VIAW8gMeAOZUTYyA/6GzIA3kBllIzPgb8gMeAOZUTYyA/6GzIA3kBll84fM4I7xCjRo0EBhYWFKTU0t9XhqaqoSExMdqsp5cXFxOumkk7RlyxYlJiaqoKBAaWlppZY5ch8lJiaWuQ8PPxeoDr+3in5+EhMTtXfv3lLPFxUV6eDBg0G//1q3bq0GDRpoy5YtkoJzXw0cOFCzZs3S999/r6ZNm5Y87q7fu/KWiYmJ4Y/EKiAzykZm2ENmVA+ZQWb4GzKjbGSGPWRG9ZAZZIa/ITPKRmbYQ2ZUD5lBZvgbMqNsZIY9ZEb1kBn+kxk0xisQGRmps846S/Pnzy95zOVyaf78+UpKSnKwMmdlZWVp69atatSokc466yxFRESU2kebN29WcnJyyT5KSkrS+vXrSx0E5s6dq5iYGLVv397r9XtLq1atlJiYWGrfZGRkaPny5aX2TVpamlatWlWyzIIFC+RyudSpU6eSZX744QcVFhaWLDN37ly1bdtWdevW9dK78b5du3bpwIEDatSokaTg2lfGGA0cOFAzZszQggUL1KpVq1LPu+v3LikpqdQ6Di8TzMe36iAzykZm2ENmVA+ZQWb4GzKjbGSGPWRG9ZAZZIa/ITPKRmbYQ2ZUD5lBZvgbMqNsZIY9ZEb1kBl+lBkGFZo6daqJiooyEydONJs2bTJ33323iYuLMykpKU6X5jVDhgwxCxcuNNu3bzdLliwxXbt2NQ0aNDB79+41xhjTv39/07x5c7NgwQKzcuVKk5SUZJKSkkpeX1RUZE499VTTrVs3s2bNGjNnzhzTsGFDM3z4cKfekttkZmaa1atXm9WrVxtJ5qWXXjKrV682O3bsMMYYM3bsWBMXF2dmzpxp1q1bZ6666irTqlUrk5ubW7KOHj16mDPOOMMsX77cLF682Jx44onm5ptvLnk+LS3NJCQkmD59+pgNGzaYqVOnmujoaPPOO+94/f1WR0X7KjMz0zz88MNm2bJlZvv27WbevHnmzDPPNCeeeKLJy8srWUew7Kt7773XxMbGmoULF5o9e/aUfOXk5JQs447fu23btpno6GgzdOhQ88svv5g33njDhIWFmTlz5nj1/QYSMoPMqAiZYR+ZYR+Z4b/IDDKjImSGfWSGfWSG/yIzyIyKkBn2kRn2kRn+i8wgMypCZthHZtjnb5lBY9yG119/3TRv3txERkaac8891/z4449Ol+RVN954o2nUqJGJjIw0TZo0MTfeeKPZsmVLyfO5ubnmvvvuM3Xr1jXR0dHmmmuuMXv27Cm1jj/++MP07NnT1KxZ0zRo0MAMGTLEFBYWevutuN33339vJB3z1a9fP2OMMS6XyzzxxBMmISHBREVFmS5dupjNmzeXWseBAwfMzTffbGrXrm1iYmLMbbfdZjIzM0sts3btWnPBBReYqKgo06RJEzN27FhvvUW3qWhf5eTkmG7dupmGDRuaiIgI06JFC3PXXXcd8wdbsOyrsvaTJDNhwoSSZdz1e/f999+bjh07msjISNO6detS20DVkBlkRnnIDPvIDPvIDP9GZpAZ5SEz7CMz7CMz/BuZQWaUh8ywj8ywj8zwb2QGmVEeMsM+MsM+f8uMkL+KBgAAAAAAAAAAAAAgIDHHOAAAAAAAAAAAAAAgoNEYBwAAAAAAAAAAAAAENBrjAAAAAAAAAAAAAICARmMcAAAAAAAAAAAAABDQaIwDAAAAAAAAAAAAAAIajXEAAAAAAAAAAAAAQECjMQ4AAAAAAAAAAAAACGg0xgEAAAAAAAAAAAAAAY3GOAAAAAAAAAAAAAAgoNEYB7youLhY5513nq699tpSj6enp6tZs2Z67LHHHKoMAOBryAwAgF1kBgDALjIDAGAXmYFAFGKMMU4XAQST3377TR07dtR7772n3r17S5L69u2rtWvX6qefflJkZKTDFQIAfAWZAQCwi8wAANhFZgAA7CIzEGhojAMOeO211/Tkk09q48aNWrFiha6//nr99NNPOv30050uDQDgY8gMAIBdZAYAwC4yAwBgF5mBQEJjHHCAMUaXXHKJwsLCtH79et1///16/PHHnS4LAOCDyAwAgF1kBgDALjIDAGAXmYFAQmMccMivv/6qk08+WR06dNDPP/+s8PBwp0sCAPgoMgMAYBeZAQCwi8wAANhFZiBQhDpdABCsxo8fr+joaG3fvl27du1yuhwAgA8jMwAAdpEZAAC7yAwAgF1kBgIFd4wDDli6dKkuuugifffdd3rmmWckSfPmzVNISIjDlQEAfA2ZAQCwi8wAANhFZgAA7CIzEEi4YxzwspycHN16662699579X//938aN26cVqxYobffftvp0gAAPobMAADYRWYAAOwiMwAAdpEZCDTcMQ542YMPPqjZs2dr7dq1io6OliS98847evjhh7V+/Xq1bNnS2QIBAD6DzAAA2EVmAADsIjMAAHaRGQg0NMYBL1q0aJG6dOmihQsX6oILLij1XPfu3VVUVMQQJAAASWQGAMA+MgMAYBeZAQCwi8xAIKIxDgAAAAAAAAAAAAAIaMwxDgAAAAAAAAAAAAAIaDTGAQAAAAAAAAAAAAABjcY4AAAAAAAAAAAAACCg0RgHAAAAAAAAAAAAAAQ0GuMAAAAAAAAAAAAAgIBGYxwAAAAAAAAAAAAAENBojAMAAAAAAAAAAAAAAhqNcQAAAAAAAAAAAABAQKMxDgAAAAAAAAAAAAAIaDTGAQAAAAAAAAAAAAABjcY4AAAAAAAAAAAAACCg0RgHAAAAAAAAAAAAAAS0/wdUmaneFgTFwgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个包含10个子图的窗口\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axs = axs.ravel()  # 将二维数组展平，方便通过索引访问每个子图\n",
    "\n",
    "env = env_test1.DroneEnv()\n",
    "for i in range(10):\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    trajectory_x = [env.xy_p[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_y = [env.xy_p[1]]  # 存储无人机路径的y坐标\n",
    "    trajectory_ex = [env.xy_e[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_ey = [env.xy_e[1]]  # 存储无人机路径的y坐标\n",
    "    \n",
    "    # 在第i个子图中绘制环境和障碍物\n",
    "    axs[i].scatter(env.xy_e[0], env.xy_e[1], marker='x', color='green', label='Goal')\n",
    "    for k in env.obstacles:\n",
    "        obstacle_circle = plt.Circle(k, env.r_obstacles, color='lightblue', fill=True)\n",
    "        axs[i].add_patch(obstacle_circle)\n",
    "    axs[i].set_xlim(env.space1.low[0], env.space1.high[0])\n",
    "    axs[i].set_ylim(env.space1.low[1], env.space1.high[1])\n",
    "    axs[i].set_xlabel('X')\n",
    "    axs[i].set_ylabel('Y')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Episode {i+1}')\n",
    "\n",
    "    # 通过预训练模型控制无人机执行任务并绘制路径\n",
    "    model = PPO.load(\"last_model\") \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        count += 1\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        next_state, reward, done, t, info = env.step(action)\n",
    "        #if reward < -10:\n",
    "            #print(state, action, reward)\n",
    "        if count > 500:\n",
    "            done = True\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory_x.append(env.xy_p[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_y.append(env.xy_p[1])  # 更新无人机路径的y坐标\n",
    "        trajectory_ex.append(env.xy_e[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_ey.append(env.xy_e[1])  # 更新无人机路径的y坐标\n",
    "\n",
    "    # 绘制无人机路径\n",
    "    axs[i].plot(trajectory_x, trajectory_y, color='red', linewidth=0.5)\n",
    "    axs[i].plot(trajectory_ex, trajectory_ey, color='red', linewidth=0.5)\n",
    "\n",
    "    # 打印每个episode的总奖励\n",
    "    print(f'Episode {i+1} total reward:', total_reward)\n",
    "\n",
    "# 显示子图窗口\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58fa9da8-3fc8-4a42-a7ef-550800440c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dc779-ac63-49ed-be21-b16acfdb9ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
