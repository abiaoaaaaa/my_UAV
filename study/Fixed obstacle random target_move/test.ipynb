{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-24T07:23:01.542482200Z",
     "start_time": "2024-04-24T07:23:01.524472600Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env_test1\n",
    "env = env_test1.DroneEnv()\n",
    "#vec_env = make_vec_env(env, n_envs=4)\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c32894c611bf66",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1728df1-593d-441c-a1e0-6c2c493ead86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "callbacks = []\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=8,\n",
    "    best_model_save_path=\".\",\n",
    "    log_path=\".\",\n",
    "    eval_freq=4000,\n",
    ")\n",
    "\n",
    "callbacks.append(eval_callback)\n",
    "kwargs = {}\n",
    "kwargs[\"callback\"] = callbacks\n",
    "\n",
    "log_name = \"ppo_run_\" + str(time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6364eb784437bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T12:30:03.983094500Z",
     "start_time": "2024-04-20T12:03:47.079933900Z"
    },
    "collapsed": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard/ppo_run_1713687030.4351685_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1434 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-13154.16 +/- 5178.12\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.32e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4000          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013020312 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -7.27e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.93e+06      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000276     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 4.26e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 722  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00115988 |\n",
      "|    clip_fraction        | 0.00107    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.000109   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.09e+05   |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00188   |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 7.05e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-27653.30 +/- 11351.86\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -2.77e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8000          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00062321324 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -4.27e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.42e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00111      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 9.58e+05      |\n",
      "-------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 700  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 726          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001914009 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 3.75e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.04e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000387    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-22110.59 +/- 16649.96\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.21e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.525155e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.31e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000216    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 2.42e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 676   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 703           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.9605444e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 1.13e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.91e+06      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000445     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 1.9e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-27960.50 +/- 10651.71\n",
      "Episode length: 479.00 +/- 34.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 479          |\n",
      "|    mean_reward          | -2.8e+04     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029242923 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -1.67e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.92e+04     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.92e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 677   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 698           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1957246e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 2.44e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.95e+06      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000212     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 6.96e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-24750.53 +/- 9492.65\n",
      "Episode length: 478.75 +/- 45.25\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 479           |\n",
      "|    mean_reward          | -2.48e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7509245e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 3.34e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+06      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000478     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.79e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 677   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 692         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001221641 |\n",
      "|    clip_fraction        | 0.000977    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 3.93e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.65e+05    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 6.63e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-12681.04 +/- 11263.09\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.27e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.848313e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 2.32e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000262    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.17e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 674   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 688           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7873265e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.08e+06      |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.00049      |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 5.32e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-5684.77 +/- 1341.32\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -5.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.952757e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+06     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000263    |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 3.24e+06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 673   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 686           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036954653 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.26e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.35e+04      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000512     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.52e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-6239.40 +/- 2423.14\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -6.24e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.821699e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.11e+06     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000303    |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 4.37e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 673   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 684           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 50            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019757028 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.13e+05      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000423     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 3.25e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-12069.94 +/- 8725.25\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.21e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 36000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033381724 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.74e+04      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000496     |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 1.15e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 681          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001192918 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.95e+05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.03e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-12343.45 +/- 12051.51\n",
      "Episode length: 496.50 +/- 11.91\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 496           |\n",
      "|    mean_reward          | -1.23e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.2632225e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000263     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 2.48e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 681          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.703011e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+06     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000352    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-15856.66 +/- 11999.32\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.59e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 44000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015633131 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.05e+06      |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.00087      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.91e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 680           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 69            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013010448 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.49e+06      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000892     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 5.32e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-6497.46 +/- 3077.42\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -6.5e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.122578e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+06     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000148    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 2.31e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 677           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 75            |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016159046 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.54e+06      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000505     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.25e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-21756.87 +/- 12949.71\n",
      "Episode length: 484.12 +/- 28.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -2.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018584207 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 3.51e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 670   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 677        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01545974 |\n",
      "|    clip_fraction        | 0.0948     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | -1.07e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.82e+04   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.933      |\n",
      "|    value_loss           | 5.45e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-7250.38 +/- 2340.96\n",
      "Episode length: 491.12 +/- 18.78\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 491           |\n",
      "|    mean_reward          | -7.25e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 56000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.7813474e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.29e+06      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000295     |\n",
      "|    std                  | 0.926         |\n",
      "|    value_loss           | 5.55e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012515229 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00097     |\n",
      "|    std                  | 0.922        |\n",
      "|    value_loss           | 7.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-22080.54 +/- 8698.13\n",
      "Episode length: 462.25 +/- 51.42\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 462           |\n",
      "|    mean_reward          | -2.21e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010047809 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.34         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.41e+06      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000695     |\n",
      "|    std                  | 0.922         |\n",
      "|    value_loss           | 6.05e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 672   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.229212e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+06     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000325    |\n",
      "|    std                  | 0.925        |\n",
      "|    value_loss           | 5.54e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-22175.84 +/- 10367.73\n",
      "Episode length: 473.38 +/- 30.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 473          |\n",
      "|    mean_reward          | -2.22e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002587432 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67e+06     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000988    |\n",
      "|    std                  | 0.934        |\n",
      "|    value_loss           | 6.61e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 97    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 676          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002874367 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.49e+06     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000739    |\n",
      "|    std                  | 0.93         |\n",
      "|    value_loss           | 3.55e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-14789.97 +/- 11645.07\n",
      "Episode length: 487.62 +/- 27.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -1.48e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0105978735 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8e+04      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0115      |\n",
      "|    std                  | 0.89         |\n",
      "|    value_loss           | 3.67e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 676           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 105           |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.2491926e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.3          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.42e+06      |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.000571     |\n",
      "|    std                  | 0.886         |\n",
      "|    value_loss           | 2.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-18318.52 +/- 14533.00\n",
      "Episode length: 485.38 +/- 41.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008573481 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+06     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000911    |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 2.72e+06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 109   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 675           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 112           |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015151853 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.3          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.44e+05      |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.000692     |\n",
      "|    std                  | 0.892         |\n",
      "|    value_loss           | 1.98e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-21467.39 +/- 9185.24\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.15e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038753268 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.66e+04     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 1.16e+05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 669   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 116   |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 674          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020816643 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.43e+05     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    std                  | 0.892        |\n",
      "|    value_loss           | 1.2e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-20054.72 +/- 14752.27\n",
      "Episode length: 491.25 +/- 16.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 491          |\n",
      "|    mean_reward          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064291977 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+04     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    std                  | 0.868        |\n",
      "|    value_loss           | 4.42e+04     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 669   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 122   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 674          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015830671 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.85e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000618    |\n",
      "|    std                  | 0.866        |\n",
      "|    value_loss           | 5.19e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-21727.84 +/- 13639.05\n",
      "Episode length: 478.00 +/- 40.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 478           |\n",
      "|    mean_reward          | -2.17e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 84000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1076665e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.52e+06      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000137     |\n",
      "|    std                  | 0.868         |\n",
      "|    value_loss           | 4.63e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 670   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 128   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-27641.76 +/- 10284.69\n",
      "Episode length: 444.38 +/- 61.33\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 444           |\n",
      "|    mean_reward          | -2.76e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 88000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027906775 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.43e+05      |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -8.09e-05     |\n",
      "|    std                  | 0.87          |\n",
      "|    value_loss           | 2.38e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 667   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 670          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036826394 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56e+04     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    std                  | 0.866        |\n",
      "|    value_loss           | 1.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-22012.30 +/- 14164.83\n",
      "Episode length: 495.12 +/- 15.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -2.2e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001530151 |\n",
      "|    clip_fraction        | 0.00083     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11e+05    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 2.15e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 667   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 670          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020424803 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35e+06     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.864        |\n",
      "|    value_loss           | 1.8e+06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-19967.06 +/- 8586.18\n",
      "Episode length: 483.50 +/- 32.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012932631 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.06e+04    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 2.1e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 665   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 144   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 146          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003261554 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+06     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000309    |\n",
      "|    std                  | 0.846        |\n",
      "|    value_loss           | 4.44e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-26703.79 +/- 12295.09\n",
      "Episode length: 484.38 +/- 23.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -2.67e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046064975 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.68e+03     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 8.39e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 150    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019467704 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+06     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 2.38e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-13910.43 +/- 12665.47\n",
      "Episode length: 492.75 +/- 16.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | -1.39e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008265653 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+06     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000985    |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 2.91e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 157    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 668           |\n",
      "|    iterations           | 52            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033188544 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.26         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.96e+06      |\n",
      "|    n_updates            | 510           |\n",
      "|    policy_gradient_loss | -0.000988     |\n",
      "|    std                  | 0.848         |\n",
      "|    value_loss           | 3.57e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-27529.20 +/- 13132.47\n",
      "Episode length: 491.88 +/- 16.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 492           |\n",
      "|    mean_reward          | -2.75e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 108000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088315294 |\n",
      "|    clip_fraction        | 0.000684      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.25         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.74e+05      |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | -0.00147      |\n",
      "|    std                  | 0.838         |\n",
      "|    value_loss           | 1.65e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 163    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012033752 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.44e+05     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 2.9e-05      |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 2.25e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-24232.10 +/- 12748.62\n",
      "Episode length: 471.25 +/- 38.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 471          |\n",
      "|    mean_reward          | -2.42e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031102104 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.28e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 1.49e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 169    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 171          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028162287 |\n",
      "|    clip_fraction        | 0.00278      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    std                  | 0.84         |\n",
      "|    value_loss           | 1.78e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-21789.34 +/- 13859.67\n",
      "Episode length: 499.25 +/- 4.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | -2.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044581005 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+06     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 2.85e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 175    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011180914 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000454    |\n",
      "|    std                  | 0.837        |\n",
      "|    value_loss           | 1.47e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-17455.98 +/- 8250.77\n",
      "Episode length: 494.00 +/- 18.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.537119e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.53e+06     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000121    |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 3.08e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 181    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007365042 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35e+06     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 1.44e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-22929.55 +/- 12745.78\n",
      "Episode length: 477.12 +/- 44.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 477          |\n",
      "|    mean_reward          | -2.29e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020703473 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    std                  | 0.859        |\n",
      "|    value_loss           | 2.62e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 187    |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 190          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002860201 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.15e+05     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000661    |\n",
      "|    std                  | 0.865        |\n",
      "|    value_loss           | 2.63e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-25257.20 +/- 13473.17\n",
      "Episode length: 479.62 +/- 37.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -2.53e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032707094 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+06     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    std                  | 0.875        |\n",
      "|    value_loss           | 1.82e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 194    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026172572 |\n",
      "|    clip_fraction        | 0.00483      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.23e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 6.77e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-31096.03 +/- 15369.35\n",
      "Episode length: 480.75 +/- 36.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 481          |\n",
      "|    mean_reward          | -3.11e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009766872 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04e+06     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.878        |\n",
      "|    value_loss           | 3.26e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 200    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 667           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048130722 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.29         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.97e+04      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | -0.00115      |\n",
      "|    std                  | 0.878         |\n",
      "|    value_loss           | 8.64e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-30397.61 +/- 13300.01\n",
      "Episode length: 494.50 +/- 10.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -3.04e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010595629 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+06     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000804    |\n",
      "|    std                  | 0.887        |\n",
      "|    value_loss           | 1.97e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 206    |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 208          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001694673 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.3e+06      |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000484    |\n",
      "|    std                  | 0.876        |\n",
      "|    value_loss           | 2.63e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-28552.66 +/- 14688.69\n",
      "Episode length: 492.50 +/- 14.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -2.86e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030356161 |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    std                  | 0.894        |\n",
      "|    value_loss           | 1.44e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 212    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011041071 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+06     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.903        |\n",
      "|    value_loss           | 2.69e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-30116.53 +/- 12813.17\n",
      "Episode length: 490.88 +/- 26.79\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 491           |\n",
      "|    mean_reward          | -3.01e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 144000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00083319494 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.31         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2e+06         |\n",
      "|    n_updates            | 700           |\n",
      "|    policy_gradient_loss | -0.002        |\n",
      "|    std                  | 0.883         |\n",
      "|    value_loss           | 2.14e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 218    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032886472 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+06      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.879        |\n",
      "|    value_loss           | 2.65e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-13791.77 +/- 8407.52\n",
      "Episode length: 500.00 +/- 2.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -1.38e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 148000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065398443 |\n",
      "|    clip_fraction        | 0.00103       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.28         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.09e+06      |\n",
      "|    n_updates            | 720           |\n",
      "|    policy_gradient_loss | -0.00176      |\n",
      "|    std                  | 0.864         |\n",
      "|    value_loss           | 3.06e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 224    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002634658 |\n",
      "|    clip_fraction        | 0.0041      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.13e+05    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    std                  | 0.874       |\n",
      "|    value_loss           | 2.35e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-30423.13 +/- 12848.79\n",
      "Episode length: 470.25 +/- 47.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 470           |\n",
      "|    mean_reward          | -3.04e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 152000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00061391434 |\n",
      "|    clip_fraction        | 0.000586      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.29         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.98e+05      |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | -0.00149      |\n",
      "|    std                  | 0.88          |\n",
      "|    value_loss           | 2.39e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 230    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 668           |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 232           |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048908486 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.3          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | -0.00052      |\n",
      "|    std                  | 0.891         |\n",
      "|    value_loss           | 2.13e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-17447.73 +/- 15471.95\n",
      "Episode length: 471.12 +/- 53.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 471         |\n",
      "|    mean_reward          | -1.74e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000597862 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.07e+05    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.000456   |\n",
      "|    std                  | 0.885       |\n",
      "|    value_loss           | 2.33e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 236    |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 668          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019784016 |\n",
      "|    clip_fraction        | 0.00469      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.43e+04     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    std                  | 0.868        |\n",
      "|    value_loss           | 2.54e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-12846.14 +/- 12481.85\n",
      "Episode length: 500.25 +/- 1.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.28e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008086888 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000339    |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 6.07e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 242    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 668         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011497587 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.03e+03    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    std                  | 0.867       |\n",
      "|    value_loss           | 1.48e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-6247.38 +/- 3324.18\n",
      "Episode length: 486.12 +/- 33.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 486         |\n",
      "|    mean_reward          | -6.25e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 164000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005377339 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.79e+06    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    std                  | 0.866       |\n",
      "|    value_loss           | 2.45e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 248    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 668         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006754908 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.74e+03    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.854       |\n",
      "|    value_loss           | 1.35e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-7950.96 +/- 6915.75\n",
      "Episode length: 483.50 +/- 33.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -7.95e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001060005 |\n",
      "|    clip_fraction        | 0.00103     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.47e+06    |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    std                  | 0.849       |\n",
      "|    value_loss           | 2.03e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 666    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 254    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-18345.84 +/- 13332.41\n",
      "Episode length: 491.50 +/- 18.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045906636 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.29e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.83         |\n",
      "|    value_loss           | 1.22e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 258    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 261          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025056163 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.48e+04     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.823        |\n",
      "|    value_loss           | 3.2e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-9193.37 +/- 7330.35\n",
      "Episode length: 484.62 +/- 29.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -9.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032050412 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.93e+05     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    std                  | 0.819        |\n",
      "|    value_loss           | 3.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 264    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 267           |\n",
      "|    total_timesteps      | 178176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029042925 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.22         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.92e+05      |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.000964     |\n",
      "|    std                  | 0.824         |\n",
      "|    value_loss           | 3.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7297.29 +/- 5855.33\n",
      "Episode length: 488.50 +/- 21.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | -7.3e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004279569 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.92e+05    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 1.11e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 270    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 273          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049220906 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.31e+05     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.816        |\n",
      "|    value_loss           | 4.83e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-6534.11 +/- 2400.90\n",
      "Episode length: 487.62 +/- 23.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -6.53e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007096815 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.13e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.824        |\n",
      "|    value_loss           | 3.98e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 277    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039603673 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.49e+03     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    std                  | 0.804        |\n",
      "|    value_loss           | 1.15e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-14246.51 +/- 12750.45\n",
      "Episode length: 490.00 +/- 29.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | -1.42e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 188000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001633557 |\n",
      "|    clip_fraction        | 0.00278     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.26e+05    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.000794   |\n",
      "|    std                  | 0.805       |\n",
      "|    value_loss           | 1.76e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 283    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009334973 |\n",
      "|    clip_fraction        | 0.0823      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.83e+04    |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 1.87e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-15271.22 +/- 13513.36\n",
      "Episode length: 485.88 +/- 40.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 486          |\n",
      "|    mean_reward          | -1.53e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033958037 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.27e+03     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.000643    |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 1.03e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 289    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 291          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014584798 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.9e+05      |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.807        |\n",
      "|    value_loss           | 2.49e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-5092.41 +/- 1314.80\n",
      "Episode length: 485.50 +/- 35.35\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 486           |\n",
      "|    mean_reward          | -5.09e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 196000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016845518 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.2          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.52e+06      |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -0.000758     |\n",
      "|    std                  | 0.807         |\n",
      "|    value_loss           | 3.67e+06      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 295    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 297          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023074336 |\n",
      "|    clip_fraction        | 0.00688      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+06     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    std                  | 0.807        |\n",
      "|    value_loss           | 1.31e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-13808.04 +/- 13784.14\n",
      "Episode length: 483.00 +/- 31.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | -1.38e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013785039 |\n",
      "|    clip_fraction        | 0.00288      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.55e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    std                  | 0.799        |\n",
      "|    value_loss           | 1.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 301    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 303          |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011976499 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 2.43e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-19316.66 +/- 11148.67\n",
      "Episode length: 473.12 +/- 31.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 473          |\n",
      "|    mean_reward          | -1.93e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 204000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025078813 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61e+06     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    std                  | 0.81         |\n",
      "|    value_loss           | 2.77e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 307    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 101          |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 206848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015677777 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.39e+05     |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.00076     |\n",
      "|    std                  | 0.811        |\n",
      "|    value_loss           | 2.81e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-14382.54 +/- 14318.65\n",
      "Episode length: 484.25 +/- 31.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -1.44e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006846187 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.69e+05     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00079     |\n",
      "|    std                  | 0.82         |\n",
      "|    value_loss           | 2.99e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 313    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 316          |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031853649 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.41e+05     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 8.56e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-10986.90 +/- 8368.17\n",
      "Episode length: 493.88 +/- 18.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.1e+04     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 212000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039849663 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5e+05        |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    std                  | 0.811        |\n",
      "|    value_loss           | 1.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 320    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 105          |\n",
      "|    time_elapsed         | 322          |\n",
      "|    total_timesteps      | 215040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011408073 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.69e+05     |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    std                  | 0.821        |\n",
      "|    value_loss           | 3.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-4692.74 +/- 683.07\n",
      "Episode length: 480.50 +/- 21.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | -4.69e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 216000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003029942 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.95e+05    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 2.94e+06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 665    |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 326    |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 666          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 328          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036032023 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+04     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 1.04e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-15149.16 +/- 16831.29\n",
      "Episode length: 493.38 +/- 20.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -1.51e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 220000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097625965 |\n",
      "|    clip_fraction        | 0.0116        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.22         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.23e+05      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -0.00504      |\n",
      "|    std                  | 0.818         |\n",
      "|    value_loss           | 9.92e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 664    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 332    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 665          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 335          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038561758 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.97e+05     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    std                  | 0.82         |\n",
      "|    value_loss           | 1.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-17180.38 +/- 15079.48\n",
      "Episode length: 496.38 +/- 12.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | -1.72e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 224000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004814559 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.86e+04    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    std                  | 0.851       |\n",
      "|    value_loss           | 5.13e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 663    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 339    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 341          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003198779 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.55e+05     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.853        |\n",
      "|    value_loss           | 2.41e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-10718.09 +/- 10269.97\n",
      "Episode length: 483.62 +/- 39.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -1.07e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 228000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005798389 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.76e+03    |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    std                  | 0.855       |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 346    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007444833 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17e+04     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00098     |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 6.95e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-10868.18 +/- 9147.14\n",
      "Episode length: 498.00 +/- 7.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 498          |\n",
      "|    mean_reward          | -1.09e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 232000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025443337 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+05     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    std                  | 0.835        |\n",
      "|    value_loss           | 1.97e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 662    |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 352    |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009779369 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.35e+05     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.84         |\n",
      "|    value_loss           | 2.52e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-11877.58 +/- 9727.00\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -1.19e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 236000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011151107 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.49e+03    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 1.41e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 358    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 663         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000906163 |\n",
      "|    clip_fraction        | 0.002       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.8e+05     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 1.41e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-9185.25 +/- 9481.48\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -9.19e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033173054 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.51e+04     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.000835    |\n",
      "|    std                  | 0.81         |\n",
      "|    value_loss           | 5.3e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 365    |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 367          |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026006352 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94e+05     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    std                  | 0.81         |\n",
      "|    value_loss           | 3.62e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-10105.09 +/- 13568.49\n",
      "Episode length: 497.38 +/- 9.59\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 497           |\n",
      "|    mean_reward          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 244000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020036194 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.21         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.05e+05      |\n",
      "|    n_updates            | 1190          |\n",
      "|    policy_gradient_loss | -0.000332     |\n",
      "|    std                  | 0.812         |\n",
      "|    value_loss           | 8.83e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 371    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 373          |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051721046 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.68e+05     |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.000823    |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 1.12e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-6792.43 +/- 2786.84\n",
      "Episode length: 492.88 +/- 12.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | -6.79e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 248000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055593764 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36e+05     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.803        |\n",
      "|    value_loss           | 4.35e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 661    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 377    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 380          |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013168362 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+04     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 6.7e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-6105.53 +/- 1898.05\n",
      "Episode length: 493.50 +/- 13.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | -6.11e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012337711 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.21e+03    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.000319   |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 1.17e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 384    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-5438.64 +/- 1832.15\n",
      "Episode length: 484.12 +/- 36.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -5.44e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 256000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013936909 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.27e+03     |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.772        |\n",
      "|    value_loss           | 6.49e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 388    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004838069 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.48e+03    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.000412   |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 1.16e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-9611.49 +/- 7857.62\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -9.61e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 260000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034426135 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.14         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.02e+05      |\n",
      "|    n_updates            | 1260          |\n",
      "|    policy_gradient_loss | -0.000341     |\n",
      "|    std                  | 0.759         |\n",
      "|    value_loss           | 4.96e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 394    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 396          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002975829 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+06     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 2.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-6798.20 +/- 2555.93\n",
      "Episode length: 492.25 +/- 15.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -6.8e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 264000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006966621 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.3e+05      |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    std                  | 0.761        |\n",
      "|    value_loss           | 7.13e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 400    |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 402           |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032852456 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.15         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8e+05         |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | -0.00104      |\n",
      "|    std                  | 0.76          |\n",
      "|    value_loss           | 1.43e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-7332.31 +/- 5549.11\n",
      "Episode length: 489.25 +/- 31.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 489          |\n",
      "|    mean_reward          | -7.33e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056573283 |\n",
      "|    clip_fraction        | 0.0766       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09e+03     |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    std                  | 0.739        |\n",
      "|    value_loss           | 1.19e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 406    |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061010495 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18e+03     |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | 0.000413     |\n",
      "|    std                  | 0.736        |\n",
      "|    value_loss           | 9.57e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-5974.95 +/- 2460.78\n",
      "Episode length: 490.25 +/- 24.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | -5.97e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010335114 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.31e+05    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.000186   |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 7.88e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 412    |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 134           |\n",
      "|    time_elapsed         | 415           |\n",
      "|    total_timesteps      | 274432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015446532 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.31e+05      |\n",
      "|    n_updates            | 1330          |\n",
      "|    policy_gradient_loss | -0.000172     |\n",
      "|    std                  | 0.739         |\n",
      "|    value_loss           | 5.87e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-7157.89 +/- 5597.39\n",
      "Episode length: 496.88 +/- 7.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | -7.16e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 276000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013745778 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.43e+03     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    std                  | 0.736        |\n",
      "|    value_loss           | 1.22e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 419    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 421           |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036808394 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.65e+05      |\n",
      "|    n_updates            | 1350          |\n",
      "|    policy_gradient_loss | 0.000316      |\n",
      "|    std                  | 0.736         |\n",
      "|    value_loss           | 1.85e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-10568.28 +/- 10139.59\n",
      "Episode length: 491.62 +/- 24.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 492           |\n",
      "|    mean_reward          | -1.06e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 280000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012617788 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.04e+05      |\n",
      "|    n_updates            | 1360          |\n",
      "|    policy_gradient_loss | -0.000558     |\n",
      "|    std                  | 0.736         |\n",
      "|    value_loss           | 2.36e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 425    |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 282624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002901785 |\n",
      "|    clip_fraction        | 0.00308     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.02e+05    |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.000611   |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 1.79e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-14921.56 +/- 11135.07\n",
      "Episode length: 482.62 +/- 25.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | -1.49e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 284000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018839605 |\n",
      "|    clip_fraction        | 0.0669      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13e+05    |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 4.63e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 431    |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 433          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003423942 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78e+05     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.000738    |\n",
      "|    std                  | 0.737        |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-14095.54 +/- 13102.82\n",
      "Episode length: 469.75 +/- 45.21\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 470           |\n",
      "|    mean_reward          | -1.41e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 288000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6938152e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.18e+05      |\n",
      "|    n_updates            | 1400          |\n",
      "|    policy_gradient_loss | -0.000233     |\n",
      "|    std                  | 0.737         |\n",
      "|    value_loss           | 1.69e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 437    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 439           |\n",
      "|    total_timesteps      | 290816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027844342 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.11         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.53e+05      |\n",
      "|    n_updates            | 1410          |\n",
      "|    policy_gradient_loss | 5.91e-05      |\n",
      "|    std                  | 0.738         |\n",
      "|    value_loss           | 3.79e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-5546.71 +/- 2165.99\n",
      "Episode length: 483.62 +/- 32.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 484        |\n",
      "|    mean_reward          | -5.55e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 292000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03702683 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.68e+03   |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | 0.00349    |\n",
      "|    std                  | 0.725      |\n",
      "|    value_loss           | 8.83e+04   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 443    |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 445          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006942353 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04e+05     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -8.55e-05    |\n",
      "|    std                  | 0.725        |\n",
      "|    value_loss           | 6.85e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-5246.68 +/- 1391.89\n",
      "Episode length: 485.88 +/- 36.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 486         |\n",
      "|    mean_reward          | -5.25e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005361247 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.65e+03    |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.000419   |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 1.18e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 449    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 451          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061685364 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+03     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.000564    |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 8.14e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-12705.76 +/- 10770.38\n",
      "Episode length: 492.25 +/- 15.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 492         |\n",
      "|    mean_reward          | -1.27e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010486906 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.49e+04    |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | 0.000234    |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 1.68e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 455    |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 662           |\n",
      "|    iterations           | 148           |\n",
      "|    time_elapsed         | 457           |\n",
      "|    total_timesteps      | 303104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018379898 |\n",
      "|    clip_fraction        | 0.000537      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.7e+05       |\n",
      "|    n_updates            | 1470          |\n",
      "|    policy_gradient_loss | 0.000387      |\n",
      "|    std                  | 0.699         |\n",
      "|    value_loss           | 8e+05         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-18546.75 +/- 12102.91\n",
      "Episode length: 476.88 +/- 38.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 477         |\n",
      "|    mean_reward          | -1.85e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 304000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011224788 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.34e+03    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | 0.00386     |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 2.08e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 461    |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 464          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026150402 |\n",
      "|    clip_fraction        | 0.00542      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.71e+05     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 1.09e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-17392.60 +/- 12999.54\n",
      "Episode length: 472.88 +/- 36.76\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 473           |\n",
      "|    mean_reward          | -1.74e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 308000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1205064e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+06      |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -0.000362     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 2.16e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 467    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 470          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004568533 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38e+06     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 1.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-21449.54 +/- 11617.66\n",
      "Episode length: 500.00 +/- 2.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -2.14e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 312000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017124688 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.25e+05      |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | -0.000805     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 1.62e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 474    |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 154           |\n",
      "|    time_elapsed         | 476           |\n",
      "|    total_timesteps      | 315392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5273952e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.75e+05      |\n",
      "|    n_updates            | 1530          |\n",
      "|    policy_gradient_loss | -7.84e-05     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 3.37e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-23572.66 +/- 6947.40\n",
      "Episode length: 490.25 +/- 19.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -2.36e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032038935 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.2e+04      |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.000186    |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 1.2e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 480    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 482          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037179133 |\n",
      "|    clip_fraction        | 0.097        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.21e+04     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | 0.0018       |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 4.71e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-11589.93 +/- 11911.38\n",
      "Episode length: 475.25 +/- 27.68\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 475           |\n",
      "|    mean_reward          | -1.16e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 320000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025247928 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 1560          |\n",
      "|    policy_gradient_loss | -0.000642     |\n",
      "|    std                  | 0.696         |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 486    |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045241587 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.52e+04    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 2e+05       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-18767.82 +/- 5104.22\n",
      "Episode length: 475.75 +/- 42.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 476           |\n",
      "|    mean_reward          | -1.88e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 324000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9576763e-05 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.18e+05      |\n",
      "|    n_updates            | 1580          |\n",
      "|    policy_gradient_loss | -8.85e-05     |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 7.64e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 492    |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 495           |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0920194e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.05e+05      |\n",
      "|    n_updates            | 1590          |\n",
      "|    policy_gradient_loss | 3.88e-05      |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 7.34e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-21545.90 +/- 10380.43\n",
      "Episode length: 497.88 +/- 8.27\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 498           |\n",
      "|    mean_reward          | -2.15e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 328000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2429653e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.98e+06      |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | -0.000208     |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 4.19e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 499    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 162           |\n",
      "|    time_elapsed         | 501           |\n",
      "|    total_timesteps      | 331776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016570062 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.37e+05      |\n",
      "|    n_updates            | 1610          |\n",
      "|    policy_gradient_loss | -0.000469     |\n",
      "|    std                  | 0.693         |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-26439.47 +/- 10066.18\n",
      "Episode length: 456.00 +/- 46.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | -2.64e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.717249e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.53e+06     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -7.89e-05    |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 3.78e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 505    |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 507          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016339404 |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.27e+05     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-21458.30 +/- 5601.63\n",
      "Episode length: 497.12 +/- 10.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | -2.15e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007918802 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.82e+05    |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | 1.04e-05    |\n",
      "|    std                  | 0.69        |\n",
      "|    value_loss           | 2.11e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 511    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 514          |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024592797 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.77e+04     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | 0.00521      |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.14e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-20664.86 +/- 8373.19\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.07e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015787362 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.19e+05     |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.000605    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 6.56e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 518    |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-16171.00 +/- 9128.50\n",
      "Episode length: 493.62 +/- 13.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.62e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 344000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025265627 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+06     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 1.64e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 521    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 169         |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 346112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014689185 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.25e+05    |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.000534   |\n",
      "|    std                  | 0.687       |\n",
      "|    value_loss           | 4.5e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-10803.63 +/- 8047.17\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.08e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056329295 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.29e+05     |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | -0.000127    |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 3.24e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 528    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 660        |\n",
      "|    iterations           | 171        |\n",
      "|    time_elapsed         | 530        |\n",
      "|    total_timesteps      | 350208     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02273851 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.48e+05   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | 0.00465    |\n",
      "|    std                  | 0.688      |\n",
      "|    value_loss           | 1.01e+06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-13403.64 +/- 11006.05\n",
      "Episode length: 496.12 +/- 12.90\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 496           |\n",
      "|    mean_reward          | -1.34e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 352000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7123122e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.5e+05       |\n",
      "|    n_updates            | 1710          |\n",
      "|    policy_gradient_loss | -4.65e-05     |\n",
      "|    std                  | 0.688         |\n",
      "|    value_loss           | 1.56e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 534    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 536          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.153574e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5e+05        |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000185    |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 1.78e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-19401.33 +/- 18374.87\n",
      "Episode length: 499.88 +/- 2.98\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -1.94e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 356000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014267818 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.44e+06      |\n",
      "|    n_updates            | 1730          |\n",
      "|    policy_gradient_loss | -0.00114      |\n",
      "|    std                  | 0.688         |\n",
      "|    value_loss           | 2.46e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 540    |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 542         |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010395614 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.17e+04    |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    std                  | 0.692       |\n",
      "|    value_loss           | 1.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-27209.87 +/- 9833.75\n",
      "Episode length: 493.62 +/- 19.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -2.72e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039253635 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.65e+05     |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | 0.00389      |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 546    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 548          |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003382783 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02e+06     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 2.24e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-9677.84 +/- 11473.41\n",
      "Episode length: 487.00 +/- 37.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 487          |\n",
      "|    mean_reward          | -9.68e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 364000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013278944 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.8e+05      |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.99e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 552    |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 555          |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.694167e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.69e+05     |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.000434    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.38e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-15281.70 +/- 13035.92\n",
      "Episode length: 499.25 +/- 3.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | -1.53e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 368000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018809382 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+05     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | 0.00174      |\n",
      "|    std                  | 0.69         |\n",
      "|    value_loss           | 1.24e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 558    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 181         |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 370688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022530261 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.03e+05    |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.687       |\n",
      "|    value_loss           | 4.69e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-9656.12 +/- 7949.19\n",
      "Episode length: 495.38 +/- 10.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -9.66e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 372000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002159676 |\n",
      "|    clip_fraction        | 0.00605     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.24e+04    |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 6.54e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 564    |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 183           |\n",
      "|    time_elapsed         | 567           |\n",
      "|    total_timesteps      | 374784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6748366e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+06      |\n",
      "|    n_updates            | 1820          |\n",
      "|    policy_gradient_loss | -0.000426     |\n",
      "|    std                  | 0.683         |\n",
      "|    value_loss           | 2.63e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-19065.06 +/- 12677.46\n",
      "Episode length: 490.38 +/- 19.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 490           |\n",
      "|    mean_reward          | -1.91e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 376000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0473916e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6e+05         |\n",
      "|    n_updates            | 1830          |\n",
      "|    policy_gradient_loss | -7.22e-05     |\n",
      "|    std                  | 0.683         |\n",
      "|    value_loss           | 2.81e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 571    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 573           |\n",
      "|    total_timesteps      | 378880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1276165e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.17e+05      |\n",
      "|    n_updates            | 1840          |\n",
      "|    policy_gradient_loss | -0.000407     |\n",
      "|    std                  | 0.683         |\n",
      "|    value_loss           | 4.53e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-17456.12 +/- 13707.41\n",
      "Episode length: 491.75 +/- 24.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -1.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074129533 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+04     |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 2.11e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 577    |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 187           |\n",
      "|    time_elapsed         | 579           |\n",
      "|    total_timesteps      | 382976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016522151 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.53e+06      |\n",
      "|    n_updates            | 1860          |\n",
      "|    policy_gradient_loss | -0.00104      |\n",
      "|    std                  | 0.678         |\n",
      "|    value_loss           | 3.56e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-17091.26 +/- 18247.09\n",
      "Episode length: 488.25 +/- 22.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 488           |\n",
      "|    mean_reward          | -1.71e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 384000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011381961 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.4e+06       |\n",
      "|    n_updates            | 1870          |\n",
      "|    policy_gradient_loss | -0.000816     |\n",
      "|    std                  | 0.677         |\n",
      "|    value_loss           | 4.59e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 583    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 585           |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010268629 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.43e+06      |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | -0.00024      |\n",
      "|    std                  | 0.677         |\n",
      "|    value_loss           | 3.37e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-25145.99 +/- 14651.26\n",
      "Episode length: 480.50 +/- 29.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 480        |\n",
      "|    mean_reward          | -2.51e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 388000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03418546 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 1.79e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.33e+06   |\n",
      "|    n_updates            | 1890       |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    std                  | 0.67       |\n",
      "|    value_loss           | 1.75e+06   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 589    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 191           |\n",
      "|    time_elapsed         | 592           |\n",
      "|    total_timesteps      | 391168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030226613 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+06       |\n",
      "|    n_updates            | 1900          |\n",
      "|    policy_gradient_loss | -0.000588     |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 2.61e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-17900.62 +/- 13468.67\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -1.79e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 392000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024188383 |\n",
      "|    clip_fraction        | 0.0867      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.94e+05    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | 0.00304     |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 1.16e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 596    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 598           |\n",
      "|    total_timesteps      | 395264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011006501 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.17e+06      |\n",
      "|    n_updates            | 1920          |\n",
      "|    policy_gradient_loss | -0.000191     |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 5.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-11184.31 +/- 8980.47\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -1.12e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 396000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072518125 |\n",
      "|    clip_fraction        | 0.00249       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 1930          |\n",
      "|    policy_gradient_loss | -0.00335      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 2.95e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 602    |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 604          |\n",
      "|    total_timesteps      | 399360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071867565 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36e+05     |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | 0.00338      |\n",
      "|    std                  | 0.671        |\n",
      "|    value_loss           | 9.33e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-36847.25 +/- 14676.73\n",
      "Episode length: 498.88 +/- 5.62\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 499           |\n",
      "|    mean_reward          | -3.68e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 400000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032312225 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.03e+05      |\n",
      "|    n_updates            | 1950          |\n",
      "|    policy_gradient_loss | -0.000949     |\n",
      "|    std                  | 0.67          |\n",
      "|    value_loss           | 2.49e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 608    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 610          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030640713 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.25e+04     |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | 0.00137      |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 1.18e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-17532.02 +/- 14914.40\n",
      "Episode length: 493.62 +/- 19.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -1.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 404000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010037206 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.15e+05     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.000113    |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 3.79e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 198    |\n",
      "|    time_elapsed    | 614    |\n",
      "|    total_timesteps | 405504 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 407552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005602197 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.85e+05    |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | 0.00269     |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 4.97e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-26483.65 +/- 15379.58\n",
      "Episode length: 498.62 +/- 6.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | -2.65e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 408000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056150774 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+04     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 2.47e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 200    |\n",
      "|    time_elapsed    | 620    |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 661           |\n",
      "|    iterations           | 201           |\n",
      "|    time_elapsed         | 622           |\n",
      "|    total_timesteps      | 411648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014550227 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+06      |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -0.00063      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 3.56e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-24916.91 +/- 14255.16\n",
      "Episode length: 493.12 +/- 20.84\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -2.49e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 412000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031040955 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.45e+06      |\n",
      "|    n_updates            | 2010          |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    std                  | 0.674         |\n",
      "|    value_loss           | 4.14e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 202    |\n",
      "|    time_elapsed    | 626    |\n",
      "|    total_timesteps | 413696 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 203           |\n",
      "|    time_elapsed         | 629           |\n",
      "|    total_timesteps      | 415744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047162434 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.02         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.28e+06      |\n",
      "|    n_updates            | 2020          |\n",
      "|    policy_gradient_loss | -0.00147      |\n",
      "|    std                  | 0.675         |\n",
      "|    value_loss           | 4.86e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-17418.10 +/- 8429.95\n",
      "Episode length: 493.38 +/- 20.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | -1.74e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004892613 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.2e+06      |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 5.11e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 204    |\n",
      "|    time_elapsed    | 632    |\n",
      "|    total_timesteps | 417792 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 419840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023012047 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5e+06      |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 3.33e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-26086.16 +/- 16490.87\n",
      "Episode length: 499.75 +/- 3.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -2.61e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015894965 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+04     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | 4.79e-05     |\n",
      "|    std                  | 0.675        |\n",
      "|    value_loss           | 2.68e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 206    |\n",
      "|    time_elapsed    | 638    |\n",
      "|    total_timesteps | 421888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 641         |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025889292 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.31e+03    |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | 0.000964    |\n",
      "|    std                  | 0.673       |\n",
      "|    value_loss           | 1.42e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-22770.56 +/- 17078.47\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -2.28e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 424000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048455084 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.62e+05     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.674        |\n",
      "|    value_loss           | 1.53e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 660    |\n",
      "|    iterations      | 208    |\n",
      "|    time_elapsed    | 645    |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-19945.39 +/- 12479.22\n",
      "Episode length: 495.12 +/- 15.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 428000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008233644 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.37e+05    |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 1.03e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 209    |\n",
      "|    time_elapsed    | 649    |\n",
      "|    total_timesteps | 428032 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 651           |\n",
      "|    total_timesteps      | 430080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040298267 |\n",
      "|    clip_fraction        | 0.00317       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.22e+06      |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | -0.000684     |\n",
      "|    std                  | 0.678         |\n",
      "|    value_loss           | 2.41e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-14983.01 +/- 8948.41\n",
      "Episode length: 498.75 +/- 5.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 499         |\n",
      "|    mean_reward          | -1.5e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 432000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003657674 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1e+06     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | 0.00343     |\n",
      "|    std                  | 0.68        |\n",
      "|    value_loss           | 1.17e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 659    |\n",
      "|    iterations      | 211    |\n",
      "|    time_elapsed    | 655    |\n",
      "|    total_timesteps | 432128 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 212          |\n",
      "|    time_elapsed         | 657          |\n",
      "|    total_timesteps      | 434176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023164393 |\n",
      "|    clip_fraction        | 0.00239      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+03     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.000921    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-11776.73 +/- 9099.82\n",
      "Episode length: 498.25 +/- 7.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 498          |\n",
      "|    mean_reward          | -1.18e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 436000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002647189 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21e+06     |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.000734    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 2.88e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 213    |\n",
      "|    time_elapsed    | 662    |\n",
      "|    total_timesteps | 436224 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 214          |\n",
      "|    time_elapsed         | 664          |\n",
      "|    total_timesteps      | 438272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020073534 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.05e+05     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | 0.000382     |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 1.27e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-22134.54 +/- 17973.73\n",
      "Episode length: 477.25 +/- 42.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 477          |\n",
      "|    mean_reward          | -2.21e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 440000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005852623 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+06     |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    std                  | 0.681        |\n",
      "|    value_loss           | 3.08e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 668    |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023426993 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.01e+04    |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | 0.00412     |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 1.47e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-16139.54 +/- 9598.10\n",
      "Episode length: 493.00 +/- 12.69\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -1.61e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 444000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016413032 |\n",
      "|    clip_fraction        | 0.00518       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.28e+05      |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | 6.93e-06      |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 1.8e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 217    |\n",
      "|    time_elapsed    | 675    |\n",
      "|    total_timesteps | 444416 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 677           |\n",
      "|    total_timesteps      | 446464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5620324e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.66e+06      |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -3.47e-06     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 2.24e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-20255.89 +/- 12846.78\n",
      "Episode length: 478.38 +/- 38.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 478          |\n",
      "|    mean_reward          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020081336 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+06     |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 1.85e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 219    |\n",
      "|    time_elapsed    | 681    |\n",
      "|    total_timesteps | 448512 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 220           |\n",
      "|    time_elapsed         | 683           |\n",
      "|    total_timesteps      | 450560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4647757e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.52e+06      |\n",
      "|    n_updates            | 2190          |\n",
      "|    policy_gradient_loss | -2.76e-05     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 4.15e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-20032.59 +/- 10024.25\n",
      "Episode length: 494.75 +/- 16.54\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 495           |\n",
      "|    mean_reward          | -2e+04        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 452000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6812125e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.03         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.62e+06      |\n",
      "|    n_updates            | 2200          |\n",
      "|    policy_gradient_loss | -0.000295     |\n",
      "|    std                  | 0.68          |\n",
      "|    value_loss           | 4.13e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 221    |\n",
      "|    time_elapsed    | 687    |\n",
      "|    total_timesteps | 452608 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 689          |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.818316e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93e+05     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.000372    |\n",
      "|    std                  | 0.68         |\n",
      "|    value_loss           | 2.76e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-14963.75 +/- 10932.20\n",
      "Episode length: 500.62 +/- 0.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -1.5e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 456000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006918802 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.14e+05    |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 6.07e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 223    |\n",
      "|    time_elapsed    | 693    |\n",
      "|    total_timesteps | 456704 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 695          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045513865 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49e+06     |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 2.23e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-27500.73 +/- 14782.35\n",
      "Episode length: 480.25 +/- 34.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -2.75e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037978985 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+05     |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | 0.00938      |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 1.54e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 225    |\n",
      "|    time_elapsed    | 699    |\n",
      "|    total_timesteps | 460800 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 702          |\n",
      "|    total_timesteps      | 462848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005164938 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.27e+05     |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 2.39e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-38989.67 +/- 13776.17\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -3.9e+04    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 464000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016199782 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.15e+05    |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | 0.00235     |\n",
      "|    std                  | 0.7         |\n",
      "|    value_loss           | 1.77e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 227    |\n",
      "|    time_elapsed    | 705    |\n",
      "|    total_timesteps | 464896 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 659           |\n",
      "|    iterations           | 228           |\n",
      "|    time_elapsed         | 708           |\n",
      "|    total_timesteps      | 466944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3846405e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.2e+06       |\n",
      "|    n_updates            | 2270          |\n",
      "|    policy_gradient_loss | -0.000154     |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 5.18e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-34061.17 +/- 15367.85\n",
      "Episode length: 477.50 +/- 40.70\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 478           |\n",
      "|    mean_reward          | -3.41e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 468000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1566706e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.7e+06       |\n",
      "|    n_updates            | 2280          |\n",
      "|    policy_gradient_loss | -3.4e-05      |\n",
      "|    std                  | 0.7           |\n",
      "|    value_loss           | 5.46e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 229    |\n",
      "|    time_elapsed    | 712    |\n",
      "|    total_timesteps | 468992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 714          |\n",
      "|    total_timesteps      | 471040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.388304e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+06     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -4.64e-05    |\n",
      "|    std                  | 0.7          |\n",
      "|    value_loss           | 4.56e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-24601.34 +/- 14478.23\n",
      "Episode length: 484.88 +/- 29.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -2.46e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 472000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001479286 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+06     |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 3.27e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 231    |\n",
      "|    time_elapsed    | 718    |\n",
      "|    total_timesteps | 473088 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 720          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017681828 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+06     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 2.24e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-13380.90 +/- 8831.52\n",
      "Episode length: 480.88 +/- 30.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 481          |\n",
      "|    mean_reward          | -1.34e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 476000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004776791 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+06      |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    std                  | 0.696        |\n",
      "|    value_loss           | 2.55e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 658    |\n",
      "|    iterations      | 233    |\n",
      "|    time_elapsed    | 724    |\n",
      "|    total_timesteps | 477184 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 658        |\n",
      "|    iterations           | 234        |\n",
      "|    time_elapsed         | 727        |\n",
      "|    total_timesteps      | 479232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00666592 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.41e+04   |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | 0.00116    |\n",
      "|    std                  | 0.704      |\n",
      "|    value_loss           | 9.35e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-26338.28 +/- 15152.13\n",
      "Episode length: 490.50 +/- 21.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -2.63e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008908616 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+06     |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 3.72e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 656    |\n",
      "|    iterations      | 235    |\n",
      "|    time_elapsed    | 733    |\n",
      "|    total_timesteps | 481280 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 656           |\n",
      "|    iterations           | 236           |\n",
      "|    time_elapsed         | 736           |\n",
      "|    total_timesteps      | 483328        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00063671026 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+06      |\n",
      "|    n_updates            | 2350          |\n",
      "|    policy_gradient_loss | -0.00208      |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 3.5e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-26609.99 +/- 11699.88\n",
      "Episode length: 491.75 +/- 17.91\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 492           |\n",
      "|    mean_reward          | -2.66e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 484000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038404987 |\n",
      "|    clip_fraction        | 0.00107       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.09e+06      |\n",
      "|    n_updates            | 2360          |\n",
      "|    policy_gradient_loss | -0.0021       |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 4.74e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 653    |\n",
      "|    iterations      | 237    |\n",
      "|    time_elapsed    | 742    |\n",
      "|    total_timesteps | 485376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 746          |\n",
      "|    total_timesteps      | 487424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011282825 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.4e+05      |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 2.58e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-29286.22 +/- 13987.26\n",
      "Episode length: 478.25 +/- 32.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 478           |\n",
      "|    mean_reward          | -2.93e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 488000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030250003 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.8e+05       |\n",
      "|    n_updates            | 2380          |\n",
      "|    policy_gradient_loss | -0.000122     |\n",
      "|    std                  | 0.703         |\n",
      "|    value_loss           | 1.68e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 650    |\n",
      "|    iterations      | 239    |\n",
      "|    time_elapsed    | 752    |\n",
      "|    total_timesteps | 489472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 756          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007804972 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26e+06     |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 3.75e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-21645.40 +/- 11799.18\n",
      "Episode length: 496.88 +/- 8.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 497           |\n",
      "|    mean_reward          | -2.16e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 492000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069258374 |\n",
      "|    clip_fraction        | 0.000928      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.8e+06       |\n",
      "|    n_updates            | 2400          |\n",
      "|    policy_gradient_loss | -0.0011       |\n",
      "|    std                  | 0.704         |\n",
      "|    value_loss           | 4.27e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 647    |\n",
      "|    iterations      | 241    |\n",
      "|    time_elapsed    | 762    |\n",
      "|    total_timesteps | 493568 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 765         |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026510742 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.67e+05    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.0203      |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 1.96e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-24503.31 +/- 10176.73\n",
      "Episode length: 469.00 +/- 42.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | -2.45e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.03612e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.16e+05    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.000109   |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 3.54e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 644    |\n",
      "|    iterations      | 243    |\n",
      "|    time_elapsed    | 771    |\n",
      "|    total_timesteps | 497664 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 775          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010296686 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+06     |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 3.83e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-20083.42 +/- 12435.70\n",
      "Episode length: 491.38 +/- 25.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | -2.01e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008786217 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.59e+05    |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 1.3e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 642    |\n",
      "|    iterations      | 245    |\n",
      "|    time_elapsed    | 781    |\n",
      "|    total_timesteps | 501760 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 246          |\n",
      "|    time_elapsed         | 784          |\n",
      "|    total_timesteps      | 503808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013513712 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+06      |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 3.29e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-26335.50 +/- 9554.09\n",
      "Episode length: 498.12 +/- 7.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 498         |\n",
      "|    mean_reward          | -2.63e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002640095 |\n",
      "|    clip_fraction        | 0.0163      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.22e+05    |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.000656   |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 1.32e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 639    |\n",
      "|    iterations      | 247    |\n",
      "|    time_elapsed    | 790    |\n",
      "|    total_timesteps | 505856 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 639           |\n",
      "|    iterations           | 248           |\n",
      "|    time_elapsed         | 794           |\n",
      "|    total_timesteps      | 507904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037070463 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.86e+06      |\n",
      "|    n_updates            | 2470          |\n",
      "|    policy_gradient_loss | -0.000578     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 3.34e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-29958.49 +/- 13563.76\n",
      "Episode length: 482.00 +/- 31.19\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 482           |\n",
      "|    mean_reward          | -3e+04        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 508000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019819659 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.38e+05      |\n",
      "|    n_updates            | 2480          |\n",
      "|    policy_gradient_loss | -0.000328     |\n",
      "|    std                  | 0.705         |\n",
      "|    value_loss           | 2.08e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 638    |\n",
      "|    iterations      | 249    |\n",
      "|    time_elapsed    | 798    |\n",
      "|    total_timesteps | 509952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-20252.10 +/- 11889.25\n",
      "Episode length: 464.12 +/- 46.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 464          |\n",
      "|    mean_reward          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025982987 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.29e+06     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 2.04e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 638    |\n",
      "|    iterations      | 250    |\n",
      "|    time_elapsed    | 802    |\n",
      "|    total_timesteps | 512000 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 638           |\n",
      "|    iterations           | 251           |\n",
      "|    time_elapsed         | 805           |\n",
      "|    total_timesteps      | 514048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049335917 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.72e+06      |\n",
      "|    n_updates            | 2500          |\n",
      "|    policy_gradient_loss | -0.000842     |\n",
      "|    std                  | 0.706         |\n",
      "|    value_loss           | 2.71e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-26540.90 +/- 14191.16\n",
      "Episode length: 494.25 +/- 12.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -2.65e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009273534 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+06     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.00058     |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 4.61e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 636    |\n",
      "|    iterations      | 252    |\n",
      "|    time_elapsed    | 811    |\n",
      "|    total_timesteps | 516096 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 253         |\n",
      "|    time_elapsed         | 814         |\n",
      "|    total_timesteps      | 518144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013567127 |\n",
      "|    clip_fraction        | 0.0779      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75e+06    |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | 0.000902    |\n",
      "|    std                  | 0.702       |\n",
      "|    value_loss           | 1.42e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-32523.69 +/- 9638.32\n",
      "Episode length: 482.00 +/- 35.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 482           |\n",
      "|    mean_reward          | -3.25e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 520000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058445265 |\n",
      "|    clip_fraction        | 0.00107       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.84e+05      |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | 0.000332      |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 1.79e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 254    |\n",
      "|    time_elapsed    | 820    |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 255          |\n",
      "|    time_elapsed         | 824          |\n",
      "|    total_timesteps      | 522240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017897319 |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38e+06     |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 3.87e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-37320.19 +/- 15569.04\n",
      "Episode length: 498.25 +/- 6.55\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 498           |\n",
      "|    mean_reward          | -3.73e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 524000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022220131 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.19e+06      |\n",
      "|    n_updates            | 2550          |\n",
      "|    policy_gradient_loss | -0.000559     |\n",
      "|    std                  | 0.702         |\n",
      "|    value_loss           | 7.67e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 256    |\n",
      "|    time_elapsed    | 830    |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 631           |\n",
      "|    iterations           | 257           |\n",
      "|    time_elapsed         | 833           |\n",
      "|    total_timesteps      | 526336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013813944 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.56e+06      |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | -0.000967     |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 5.95e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-26248.89 +/- 13370.35\n",
      "Episode length: 493.25 +/- 12.83\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 493           |\n",
      "|    mean_reward          | -2.62e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 528000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019220595 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.05e+06      |\n",
      "|    n_updates            | 2570          |\n",
      "|    policy_gradient_loss | -0.000562     |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 3.64e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 258    |\n",
      "|    time_elapsed    | 839    |\n",
      "|    total_timesteps | 528384 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 628           |\n",
      "|    iterations           | 259           |\n",
      "|    time_elapsed         | 843           |\n",
      "|    total_timesteps      | 530432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026081243 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.18e+06      |\n",
      "|    n_updates            | 2580          |\n",
      "|    policy_gradient_loss | -0.00016      |\n",
      "|    std                  | 0.701         |\n",
      "|    value_loss           | 4.33e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-18325.25 +/- 6836.85\n",
      "Episode length: 500.25 +/- 1.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 532000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033047916 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.18e+04     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 2.28e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 626    |\n",
      "|    iterations      | 260    |\n",
      "|    time_elapsed    | 849    |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 626           |\n",
      "|    iterations           | 261           |\n",
      "|    time_elapsed         | 852           |\n",
      "|    total_timesteps      | 534528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011933569 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.43e+06      |\n",
      "|    n_updates            | 2600          |\n",
      "|    policy_gradient_loss | -0.000275     |\n",
      "|    std                  | 0.707         |\n",
      "|    value_loss           | 5.31e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-13451.35 +/- 8232.97\n",
      "Episode length: 481.25 +/- 31.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 481          |\n",
      "|    mean_reward          | -1.35e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 536000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010914741 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.74e+06     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.000866    |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 4.01e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 624    |\n",
      "|    iterations      | 262    |\n",
      "|    time_elapsed    | 858    |\n",
      "|    total_timesteps | 536576 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 263         |\n",
      "|    time_elapsed         | 862         |\n",
      "|    total_timesteps      | 538624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003238821 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24e+05    |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 9.3e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-25742.27 +/- 16109.93\n",
      "Episode length: 480.50 +/- 33.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | -2.57e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026791897 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.48e+05    |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.00384     |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 1.4e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 264    |\n",
      "|    time_elapsed    | 868    |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 265          |\n",
      "|    time_elapsed         | 871          |\n",
      "|    total_timesteps      | 542720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010218545 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45e+06     |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 5.31e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-18322.00 +/- 9662.29\n",
      "Episode length: 496.50 +/- 11.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 496          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.930867e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.56e+06     |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    std                  | 0.697        |\n",
      "|    value_loss           | 5.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 620    |\n",
      "|    iterations      | 266    |\n",
      "|    time_elapsed    | 877    |\n",
      "|    total_timesteps | 544768 |\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 616    |\n",
      "|    iterations      | 270    |\n",
      "|    time_elapsed    | 897    |\n",
      "|    total_timesteps | 552960 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 615           |\n",
      "|    iterations           | 271           |\n",
      "|    time_elapsed         | 901           |\n",
      "|    total_timesteps      | 555008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025207584 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.06         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.59e+06      |\n",
      "|    n_updates            | 2700          |\n",
      "|    policy_gradient_loss | -0.000256     |\n",
      "|    std                  | 0.695         |\n",
      "|    value_loss           | 4.28e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-27267.56 +/- 10968.18\n",
      "Episode length: 493.50 +/- 10.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | -2.73e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 556000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021876087 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | 0.00267      |\n",
      "|    std                  | 0.684        |\n",
      "|    value_loss           | 9.2e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 614    |\n",
      "|    iterations      | 272    |\n",
      "|    time_elapsed    | 907    |\n",
      "|    total_timesteps | 557056 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 273          |\n",
      "|    time_elapsed         | 910          |\n",
      "|    total_timesteps      | 559104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004686089 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25e+06     |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.000521    |\n",
      "|    std                  | 0.684        |\n",
      "|    value_loss           | 3.19e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-29624.85 +/- 20648.12\n",
      "Episode length: 495.88 +/- 13.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 496           |\n",
      "|    mean_reward          | -2.96e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 560000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054304744 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+06      |\n",
      "|    n_updates            | 2730          |\n",
      "|    policy_gradient_loss | -0.00182      |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 2.78e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 612    |\n",
      "|    iterations      | 274    |\n",
      "|    time_elapsed    | 916    |\n",
      "|    total_timesteps | 561152 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 612           |\n",
      "|    iterations           | 275           |\n",
      "|    time_elapsed         | 920           |\n",
      "|    total_timesteps      | 563200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012044067 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.75e+06      |\n",
      "|    n_updates            | 2740          |\n",
      "|    policy_gradient_loss | -0.000407     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 4.24e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-20960.60 +/- 14421.07\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -2.1e+04      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 564000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018746211 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+06      |\n",
      "|    n_updates            | 2750          |\n",
      "|    policy_gradient_loss | -0.000494     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 4.32e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 610    |\n",
      "|    iterations      | 276    |\n",
      "|    time_elapsed    | 926    |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 610          |\n",
      "|    iterations           | 277          |\n",
      "|    time_elapsed         | 929          |\n",
      "|    total_timesteps      | 567296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005320124 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.87e+05     |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    std                  | 0.683        |\n",
      "|    value_loss           | 1.99e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-31668.97 +/- 14841.82\n",
      "Episode length: 495.25 +/- 9.68\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 495           |\n",
      "|    mean_reward          | -3.17e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 568000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047682915 |\n",
      "|    clip_fraction        | 0.000537      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 2770          |\n",
      "|    policy_gradient_loss | -3.53e-05     |\n",
      "|    std                  | 0.684         |\n",
      "|    value_loss           | 2.57e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 278    |\n",
      "|    time_elapsed    | 935    |\n",
      "|    total_timesteps | 569344 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 608           |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 939           |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027846778 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.08e+06      |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | -0.000363     |\n",
      "|    std                  | 0.685         |\n",
      "|    value_loss           | 3.38e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=-28510.05 +/- 13578.49\n",
      "Episode length: 494.50 +/- 11.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | -2.85e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 572000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002920227 |\n",
      "|    clip_fraction        | 0.0158      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.79e+05    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | 0.00122     |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 2.39e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 606    |\n",
      "|    iterations      | 280    |\n",
      "|    time_elapsed    | 945    |\n",
      "|    total_timesteps | 573440 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 281         |\n",
      "|    time_elapsed         | 948         |\n",
      "|    total_timesteps      | 575488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023991866 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.48e+05    |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | 0.00297     |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 1.36e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-19431.99 +/- 12583.42\n",
      "Episode length: 496.00 +/- 13.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | -1.94e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 576000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003556307 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.84e+05    |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.000813   |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 1.49e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 604    |\n",
      "|    iterations      | 282    |\n",
      "|    time_elapsed    | 954    |\n",
      "|    total_timesteps | 577536 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 604           |\n",
      "|    iterations           | 283           |\n",
      "|    time_elapsed         | 958           |\n",
      "|    total_timesteps      | 579584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038180698 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.5e+05       |\n",
      "|    n_updates            | 2820          |\n",
      "|    policy_gradient_loss | -0.000368     |\n",
      "|    std                  | 0.691         |\n",
      "|    value_loss           | 3.54e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-22375.76 +/- 10832.71\n",
      "Episode length: 495.00 +/- 15.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | -2.24e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 580000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004866657 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 2.36e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 603    |\n",
      "|    iterations      | 284    |\n",
      "|    time_elapsed    | 964    |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 967          |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003164097 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+06     |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 2.23e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-19130.55 +/- 13959.20\n",
      "Episode length: 468.62 +/- 46.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | -1.91e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002944519 |\n",
      "|    clip_fraction        | 0.00918     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.38e+05    |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.000471   |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 1.32e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 601    |\n",
      "|    iterations      | 286    |\n",
      "|    time_elapsed    | 973    |\n",
      "|    total_timesteps | 585728 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 601           |\n",
      "|    iterations           | 287           |\n",
      "|    time_elapsed         | 977           |\n",
      "|    total_timesteps      | 587776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043682492 |\n",
      "|    clip_fraction        | 0.000586      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.05         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.21e+06      |\n",
      "|    n_updates            | 2860          |\n",
      "|    policy_gradient_loss | -0.00126      |\n",
      "|    std                  | 0.691         |\n",
      "|    value_loss           | 3.48e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-18325.70 +/- 18210.94\n",
      "Episode length: 477.25 +/- 35.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 477          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 588000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006139282 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+06     |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.000667    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.76e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 599    |\n",
      "|    iterations      | 288    |\n",
      "|    time_elapsed    | 983    |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 986          |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.705423e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.48e+06     |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -1.88e-05    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 3.74e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-18599.18 +/- 12305.94\n",
      "Episode length: 490.88 +/- 26.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | -1.86e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 592000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014879089 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.23e+06    |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.000481   |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 2.67e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 598    |\n",
      "|    iterations      | 290    |\n",
      "|    time_elapsed    | 992    |\n",
      "|    total_timesteps | 593920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 996         |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009422754 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.19e+05    |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | 0.00697     |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 8.06e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-23685.42 +/- 17641.96\n",
      "Episode length: 483.38 +/- 29.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | -2.37e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 596000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025964323 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24e+05     |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 2.04e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 596    |\n",
      "|    iterations      | 292    |\n",
      "|    time_elapsed    | 1002   |\n",
      "|    total_timesteps | 598016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-21639.49 +/- 10432.63\n",
      "Episode length: 494.62 +/- 16.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | -2.16e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009212188 |\n",
      "|    clip_fraction        | 0.0607      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.86e+05    |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    std                  | 0.702       |\n",
      "|    value_loss           | 3.59e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 595    |\n",
      "|    iterations      | 293    |\n",
      "|    time_elapsed    | 1008   |\n",
      "|    total_timesteps | 600064 |\n",
      "-------------------------------\n",
      "Before training: mean_reward:-20834.55 +/- 11996.51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#vec_env = VecNormalize(env, norm_obs=True, norm_reward=True,clip_obs=1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=600000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Before training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bab43037b8499f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T06:51:03.775963200Z",
     "start_time": "2024-04-18T06:51:00.227698Z"
    },
    "collapsed": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard/ppo_run_1713689451.8989656_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -7.91e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 1362      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-7367.56 +/- 6005.28\n",
      "Episode length: 497.75 +/- 8.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 498          |\n",
      "|    mean_reward          | -7.37e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043718563 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.77        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+04     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 7.3e+04      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.15e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 735       |\n",
      "|    iterations      | 2         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 4096      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 779          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045188316 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.765       |\n",
      "|    explained_variance   | 0.0393       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00322     |\n",
      "|    std                  | 0.521        |\n",
      "|    value_loss           | 1.58e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-8195.65 +/- 2355.63\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -8.2e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005994937 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.768       |\n",
      "|    explained_variance   | 0.0353       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.73e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.00047      |\n",
      "|    std                  | 0.522        |\n",
      "|    value_loss           | 1.57e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 688       |\n",
      "|    iterations      | 4         |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 8192      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 713          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009513359 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.769       |\n",
      "|    explained_variance   | 0.0739       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+04     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.00039      |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 1.72e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-7958.83 +/- 4660.71\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.96e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011465271 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.761       |\n",
      "|    explained_variance   | 0.0693       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.58e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.516        |\n",
      "|    value_loss           | 7.82e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.29e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 669       |\n",
      "|    iterations      | 6         |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 12288     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.57e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 693          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039432365 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.755       |\n",
      "|    explained_variance   | 0.0416       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.85e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000177    |\n",
      "|    std                  | 0.515        |\n",
      "|    value_loss           | 1.16e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-4902.77 +/- 697.81\n",
      "Episode length: 491.25 +/- 17.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | -4.9e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000696138 |\n",
      "|    clip_fraction        | 0.00557     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.755      |\n",
      "|    explained_variance   | 0.056       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.2e+04     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.515       |\n",
      "|    value_loss           | 4.16e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -5.1e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 660      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 679          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059226886 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.754       |\n",
      "|    explained_variance   | 0.0606       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+04     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 9.12e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-5967.94 +/- 1889.91\n",
      "Episode length: 484.00 +/- 31.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -5.97e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003105682 |\n",
      "|    clip_fraction        | 0.00625     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.0603      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0005     |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 8.72e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.16e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 659       |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 31        |\n",
      "|    total_timesteps | 20480     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.12e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017803528 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.753       |\n",
      "|    explained_variance   | 0.0432       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.96e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 6.95e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-4995.22 +/- 1075.80\n",
      "Episode length: 484.12 +/- 37.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 484         |\n",
      "|    mean_reward          | -5e+03      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004713544 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.75       |\n",
      "|    explained_variance   | 0.00162     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.01e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000398   |\n",
      "|    std                  | 0.511       |\n",
      "|    value_loss           | 2.03e+04    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | -1.3e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 657      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -6.1e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 671          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037124318 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.745       |\n",
      "|    explained_variance   | 0.0895       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.5e+04      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000694    |\n",
      "|    std                  | 0.507        |\n",
      "|    value_loss           | 5.21e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-9352.61 +/- 5726.00\n",
      "Episode length: 486.62 +/- 35.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 487          |\n",
      "|    mean_reward          | -9.35e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041166293 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.742       |\n",
      "|    explained_variance   | 0.0767       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    std                  | 0.51         |\n",
      "|    value_loss           | 6.32e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -1.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 655       |\n",
      "|    iterations      | 14        |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 28672     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.12e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 667          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029931196 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.734       |\n",
      "|    explained_variance   | 0.0714       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.28e+03     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    std                  | 0.5          |\n",
      "|    value_loss           | 1.25e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-4947.24 +/- 974.73\n",
      "Episode length: 478.88 +/- 24.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 479          |\n",
      "|    mean_reward          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015261171 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.727       |\n",
      "|    explained_variance   | 0.04         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    std                  | 0.5          |\n",
      "|    value_loss           | 6.34e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 539       |\n",
      "|    ep_rew_mean     | -5.41e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 655       |\n",
      "|    iterations      | 16        |\n",
      "|    time_elapsed    | 49        |\n",
      "|    total_timesteps | 32768     |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.14e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 666           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072448683 |\n",
      "|    clip_fraction        | 0.00176       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.728        |\n",
      "|    explained_variance   | 0.049         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.64e+05      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.00125      |\n",
      "|    std                  | 0.503         |\n",
      "|    value_loss           | 3.05e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-5015.75 +/- 1451.15\n",
      "Episode length: 476.50 +/- 32.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | -5.02e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055962075 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.0339       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+04     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    std                  | 0.497        |\n",
      "|    value_loss           | 1.71e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -4.85e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 649       |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 56        |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.99e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 657          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018193745 |\n",
      "|    clip_fraction        | 0.00547      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.718       |\n",
      "|    explained_variance   | 0.0445       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62e+05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    std                  | 0.496        |\n",
      "|    value_loss           | 9.61e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-7709.74 +/- 3348.60\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -7.71e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058575615 |\n",
      "|    clip_fraction        | 0.0022        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.716        |\n",
      "|    explained_variance   | 0.0265        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.99e+05      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | 0.00013       |\n",
      "|    std                  | 0.493         |\n",
      "|    value_loss           | 6.63e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 994      |\n",
      "|    ep_rew_mean     | -1.6e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.38e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012557252 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.712       |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.89e+05     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00083     |\n",
      "|    std                  | 0.494        |\n",
      "|    value_loss           | 1.45e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-5151.65 +/- 1140.17\n",
      "Episode length: 494.88 +/- 16.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | -5.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055267895 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.706       |\n",
      "|    explained_variance   | 0.0212       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.69e+05     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    std                  | 0.488        |\n",
      "|    value_loss           | 2.89e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.33e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 647       |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 69        |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.48e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018154065 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.7         |\n",
      "|    explained_variance   | 0.0181       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.17e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    std                  | 0.486        |\n",
      "|    value_loss           | 1.51e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-6386.48 +/- 4371.39\n",
      "Episode length: 479.12 +/- 39.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | -6.39e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011442861 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.702      |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.69e+04    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.489       |\n",
      "|    value_loss           | 1.71e+05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -2.5e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.59e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022734911 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.701       |\n",
      "|    explained_variance   | 0.0258       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    std                  | 0.487        |\n",
      "|    value_loss           | 4.88e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-5836.70 +/- 1272.82\n",
      "Episode length: 499.75 +/- 3.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -5.84e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005748086 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 0.0332      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.48e+05    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    std                  | 0.484       |\n",
      "|    value_loss           | 1.9e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.13e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 647       |\n",
      "|    iterations      | 26        |\n",
      "|    time_elapsed    | 82        |\n",
      "|    total_timesteps | 53248     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 501        |\n",
      "|    ep_rew_mean          | -5.13e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 84         |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00165405 |\n",
      "|    clip_fraction        | 0.0105     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.692     |\n",
      "|    explained_variance   | 0.0114     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.64e+03   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.000126   |\n",
      "|    std                  | 0.486      |\n",
      "|    value_loss           | 1.48e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-7313.55 +/- 5947.10\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.31e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048461584 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | 0.0153       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.02e+03     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000583    |\n",
      "|    std                  | 0.48         |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.08e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 646       |\n",
      "|    iterations      | 28        |\n",
      "|    time_elapsed    | 88        |\n",
      "|    total_timesteps | 57344     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -2.45e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008133566 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.0633      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36e+04    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 1.3e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-7879.59 +/- 3303.57\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.88e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013425966 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.655       |\n",
      "|    explained_variance   | 0.0369       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.28e+05     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    std                  | 0.466        |\n",
      "|    value_loss           | 1.15e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -1.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 646       |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 94        |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.08e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011129319 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.661       |\n",
      "|    explained_variance   | 0.0302       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+04     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.473        |\n",
      "|    value_loss           | 8.37e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-6043.26 +/- 1619.36\n",
      "Episode length: 497.12 +/- 10.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | -6.04e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055721086 |\n",
      "|    clip_fraction        | 0.00713      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.0432       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.59e+04     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 9.73e-05     |\n",
      "|    std                  | 0.47         |\n",
      "|    value_loss           | 9.57e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.23e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 640       |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 102       |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.18e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005744797 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.0388      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.23e+04    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    std                  | 0.465       |\n",
      "|    value_loss           | 1.67e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-5146.74 +/- 954.66\n",
      "Episode length: 489.12 +/- 31.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 489          |\n",
      "|    mean_reward          | -5.15e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021356475 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.647       |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+04     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -1.33e-05    |\n",
      "|    std                  | 0.46         |\n",
      "|    value_loss           | 1.92e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.94e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 621       |\n",
      "|    iterations      | 34        |\n",
      "|    time_elapsed    | 112       |\n",
      "|    total_timesteps | 69632     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 619          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025869492 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.632       |\n",
      "|    explained_variance   | 0.0815       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.92e+03     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.451        |\n",
      "|    value_loss           | 1.74e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-6694.57 +/- 2805.69\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -6.69e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003753365 |\n",
      "|    clip_fraction        | 0.0158      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.57e+03    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.000378   |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 1.65e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.23e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 36        |\n",
      "|    time_elapsed    | 121       |\n",
      "|    total_timesteps | 73728     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.3e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035547698 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.0656       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.14e+03     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00083     |\n",
      "|    std                  | 0.449        |\n",
      "|    value_loss           | 1.23e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-6770.33 +/- 2546.61\n",
      "Episode length: 488.25 +/- 33.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -6.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021324586 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.618       |\n",
      "|    explained_variance   | 0.0795       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.7e+04      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    std                  | 0.449        |\n",
      "|    value_loss           | 1.62e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.25e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 590       |\n",
      "|    iterations      | 38        |\n",
      "|    time_elapsed    | 131       |\n",
      "|    total_timesteps | 77824     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 590          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011437298 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.0793       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.14e+03     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 1.95e-05     |\n",
      "|    std                  | 0.448        |\n",
      "|    value_loss           | 5.52e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-7606.44 +/- 3932.12\n",
      "Episode length: 496.88 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | -7.61e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007590077 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.82e+03    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.52e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -2.13e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 578       |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 141       |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.04e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 578          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006594521 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.6         |\n",
      "|    explained_variance   | 0.0538       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000111    |\n",
      "|    std                  | 0.441        |\n",
      "|    value_loss           | 3.26e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-5184.87 +/- 1456.03\n",
      "Episode length: 485.00 +/- 27.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -5.18e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011976447 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.601       |\n",
      "|    explained_variance   | 0.0927       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84e+04     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.441        |\n",
      "|    value_loss           | 1.42e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.13e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 569       |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 151       |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-5575.05 +/- 2128.84\n",
      "Episode length: 492.00 +/- 12.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | -5.58e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014541766 |\n",
      "|    clip_fraction        | 0.00303      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | 0.0573       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+05     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.442        |\n",
      "|    value_loss           | 3.63e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 466      |\n",
      "|    ep_rew_mean     | -4.7e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -2.54e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 560          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075456826 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.0709       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.14e+03     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.435        |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-6422.91 +/- 3053.98\n",
      "Episode length: 499.75 +/- 3.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -6.42e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010512683 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.0673       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.12e+04     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    std                  | 0.435        |\n",
      "|    value_loss           | 4.81e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.19e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 551       |\n",
      "|    iterations      | 45        |\n",
      "|    time_elapsed    | 167       |\n",
      "|    total_timesteps | 92160     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.27e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 552         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006210123 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.7e+03     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 3.72e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-4774.98 +/- 848.00\n",
      "Episode length: 476.12 +/- 31.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | -4.77e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017014747 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.091        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.33e+03     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    std                  | 0.428        |\n",
      "|    value_loss           | 1.68e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 544       |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 176       |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -8.32e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017926497 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.0143       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.24e+03     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000532    |\n",
      "|    std                  | 0.423        |\n",
      "|    value_loss           | 1.01e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-5487.95 +/- 1800.57\n",
      "Episode length: 495.12 +/- 15.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | -5.49e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029440613 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.0668       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.7e+04      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.423        |\n",
      "|    value_loss           | 4.49e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 537       |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 186       |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.01e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 537           |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 190           |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053785666 |\n",
      "|    clip_fraction        | 0.00146       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.558        |\n",
      "|    explained_variance   | 0.0867        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.71e+04      |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | -0.000162     |\n",
      "|    std                  | 0.422         |\n",
      "|    value_loss           | 9.59e+04      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-4998.69 +/- 574.60\n",
      "Episode length: 493.25 +/- 20.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 493         |\n",
      "|    mean_reward          | -5e+03      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005159573 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.71e+03    |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    std                  | 0.42        |\n",
      "|    value_loss           | 1.66e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 531       |\n",
      "|    iterations      | 51        |\n",
      "|    time_elapsed    | 196       |\n",
      "|    total_timesteps | 104448    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -4.97e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 531         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006094318 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.0495      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14e+06    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 7.67e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-6653.71 +/- 5708.60\n",
      "Episode length: 478.25 +/- 31.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 478          |\n",
      "|    mean_reward          | -6.65e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047028577 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.543       |\n",
      "|    explained_variance   | 0.0562       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+04      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.000418    |\n",
      "|    std                  | 0.414        |\n",
      "|    value_loss           | 1.73e+04     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 718      |\n",
      "|    ep_rew_mean     | -7.4e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.11e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 210           |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028663865 |\n",
      "|    clip_fraction        | 0.038         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.539        |\n",
      "|    explained_variance   | 0.0955        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+04      |\n",
      "|    n_updates            | 530           |\n",
      "|    policy_gradient_loss | -0.000249     |\n",
      "|    std                  | 0.415         |\n",
      "|    value_loss           | 1.5e+04       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-5909.79 +/- 1868.77\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 501         |\n",
      "|    mean_reward          | -5.91e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001069733 |\n",
      "|    clip_fraction        | 0.00132     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.0442      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.78e+05    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00059    |\n",
      "|    std                  | 0.416       |\n",
      "|    value_loss           | 5.79e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -1.69e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 521       |\n",
      "|    iterations      | 55        |\n",
      "|    time_elapsed    | 216       |\n",
      "|    total_timesteps | 112640    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.12e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 521         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002202771 |\n",
      "|    clip_fraction        | 0.00337     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.0652      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11e+04    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.415       |\n",
      "|    value_loss           | 1.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-7204.28 +/- 4664.47\n",
      "Episode length: 488.38 +/- 33.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | -7.2e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 116000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005357297 |\n",
      "|    clip_fraction        | 0.0108      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.0976      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02e+04    |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.000134   |\n",
      "|    std                  | 0.412       |\n",
      "|    value_loss           | 1.47e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | -9.91e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 516       |\n",
      "|    iterations      | 57        |\n",
      "|    time_elapsed    | 225       |\n",
      "|    total_timesteps | 116736    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.05e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 229          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044694655 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.532       |\n",
      "|    explained_variance   | 0.092        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.62e+04     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    std                  | 0.412        |\n",
      "|    value_loss           | 8.09e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-5134.46 +/- 1294.38\n",
      "Episode length: 484.12 +/- 21.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | -5.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060984846 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 0.0572       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+04     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    std                  | 0.406        |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 551       |\n",
      "|    ep_rew_mean     | -5.45e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 513       |\n",
      "|    iterations      | 59        |\n",
      "|    time_elapsed    | 235       |\n",
      "|    total_timesteps | 120832    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.09e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 239          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034583462 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | 0.0209       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+04     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.397        |\n",
      "|    value_loss           | 1.32e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-9325.46 +/- 9089.29\n",
      "Episode length: 480.25 +/- 34.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -9.33e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042910306 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.0425       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.77e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.397        |\n",
      "|    value_loss           | 2.36e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 994      |\n",
      "|    ep_rew_mean     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    fps             | 509      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030813026 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.0622       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 657          |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    std                  | 0.394        |\n",
      "|    value_loss           | 1.35e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-5619.94 +/- 1678.79\n",
      "Episode length: 499.75 +/- 3.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -5.62e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027328103 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.0491       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.65e+03     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.391        |\n",
      "|    value_loss           | 1.65e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 511       |\n",
      "|    ep_rew_mean     | -5.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 505       |\n",
      "|    iterations      | 63        |\n",
      "|    time_elapsed    | 255       |\n",
      "|    total_timesteps | 129024    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -2.07e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 258          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026115838 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.0301       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+04     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.388        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-7220.95 +/- 3740.10\n",
      "Episode length: 489.62 +/- 30.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -7.22e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015535294 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.045        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.87e+05     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    std                  | 0.389        |\n",
      "|    value_loss           | 7.43e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.99e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 502       |\n",
      "|    iterations      | 65        |\n",
      "|    time_elapsed    | 264       |\n",
      "|    total_timesteps | 133120    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -7e+03        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 503           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 268           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.8751706e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.476        |\n",
      "|    explained_variance   | 0.0494        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.51e+06      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | 0.000297      |\n",
      "|    std                  | 0.39          |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-6583.87 +/- 2745.94\n",
      "Episode length: 488.00 +/- 34.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -6.58e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059500895 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | -0.00576     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56e+03     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000253    |\n",
      "|    std                  | 0.382        |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -5.7e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 499      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -3.7e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 278          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021517812 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0473       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.08e+05     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    std                  | 0.382        |\n",
      "|    value_loss           | 9.86e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-8370.61 +/- 3923.30\n",
      "Episode length: 501.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 501           |\n",
      "|    mean_reward          | -8.37e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 140000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032453547 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.457        |\n",
      "|    explained_variance   | 0.0278        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.69e+05      |\n",
      "|    n_updates            | 680           |\n",
      "|    policy_gradient_loss | -0.000494     |\n",
      "|    std                  | 0.382         |\n",
      "|    value_loss           | 1.2e+06       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -3.68e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 496       |\n",
      "|    iterations      | 69        |\n",
      "|    time_elapsed    | 284       |\n",
      "|    total_timesteps | 141312    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -5.01e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058652125 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.78e+05     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    std                  | 0.383        |\n",
      "|    value_loss           | 1.93e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-8128.91 +/- 9691.12\n",
      "Episode length: 484.88 +/- 23.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | -8.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010654543 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.462       |\n",
      "|    explained_variance   | 0.0144       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+04     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.000709    |\n",
      "|    std                  | 0.385        |\n",
      "|    value_loss           | 2.29e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 454       |\n",
      "|    ep_rew_mean     | -1.23e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 494       |\n",
      "|    iterations      | 71        |\n",
      "|    time_elapsed    | 294       |\n",
      "|    total_timesteps | 145408    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -4.95e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 494           |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 297           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028154056 |\n",
      "|    clip_fraction        | 0.000635      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.467        |\n",
      "|    explained_variance   | 0.0381        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.75e+05      |\n",
      "|    n_updates            | 710           |\n",
      "|    policy_gradient_loss | -5.35e-05     |\n",
      "|    std                  | 0.387         |\n",
      "|    value_loss           | 6.19e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-8963.16 +/- 6486.09\n",
      "Episode length: 486.25 +/- 36.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 486          |\n",
      "|    mean_reward          | -8.96e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018029745 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.043        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.83e+04     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.39         |\n",
      "|    value_loss           | 4.29e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -2.76e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 491       |\n",
      "|    iterations      | 73        |\n",
      "|    time_elapsed    | 304       |\n",
      "|    total_timesteps | 149504    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.49e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 307          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017900508 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.479       |\n",
      "|    explained_variance   | 0.0358       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+05     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00252     |\n",
      "|    std                  | 0.39         |\n",
      "|    value_loss           | 1.09e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-5991.74 +/- 2437.61\n",
      "Episode length: 479.88 +/- 38.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | -5.99e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048378184 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.0256       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.76e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 1.57e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -3.51e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 489       |\n",
      "|    iterations      | 75        |\n",
      "|    time_elapsed    | 313       |\n",
      "|    total_timesteps | 153600    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 501           |\n",
      "|    ep_rew_mean          | -5.07e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 490           |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 317           |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081118103 |\n",
      "|    clip_fraction        | 0.00464       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.468        |\n",
      "|    explained_variance   | 0.0183        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.14e+05      |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | 0.000382      |\n",
      "|    std                  | 0.386         |\n",
      "|    value_loss           | 1.1e+06       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-6889.36 +/- 3925.05\n",
      "Episode length: 479.88 +/- 40.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | -6.89e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002692502 |\n",
      "|    clip_fraction        | 0.0582      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.0347      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46e+04    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    std                  | 0.386       |\n",
      "|    value_loss           | 2.42e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -1.79e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 487       |\n",
      "|    iterations      | 77        |\n",
      "|    time_elapsed    | 323       |\n",
      "|    total_timesteps | 157696    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -2.62e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 326          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029994943 |\n",
      "|    clip_fraction        | 0.00864      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.0546       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 2.51e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-9329.95 +/- 8339.43\n",
      "Episode length: 496.25 +/- 12.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 496          |\n",
      "|    mean_reward          | -9.33e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024951617 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.62e+04     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    std                  | 0.381        |\n",
      "|    value_loss           | 8.93e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.01e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 485       |\n",
      "|    iterations      | 79        |\n",
      "|    time_elapsed    | 333       |\n",
      "|    total_timesteps | 161792    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.88e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 486          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031740274 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0695       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.17e+03     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 1.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-7244.82 +/- 5241.39\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -7.24e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041970583 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | 0.0683       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.83e+03     |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.000254    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 2.06e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -6.79e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 483       |\n",
      "|    iterations      | 81        |\n",
      "|    time_elapsed    | 342       |\n",
      "|    total_timesteps | 165888    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.79e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005268968 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.0541      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.71e+04    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    std                  | 0.381       |\n",
      "|    value_loss           | 5.95e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-10141.12 +/- 7144.65\n",
      "Episode length: 501.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 501          |\n",
      "|    mean_reward          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013853812 |\n",
      "|    clip_fraction        | 0.00439      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | 0.0611       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.381        |\n",
      "|    value_loss           | 5.42e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.96e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 482       |\n",
      "|    iterations      | 83        |\n",
      "|    time_elapsed    | 352       |\n",
      "|    total_timesteps | 169984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-6382.08 +/- 3707.87\n",
      "Episode length: 490.50 +/- 19.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | -6.38e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065498874 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.00817      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.00016      |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 1.4e+04      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 494       |\n",
      "|    ep_rew_mean     | -5.85e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 479       |\n",
      "|    iterations      | 84        |\n",
      "|    time_elapsed    | 358       |\n",
      "|    total_timesteps | 172032    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.6e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 362          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051843203 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.00508      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21e+04     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000413    |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 2.21e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-5802.71 +/- 2141.08\n",
      "Episode length: 491.25 +/- 19.63\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 491           |\n",
      "|    mean_reward          | -5.8e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 176000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7313392e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.46         |\n",
      "|    explained_variance   | 0.0465        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.88e+05      |\n",
      "|    n_updates            | 850           |\n",
      "|    policy_gradient_loss | 9.45e-05      |\n",
      "|    std                  | 0.383         |\n",
      "|    value_loss           | 8.42e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -4.93e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 478       |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 368       |\n",
      "|    total_timesteps | 176128    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -4.95e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 371          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014938596 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0563       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54e+04     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    std                  | 0.38         |\n",
      "|    value_loss           | 2.47e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-8782.10 +/- 7246.18\n",
      "Episode length: 500.00 +/- 2.65\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 500           |\n",
      "|    mean_reward          | -8.78e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 180000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039029226 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.453        |\n",
      "|    explained_variance   | 0.0708        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.76e+05      |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | 0.000122      |\n",
      "|    std                  | 0.381         |\n",
      "|    value_loss           | 5.44e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 476       |\n",
      "|    ep_rew_mean     | -4.71e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 476       |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 378       |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 447          |\n",
      "|    ep_rew_mean          | -4.32e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 381          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072109727 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | 0.0441       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.7e+03      |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    std                  | 0.376        |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-5036.11 +/- 1353.04\n",
      "Episode length: 475.50 +/- 31.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | -5.04e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 184000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013252601 |\n",
      "|    clip_fraction        | 0.00396      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.0614       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.41e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000402    |\n",
      "|    std                  | 0.375        |\n",
      "|    value_loss           | 3.27e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 475       |\n",
      "|    iterations      | 90        |\n",
      "|    time_elapsed    | 387       |\n",
      "|    total_timesteps | 184320    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -5.03e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001021493 |\n",
      "|    clip_fraction        | 0.00239     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.438      |\n",
      "|    explained_variance   | 0.0567      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+06    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.000983   |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 1.18e+06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-4937.15 +/- 682.47\n",
      "Episode length: 488.50 +/- 33.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | -4.94e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042472114 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.0208       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+04     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | 5.02e-05     |\n",
      "|    std                  | 0.37         |\n",
      "|    value_loss           | 2.09e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 501       |\n",
      "|    ep_rew_mean     | -1.56e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 473       |\n",
      "|    iterations      | 92        |\n",
      "|    time_elapsed    | 397       |\n",
      "|    total_timesteps | 188416    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 461          |\n",
      "|    ep_rew_mean          | -4.44e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 401          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036777584 |\n",
      "|    clip_fraction        | 0.00952      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.426       |\n",
      "|    explained_variance   | 0.0763       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.59e+05     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    std                  | 0.371        |\n",
      "|    value_loss           | 7.01e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-5809.92 +/- 2685.15\n",
      "Episode length: 495.75 +/- 9.09\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 496        |\n",
      "|    mean_reward          | -5.81e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 192000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00660877 |\n",
      "|    clip_fraction        | 0.0348     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | 0.0142     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.04e+03   |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.00228   |\n",
      "|    std                  | 0.369      |\n",
      "|    value_loss           | 4.59e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 505       |\n",
      "|    ep_rew_mean     | -4.98e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 472       |\n",
      "|    iterations      | 94        |\n",
      "|    time_elapsed    | 407       |\n",
      "|    total_timesteps | 192512    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 501          |\n",
      "|    ep_rew_mean          | -1.17e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 473          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 411          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016286611 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | 0.0741       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+04      |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.000445    |\n",
      "|    std                  | 0.373        |\n",
      "|    value_loss           | 1.97e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-6467.68 +/- 3384.60\n",
      "Episode length: 488.88 +/- 21.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 489         |\n",
      "|    mean_reward          | -6.47e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001782655 |\n",
      "|    clip_fraction        | 0.000879    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.052       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.64e+04    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.000392   |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 7.57e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 903       |\n",
      "|    ep_rew_mean     | -9.09e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 470       |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 417       |\n",
      "|    total_timesteps | 196608    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 501         |\n",
      "|    ep_rew_mean          | -4.93e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 421         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001806881 |\n",
      "|    clip_fraction        | 0.00728     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.072       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.52e+05    |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 4.24e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-4696.01 +/- 749.74\n",
      "Episode length: 470.00 +/- 36.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 470          |\n",
      "|    mean_reward          | -4.7e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012703352 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.0531       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.373        |\n",
      "|    value_loss           | 7.24e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 660       |\n",
      "|    ep_rew_mean     | -3.36e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 469       |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 427       |\n",
      "|    total_timesteps | 200704    |\n",
      "----------------------------------\n",
      "After training: mean_reward:-6787.99 +/- 4464.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#del model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard/\",device=\"cuda\")\n",
    "model.set_parameters(\"last_model\")\n",
    "model.learn(\n",
    "    total_timesteps=200000,\n",
    "    tb_log_name=log_name,\n",
    "    **kwargs\n",
    ")\n",
    "# 评估训练后的 policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"After training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de305199a42f8d",
   "metadata": {},
   "source": [
    "### 绘图检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85b9e88968d1a4a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T12:30:37.799337Z",
     "start_time": "2024-04-20T12:30:34.259247500Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 total reward: -5261.017932448655\n",
      "Episode 2 total reward: -10806.550429608842\n",
      "Episode 3 total reward: -4735.371204648205\n",
      "Episode 4 total reward: -7436.770954344722\n",
      "Episode 5 total reward: -4863.940269589888\n",
      "Episode 6 total reward: -4730.810927739599\n",
      "Episode 7 total reward: -7520.096420014863\n",
      "Episode 8 total reward: -4445.659774310201\n",
      "Episode 9 total reward: -4932.774131543555\n",
      "Episode 10 total reward: -4752.616243786749\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个包含10个子图的窗口\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axs = axs.ravel()  # 将二维数组展平，方便通过索引访问每个子图\n",
    "\n",
    "env = env_test1.DroneEnv()\n",
    "for i in range(10):\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    trajectory_x = [env.xy_p[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_y = [env.xy_p[1]]  # 存储无人机路径的y坐标\n",
    "    trajectory_ex = [env.xy_e[0]]  # 存储无人机路径的x坐标\n",
    "    trajectory_ey = [env.xy_e[1]]  # 存储无人机路径的y坐标\n",
    "    \n",
    "    # 在第i个子图中绘制环境和障碍物\n",
    "    axs[i].scatter(env.xy_e[0], env.xy_e[1], marker='x', color='green', label='Goal')\n",
    "    for k in env.obstacles:\n",
    "        obstacle_circle = plt.Circle(k, env.r_obstacles, color='lightblue', fill=True)\n",
    "        axs[i].add_patch(obstacle_circle)\n",
    "    axs[i].set_xlim(env.space1.low[0], env.space1.high[0])\n",
    "    axs[i].set_ylim(env.space1.low[1], env.space1.high[1])\n",
    "    axs[i].set_xlabel('X')\n",
    "    axs[i].set_ylabel('Y')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Episode {i+1}')\n",
    "\n",
    "    # 通过预训练模型控制无人机执行任务并绘制路径\n",
    "    model = PPO.load(\"best_model\") \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        count += 1\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        next_state, reward, done, t, info = env.step(action)\n",
    "        #if reward < -10:\n",
    "            #print(state, action, reward)\n",
    "        if count > 500:\n",
    "            done = True\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory_x.append(env.xy_p[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_y.append(env.xy_p[1])  # 更新无人机路径的y坐标\n",
    "        trajectory_ex.append(env.xy_e[0])  # 更新无人机路径的x坐标\n",
    "        trajectory_ey.append(env.xy_e[1])  # 更新无人机路径的y坐标\n",
    "\n",
    "    # 绘制无人机路径\n",
    "    axs[i].plot(trajectory_x, trajectory_y, color='red', linewidth=0.5)\n",
    "    axs[i].plot(trajectory_ex, trajectory_ey, color='red', linewidth=0.5)\n",
    "\n",
    "    # 打印每个episode的总奖励\n",
    "    print(f'Episode {i+1} total reward:', total_reward)\n",
    "\n",
    "# 显示子图窗口\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58fa9da8-3fc8-4a42-a7ef-550800440c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dc779-ac63-49ed-be21-b16acfdb9ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
